{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "323c4a86-a69d-4b72-801e-ca3574399fc9"
   },
   "source": [
    "# GPU Accelerated Elastic Deep Learning Service in Cloud Pak for Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f736755b-56bf-4355-8aca-dc0b3da985c0"
   },
   "source": [
    "#### Notebook created by Kelvin Lui, Xue Yin Zhuang, Xue Zhou Yuan (January 2021)\n",
    "\n",
    "Watson Machine Learning Accelerator in Cloud Pak for Data offers GPU Accelerated Elastic Deep Learning service.   This service enables multiple data scientists to accelerate deep learning model training across multiple GPUs and server, share GPUs in a dynamic fashion,  and drives data scientist productivity and overall GPU utilization.\n",
    "\n",
    "In this notebook, you will learn how to scale PyTorch model with multiple GPUs with GPU Accelerated Elastic Deep Learning service, monitor the running job, and debug any issues seen.\n",
    "\n",
    "This notebook uses Watson Machine learning Accelerator 2.2 with Cloud Pak for Data 3.5. \n",
    "\n",
    "### Contents\n",
    "\n",
    "- [The big picture](#The-big-picture)\n",
    "- [Changes to your code](#Changes-to-your-code)\n",
    "- [Set up API end point and log on](#Set-up-API-end-point-and-log-on)\n",
    "- [Submit job via API](#Submit-job-via-API)\n",
    "- [Monitor running job](#Monitor-running-job)\n",
    "- [Training metrics and logs](#Training-metrics-and-logs)\n",
    "- [Download trained model](#Download-trained-model)\n",
    "- [Further information and useful links](#Further-information-and-useful-links)\n",
    "- [Appendix](#Appendix)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef0c8184-d2a6-4f50-bc75-41853fda7c2e"
   },
   "source": [
    "## The big picture\n",
    "[Back to top](#Contents)\n",
    "\n",
    "This notebook details the process of taking your PyTorch model and making the changes required to train the model using [IBM Watson Machine Learning GPU Accelerated Elastic Deep Learning service](https://developer.ibm.com/series/learning-path-get-started-with-watson-machine-learning-accelerator/) (WML Accelerator) \n",
    "\n",
    "\n",
    "The image below shows the various elements required to use Elastic Deep Learning Service. In this notebook we will step through each of these elements in more detail. Through this process you will offload your code to a WML Accelerator cluster, monitor the running job, retrieve the output and debug any issues seen. A [static version](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/shared-images/5_running_job.png) is also available.\n",
    "\n",
    "![overall](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/shared-images/5_running_job.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52d6cd54-89df-4284-b2b1-bd298c02db85"
   },
   "source": [
    "## Changes to your code\n",
    "[Back to top](#Contents)\n",
    "\n",
    "In this section we will use the PyTorch Resnet 50 model and make the required changes needed to use this model with the elastic distributed training engine (EDT). An overview of these changes can be seen in the diagram below. A [static version](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/shared-images/2_code_adaptations.png) is also available.\n",
    "\n",
    "![code](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/shared-images/2_code_adaptations.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83c55cc3-408b-417a-89cb-7873067a05ce"
   },
   "source": [
    "The key changes to your code in order to use elastic distributed training are the following:\n",
    "- Importing libraries and setting up environment variables\n",
    "- Data loading function for elastic distributed training\n",
    "- Extract parameters for training\n",
    "- Replace training and testing loops with the loop equivalents for elastic distributed training\n",
    "\n",
    "For the purpose of this tutorial we train RestNet50 model with Elastic Distributed Training (EDT).\n",
    "\n",
    "See the blog associated with this notebook with more detailed explanation of the above changes.\n",
    "https://developer.ibm.com/articles/elastic-distributed-training-edt-in-watson-machine-learning-accelerator/\n",
    "\n",
    "See more information about the Elastic Distributed Training API in \n",
    " [IBM Documentation](https://www.ibm.com/docs/en/wmla/2.2.0?topic=SSFHA8_2.2.0/wmla_workloads_elastic_distributed_training.html).\n",
    "\n",
    "\n",
    "\n",
    "Your modified code should be made available in a directory which also contains the EDT helper scripts: `edtcallback.py`, `emetrics.py` and `elog.py`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6730304-6c56-45d0-b5e4-6635504b1fc8"
   },
   "source": [
    "## Define helper methods\n",
    "Define the required helper methods. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7f543042-bd1f-4210-af76-7e03e5949e98",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# import tarfile\n",
    "import tempfile\n",
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import pandas as pd\n",
    "from IPython.display import display, FileLink, clear_output\n",
    "\n",
    "import requests\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%pylab inline\n",
    "\n",
    "import base64\n",
    "import json\n",
    "import time\n",
    "import urllib\n",
    "import tarfile\n",
    "\n",
    "\n",
    "def query_job_status(job_id,refresh_rate=3) :\n",
    "\n",
    "    execURL = dl_rest_url  +'/execs/'+ job_id['id']\n",
    "    pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "    keep_running=True\n",
    "    res=None\n",
    "    while(keep_running):\n",
    "        res = req.get(execURL, headers=commonHeaders, verify=False)\n",
    "        monitoring = pd.DataFrame(res.json(), index=[0])\n",
    "        pd.set_option('max_colwidth', 120)\n",
    "        clear_output()\n",
    "        print(\"Refreshing every {} seconds\".format(refresh_rate))\n",
    "        display(monitoring)\n",
    "        pp.pprint(res.json())\n",
    "        if(res.json()['state'] not in ['PENDING_CRD_SCHEDULER', 'SUBMITTED','RUNNING']) :\n",
    "            keep_running=False\n",
    "        time.sleep(refresh_rate)\n",
    "    return res\n",
    "\n",
    "def query_executor_stdout_log(job_id) :\n",
    "\n",
    "    execURL = dl_rest_url  +'/scheduler/applications/'+ job_id['id'] + '/executor/1/logs/stdout?lastlines=1000'\n",
    "    #'https://{}/platform/rest/deeplearning/v1/scheduler/applications/wmla-267/driver/logs/stderr?lastlines=10'.format(hostname)\n",
    "    commonHeaders2={'accept': 'text/plain', 'X-Auth-Token': access_token}\n",
    "    print (execURL)\n",
    "    res = req.get(execURL, headers=commonHeaders2, verify=False)\n",
    "    print(res.text)\n",
    "    \n",
    "    \n",
    "def query_train_metric(job_id) :\n",
    "\n",
    "    #execURL = dl_rest_url  +'/execs/'+ job_id['id'] + '/log'\n",
    "    execURL = dl_rest_url  +'/execs/'+ job_id['id'] + '/log'\n",
    "    #'https://{}/platform/rest/deeplearning/v1/scheduler/applications/wmla-267/driver/logs/stderr?lastlines=10'.format(hostname)\n",
    "    commonHeaders2={'accept': 'text/plain', 'X-Auth-Token': access_token}\n",
    "    print (execURL)\n",
    "    res = req.get(execURL, headers=commonHeaders2, verify=False)\n",
    "    print(res.text)\n",
    "\n",
    "    # save result file    \n",
    "def download_trained_model(job_id) :\n",
    "\n",
    "    from IPython.display import display, FileLink\n",
    "\n",
    "    # save result file\n",
    "    commonHeaders3={'accept': 'application/octet-stream', 'X-Auth-Token': access_token}\n",
    "    execURL = dl_rest_url  +'/execs/'+ r.json()['id'] + '/result'\n",
    "    res = req.get(execURL, headers=commonHeaders3, verify=False, stream=True)\n",
    "    print (execURL)\n",
    "\n",
    "    tmpfile = model_dir + '/' + r.json()['id'] +'.zip'\n",
    "    print ('Save model: ', tmpfile )\n",
    "    with open(tmpfile,'wb') as f:\n",
    "        f.write(res.content)\n",
    "        f.close()\n",
    "\n",
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "200c97d5-d65d-48ea-ad82-92c1dea21e59",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "model_dir = f'./resnet-wmla' \n",
    "model_main = f'elastic-main.py'\n",
    "model_callback = f'edtcallback.py'\n",
    "model_elog = f'elog.py'\n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "463704fb-9d32-4dcd-a9b7-513ead053f25"
   },
   "source": [
    "Resnet50 model: elastic-main.py\n",
    "This is the main file that is required by the elastic distributed training engine. It acts as the program main entrance. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "29cfb171-a98e-4a32-9162-01f1c4eafd09",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./resnet-wmla/elastic-main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {model_dir}/{model_main}\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import print_function\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "from callbacks import Callback\n",
    "from fabric_model import FabricModel\n",
    "from edtcallback import EDTLoggerCallback\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "## Define model and extract training parameters\n",
    "def get_max_worker():\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description='EDT Example')\n",
    "    parser.add_argument('--numWorker', type=int, default='16', help='input the max number ')\n",
    "    parser.add_argument('--gpuPerWorker', type=int, default='1', help='input the path of initial weight file')\n",
    "    args, _ = parser.parse_known_args()\n",
    "    num_worker = args.numWorker * args.gpuPerWorker\n",
    "    print ('args.numWorker: ', args.numWorker , 'args.gpuPerWorker: ', args.gpuPerWorker)\n",
    "    return num_worker\n",
    "\n",
    "BATCH_SIZE_PER_DEVICE = 64\n",
    "NUM_EPOCHS = 3\n",
    "MAX_NUM_WORKERS = get_max_worker()\n",
    "START_LEARNING_RATE = 0.4\n",
    "LR_STEP_SIZE = 30\n",
    "LR_GAMMA = 0.1\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "## Define dataset location \n",
    "DATA_DIR = os.getenv(\"DATA_DIR\")\n",
    "if DATA_DIR is None:\n",
    "    DATA_DIR = '/tmp'\n",
    "print(\"DATA_DIR: \" + DATA_DIR)\n",
    "TRAIN_DATA = DATA_DIR + \"/cifar10\"\n",
    "TEST_DATA = DATA_DIR + \"/cifar10\"\n",
    "\n",
    "\n",
    "## <Xue Yin>  Documentation of Callback function\n",
    "class LRScheduleCallback(Callback):\n",
    "    def __init__(self, step_size, gamma):\n",
    "        super(LRScheduleCallback, self).__init__()\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def on_epoch_begin(self, epoch):\n",
    "        if (epoch != 0) and (epoch % self.step_size == 0):\n",
    "            for param_group in self.params['optimizer'].param_groups:\n",
    "                param_group['lr'] *= self.gamma\n",
    "\n",
    "        print(\"LRScheduleCallback epoch={}, learning_rate={}\".format(epoch,\n",
    "              self.params['optimizer'].param_groups[0]['lr']))\n",
    "\n",
    "## Data loading function for EDT\n",
    "def getDatasets():\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    return (torchvision.datasets.CIFAR10(root=TRAIN_DATA, train=True, download=True, transform=transform_train),\n",
    "            torchvision.datasets.CIFAR10(root=TEST_DATA, train=False, download=True, transform=transform_test))\n",
    "\n",
    "def custom_train(model, data, eva, train_loader, fn_args):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    opt = model.get_optimizer()\n",
    "    opt.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    cri = model.get_loss_function()\n",
    "    loss = cri(outputs, labels)\n",
    "    loss.backward()\n",
    "    acc = eva(outputs, labels)\n",
    "    return acc, loss\n",
    "\n",
    "def custom_test(model, test_iter, fn_args):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    cri = model.get_loss_function()\n",
    "    valid_loss = 0.0\n",
    "    counter = 0\n",
    "    for(inputs, labels) in test_iter:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        output = model(inputs)\n",
    "        loss = cri(output, labels)\n",
    "        valid_loss += loss.item()\n",
    "        counter += 1\n",
    "    valid_loss /= counter\n",
    "    return valid_loss\n",
    "\n",
    "def main(model_type):\n",
    "    print('==> Building model..' + str(model_type))\n",
    "    model = models.__dict__[model_type]()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=START_LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "    loss_function = F.cross_entropy\n",
    "    \n",
    "    edt_m = FabricModel(model, getDatasets, loss_function, optimizer, enable_onnx=True, fn_step_train=custom_train, fn_test=custom_test, user_callback=[LRScheduleCallback(LR_STEP_SIZE, LR_GAMMA)],  driver_logger=EDTLoggerCallback())\n",
    "    print('==> epochs:' + str(NUM_EPOCHS) + ', batchsize:' + str(BATCH_SIZE_PER_DEVICE) + ', engines_number:' + str(MAX_NUM_WORKERS))\n",
    "    edt_m.train(NUM_EPOCHS, BATCH_SIZE_PER_DEVICE, MAX_NUM_WORKERS, num_dataloader_threads=4, validation_freq=10, checkpoint_freq=0)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(\"resnet50\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d928bf7-2725-4c10-8a05-1b5542537123"
   },
   "source": [
    "### EDT helper scripts: edtcallback.py\n",
    "The edtcallback.py scripts counts model loss and accuracy and logs them to a the driver log. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ebe7432b-3b26-4060-afd2-5ca129183440",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./resnet-wmla/edtcallback.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {model_dir}/{model_callback}\n",
    "#! /usr/bin/env python\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from callbacks import LoggerCallback\n",
    "from emetrics import EMetrics\n",
    "from elog import ELog\n",
    "\n",
    "'''\n",
    "    EDTLoggerCallback class define LoggerCallback to trigger Elog.\n",
    "'''\n",
    "\n",
    "class EDTLoggerCallback(LoggerCallback):\n",
    "    def __init__(self):\n",
    "        self.gs =0\n",
    "\n",
    "    def log_train_metrics(self, loss, acc, completed_batch,  worker=0):\n",
    "        acc = acc/100.0\n",
    "        self.gs += 1\n",
    "        with EMetrics.open() as em:\n",
    "            em.record(EMetrics.TEST_GROUP,completed_batch,{'loss': loss, 'accuracy': acc})\n",
    "        with ELog.open() as log:\n",
    "            log.recordTrain(\"Train\", completed_batch, self.gs, loss, acc, worker)\n",
    "\n",
    "    def log_test_metrics(self, loss, acc, completed_batch, worker=0):\n",
    "        acc = acc/100.0\n",
    "        with ELog.open() as log:\n",
    "            log.recordTest(\"Test\", loss, acc, worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "537ad0d4-0a40-485c-8560-b48b80de903b"
   },
   "source": [
    "### EDT helper scripts: elog.py\n",
    "The elog.py script defines the path and content of the training and test log. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "85cacfdf-9e9e-4b1c-aba0-8bf7a250fb28",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./resnet-wmla/elog.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {model_dir}/{model_elog}\n",
    "import time\n",
    "import os\n",
    "\n",
    "'''\n",
    "    ELog class define the path and content of train and test log.\n",
    "'''\n",
    "\n",
    "class ELog(object):\n",
    "\n",
    "    def __init__(self,subId,f):\n",
    "        if \"TRAINING_ID\" in os.environ:\n",
    "            self.trainingId = os.environ[\"TRAINING_ID\"]\n",
    "        elif \"DLI_EXECID\" in os.environ:\n",
    "            self.trainingId = os.environ[\"DLI_EXECID\"]\n",
    "        else:\n",
    "            self.trainingId = \"\"\n",
    "        self.subId = subId\n",
    "        self.f = f\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, tb):\n",
    "        self.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def open(subId=None):\n",
    "        if \"LOG_DIR\" in os.environ:\n",
    "            folder = os.environ[\"LOG_DIR\"]\n",
    "        elif \"JOB_STATE_DIR\" in os.environ:\n",
    "            folder = os.path.join(os.environ[\"JOB_STATE_DIR\"],\"logs\")\n",
    "        else:\n",
    "            folder = \"/tmp\"\n",
    "\n",
    "        if subId is not None:\n",
    "            folder = os.path.join(folder, subId)\n",
    "\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "\n",
    "        f = open(os.path.join(folder, \"stdout\"), \"a\")\n",
    "        return ELog(subId,f)\n",
    "\n",
    "    def recordText(self,text):\n",
    "        timestr = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "        timestr = \"[\"+ timestr + \"]\"\n",
    "        if self.f:\n",
    "            self.f.write(timestr + \" \" + text + \"\\n\")\n",
    "            self.f.flush()\n",
    "\n",
    "    def recordTrain(self,title,iteration,global_steps,loss,accuracy,worker):\n",
    "        text = title\n",
    "        text = text + \",\tTimestamp: \" + str(int(round(time.time() * 1000)))\n",
    "        text = text + \",\tGlobal steps: \" + str(global_steps)\n",
    "        text = text + \",\tIteration: \" + str(iteration)\n",
    "        text = text + \",\tLoss: \" + str(float('%.5f' % loss) )\n",
    "        text = text + \",\tAccuracy: \" + str(float('%.5f' % accuracy) )\n",
    "        self.recordText(text)\n",
    "\n",
    "    def recordTest(self,title,loss,accuracy,worker):\n",
    "        text = title\n",
    "        text = text + \",\tTimestamp: \" + str(int(round(time.time() * 1000)))\n",
    "        text = text + \",\tLoss: \" + str(float('%.5f' % loss) )\n",
    "        text = text + \",\tAccuracy: \" + str(float('%.5f' % accuracy) )\n",
    "        self.recordText(text)\n",
    "\n",
    "    def close(self):\n",
    "        if self.f:\n",
    "            self.f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2c77818b-dee7-461c-b948-0f8be09f2995",
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Package model files for training\n",
    "import requests, json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "# from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "%matplotlib inline\n",
    "# plt.rcParams['figure.figsize'] = [24, 8.0]\n",
    "#import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', 999)\n",
    "pd.set_option('max_colwidth', 300)\n",
    "\n",
    "import tarfile\n",
    "import tempfile\n",
    "import os\n",
    "#Package the updated model files into a tar file ending with `.modelDir.tar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "07b2dd85-dd71-4e33-9e11-e25bf69a98b2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tempFile: /tmp/1000660000/tmpkt1d_zkg.modelDir.tar\n"
     ]
    }
   ],
   "source": [
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
    "\n",
    "\n",
    "MODEL_DIR_SUFFIX = \".modelDir.tar\"\n",
    "tempFile = tempfile.mktemp(MODEL_DIR_SUFFIX)\n",
    "\n",
    "make_tarfile(tempFile, model_dir)\n",
    "\n",
    "print(\" tempFile: \" + tempFile)\n",
    "files = {'file': open(tempFile, 'rb')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9e80dec4-f966-42c6-b1bb-1156294b0542"
   },
   "source": [
    "## Set up API end point and log on\n",
    "[Back to top](#Contents)\n",
    "\n",
    "In this section we set up the API endpoint which will be used in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3e93025-273b-48bf-be9f-0124e8783f2a"
   },
   "source": [
    "The following sections use the Watson ML Accelerator API to complete the various tasks required. \n",
    "We've given examples of a number of tasks but you should refer to the documentation at to see more details \n",
    "of what is possible and sample output you might expect.\n",
    "\n",
    "- https://www.ibm.com/support/knowledgecenter/SSFHA8_2.2.0/cm/deeplearning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ad1716c6-bd75-411a-8d45-a28435dd016b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests, json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "# from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "%matplotlib inline\n",
    "# plt.rcParams['figure.figsize'] = [24, 8.0]\n",
    "#import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', 999)\n",
    "pd.set_option('max_colwidth', 300)\n",
    "\n",
    "import tarfile\n",
    "import tempfile\n",
    "import os\n",
    "import base64\n",
    "import urllib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8180b7f4-f1e6-480d-816b-b6da815363a3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d2VuZHk6UGFzc3cwcmQyMDIx\n",
      "https://wmla-console-wmla-ns.apps.cpolab.ibm.com/auth/v1/logon\n",
      "eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6IkViSEpBdDhiVTNIUW1HeGw5ZmVJMXRuMWhkQTBSekV6TUNIZ3lsOTdfMVUifQ.eyJ1c2VybmFtZSI6IndlbmR5Iiwic3ViIjoid2VuZHkiLCJpc3MiOiJLTk9YU1NPIiwiYXVkIjoiRFNYIiwicm9sZSI6IkFkbWluIiwicGVybWlzc2lvbnMiOlsiYWRtaW5pc3RyYXRvciIsImNhbl9wcm92aXNpb24iLCJtYW5hZ2VfY2F0YWxvZyIsImNyZWF0ZV9wcm9qZWN0IiwiY3JlYXRlX3NwYWNlIiwibWFuYWdlX3F1YWxpdHkiLCJtYW5hZ2VfaW5mb3JtYXRpb25fYXNzZXRzIiwibWFuYWdlX2Rpc2NvdmVyeSIsIm1hbmFnZV9tZXRhZGF0YV9pbXBvcnQiLCJhdXRob3JfZ292ZXJuYW5jZV9hcnRpZmFjdHMiLCJtYW5hZ2VfZ292ZXJuYW5jZV93b3JrZmxvdyIsInZpZXdfZ292ZXJuYW5jZV9hcnRpZmFjdHMiLCJtYW5hZ2VfY2F0ZWdvcmllcyIsImFjY2Vzc19jYXRhbG9nIiwidmlld19xdWFsaXR5Il0sImdyb3VwcyI6WzEwMDAwXSwidWlkIjoiMTAwMDMzMTAwMiIsImF1dGhlbnRpY2F0b3IiOiJkZWZhdWx0IiwiZGlzcGxheV9uYW1lIjoiV2VuZHkgV2FuZyIsImNhbl9yZWZyZXNoX3VudGlsIjoxNjM4Mzc2MTEzNzgyLCJjc3JmX3Rva2VuIjoiZmI5ZmJjNDMwZDFlZTljYWQ1OTdmMzBmM2UwOTllMGIiLCJzZXNzaW9uX2lkIjoiMzIzMjlmMTUtZTc3Ny00ZjE3LWE2MDYtNzMwMzM2MWQzMmE1IiwiaWF0IjoxNjM4MzMyOTQ5LCJleHAiOjE2MzgzNzYxNDl9.WwwPcUpOU9Fl-7dtc4MQrutLGdGWMVQshHejvRUgwYWwI1yK2H5ZyajGatXmDq9B_DBwsXgqRpLctWRGaae7l-TcBo1pOTHv0OwOR5H2CeHs9ExBt0TytfNwAfK3dUBlHV6ZN-wxRuePBHg4DO1qm2iZihsbbsHgG7Bgl1LaySs5DOMadOxdEk5XNa-wvVO36uR_p8T9t4yHVIVRNgmwUR1vX5J5wJ3P-61VcdAxYGRK_GTOIkGZa6dwngG-Y3qVjVmmDu5IyRF1nNRfdeAbiNMjvPTOzb7CyhorSqqpKdtibr3R7vM7MraZPbJPTrpBVCqgFsxjZSowPfVzKttwKw\n"
     ]
    }
   ],
   "source": [
    "#hostname='wmla-console-wmla.apps.cpd35-beta.cpolab.ibm.com'  # please enter Watson Machine Learning Accelerator host name\n",
    "#hostname = 'wmla-console-liqbj.apps.wml1x210.ma.platformlab.ibm.com'\n",
    "hostname = 'wmla-console-wmla-ns.apps.cpolab.ibm.com'\n",
    "login='wendy:Passw0rd2021' # please enter the login and password\n",
    "# hostname='wmla-console-xwmla.apps.wml1x180.ma.platformlab.ibm.com'\n",
    "# login='admin:password'\n",
    "\n",
    "es = base64.b64encode(login.encode('utf-8')).decode(\"utf-8\")\n",
    "print(es)\n",
    "commonHeaders={'Authorization': 'Basic '+es}\n",
    "req = requests.Session()\n",
    "auth_url = 'https://{}/auth/v1/logon'.format(hostname)\n",
    "print(auth_url)\n",
    "a=requests.get(auth_url,headers=commonHeaders, verify=False)\n",
    "access_token=a.json()['accessToken']\n",
    "print(access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2d5546f5-0eb4-408c-b1fc-8412b83f1572"
   },
   "source": [
    "### Log on\n",
    "\n",
    "\n",
    "Obtain login session tokens to be used for session authentication within the RESTful API. Tokens are valid for 8 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "e4ee5397-f346-463a-a913-454dd4498110",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dl_rest_url = 'https://{}/platform/rest/deeplearning/v1'.format(hostname)\n",
    "commonHeaders={'accept': 'application/json', 'X-Auth-Token': access_token}\n",
    "req = requests.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beff5485-7bc1-4030-87b6-ddfcd0a76390"
   },
   "source": [
    "### Check deep learning framework details\n",
    "\n",
    "Check what framework plugins are available and see example execution commands.  In this demonstration we will use **edtPyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bd96fc05-9bb0-4965-a8b2-4be56ce5f1ef",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"name\": \"edtPyTorch\",\n",
      "        \"description\": \"\",\n",
      "        \"desc\": [\n",
      "            \"PyTorch - IBM Elastic Distributed Training (EDT)\",\n",
      "            \"Examples:\",\n",
      "            \"$ python dlicmd.py --exec-start edtPyTorch <connection-options> --cs-datastore-meta type=fs,data_path=mnist --model-main mnist.py\",\n",
      "            \"\",\n",
      "            \"Prebuilt Models:\",\n",
      "            \"  $ python dlicmd.py --exec-start edtPyTorch <connection-options> <prebuilt-model-params>\",\n",
      "            \"\",\n",
      "            \"  where:\",\n",
      "            \"    <prebuilt-model-params>:\",\n",
      "            \"      --pbmodel-cmd <command>: <command> is 'train'\",\n",
      "            \"        Specify 'train' to train the prebuilt models below\",\n",
      "            \"      --pbmodel-name <name>:\",\n",
      "            \"        For 'train', <name> is 'MNIST'\",\n",
      "            \"      --epochs. Optional for 'train'. Default 10\",\n",
      "            \"      --batch-size. Optional for 'train'. Default 20\",\n",
      "            \"      --lr. Learning rate. Optional for 'train'. Default 0.01\",\n",
      "            \"      --momentum. Optional for 'train'. Default 0.9\",\n",
      "            \"  Examples:\",\n",
      "            \"    $ python dlicmd.py --exec-start edtPyTorch --rest-host localhost --numWorker 2 --pbmodel-cmd train --pbmodel-name MNIST --epochs 10 --batch-size 12\"\n",
      "        ],\n",
      "        \"deployMode\": \"cluster\",\n",
      "        \"hasTask0\": \"\",\n",
      "        \"execMode\": \"distributed\",\n",
      "        \"appName\": \"ElasticPyTorchTrain\",\n",
      "        \"frameworkCmdGenerator\": \"edtCmdGen.pyc\",\n",
      "        \"backend\": \"pytorch\",\n",
      "        \"prebuiltModelMain\": \"PyTorchPrebuiltMain.pyc\",\n",
      "        \"prebuiltModelDir\": \"edtPyTorch\",\n",
      "        \"frameworkVersion\": \"1.7.1\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"tensorflow\",\n",
      "        \"description\": \"\",\n",
      "        \"desc\": [\n",
      "            \"Single-node TensorFlow\",\n",
      "            \"NOTES:\",\n",
      "            \"- Since DLI manages GPU allocation, if you explicitly assign devices using\",\n",
      "            \"  calls such as `tf.device`, you should use Tensorflow configuration flag\",\n",
      "            \"  `allow_soft_placement=True`\",\n",
      "            \"\",\n",
      "            \"Prebuilt Models:\",\n",
      "            \"  $ python dlicmd.py --exec-start tensorflow <connection-options> <prebuilt-model-params>\",\n",
      "            \"\",\n",
      "            \"  where:\",\n",
      "            \"    <prebuilt-model-params>:\",\n",
      "            \"      --pbmodel-cmd <command>: <command> is 'train'\",\n",
      "            \"        Specify 'train' to train the prebuilt models\",\n",
      "            \"      --pbmodel-name <name>:\",\n",
      "            \"        For 'train', <name> can be 'MNIST'\",\n",
      "            \"      --epochs. Optional for 'train'. Default 10\",\n",
      "            \"      --batch-size. Optional for 'train'. Default 20\",\n",
      "            \"      --lr. Learning rate. Optional for 'train'. Default 0.01\",\n",
      "            \"      --momentum. Optional for 'train'. Default 0.9\",\n",
      "            \"  Examples:\",\n",
      "            \"    $ python dlicmd.py --exec-start tensorflow --rest-host localhost --pbmodel-cmd train --pbmodel-name MNIST --epochs 10 --batch-size 12\",\n",
      "            \"Examples:\",\n",
      "            \"$ python dlicmd.py --exec-start tensorflow <connection-options> --cs-datastore-meta type=fs,data_path=mnist --model-main mnist.py\"\n",
      "        ],\n",
      "        \"deployMode\": \"cluster\",\n",
      "        \"hasTask0\": \"\",\n",
      "        \"execMode\": \"single\",\n",
      "        \"appName\": \"SingleNodeTensorflowTrain\",\n",
      "        \"numWorkers\": 1,\n",
      "        \"maxWorkers\": 1,\n",
      "        \"frameworkCmdGenerator\": \"tensorflowCmdGen.pyc\",\n",
      "        \"backend\": \"TensorFlow\",\n",
      "        \"prebuiltModelMain\": \"tensorflowPrebuiltMain.pyc\",\n",
      "        \"prebuiltModelDir\": \"tensorflow\",\n",
      "        \"frameworkVersion\": \"2.4.3\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"edtTensorflow\",\n",
      "        \"description\": \"\",\n",
      "        \"desc\": [\n",
      "            \"Tensorflow - IBM Elastic Distributed Training (EDT)\",\n",
      "            \"Examples:\",\n",
      "            \"$ python dlicmd.py --exec-start edtTf1x <connection-options> --cs-datastore-meta type=fs,data_path=mnist --model-main mnist.py\"\n",
      "        ],\n",
      "        \"deployMode\": \"cluster\",\n",
      "        \"hasTask0\": \"\",\n",
      "        \"execMode\": \"distributed\",\n",
      "        \"appName\": \"ElasticTensorflowTrain\",\n",
      "        \"frameworkCmdGenerator\": \"edtCmdGen.pyc\",\n",
      "        \"backend\": \"tensorflow\",\n",
      "        \"frameworkVersion\": \"2.4.3\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"disttensorflow\",\n",
      "        \"description\": \"\",\n",
      "        \"desc\": [\n",
      "            \"Distributed TensorFlow\",\n",
      "            \"Instead of passing parameters such as ps_hosts, worker_hosts, specify --numPs\",\n",
      "            \"as in example below. Parameter servers (ps) and worker hosts will be allocated\",\n",
      "            \"dynamically.\",\n",
      "            \"Use --gpuPerWorker flag to specify number of GPUs per worker.\",\n",
      "            \"The maximum number of worker is 2.\",\n",
      "            \"The maximum number of GPUs per worker is 2.\",\n",
      "            \"Examples:\",\n",
      "            \"$ python dlicmd.py --exec-start disttensorflow <connection-options> --cs-datastore-meta type=fs,data_path=mnist --model-main mnist.py --numPs 1 --numWorker 1 --gpuPerWorker 1\",\n",
      "            \"$ python dlicmd.py --exec-start disttensorflow <connection-options> --cs-datastore-meta type=fs,data_path=mnist --model-main mnist.py --numPs 1 --numWorker 2 --gpuPerWorker 1\"\n",
      "        ],\n",
      "        \"deployMode\": \"cluster\",\n",
      "        \"hasTask0\": \"false\",\n",
      "        \"execMode\": \"distributed\",\n",
      "        \"appName\": \"DistributeTensorflow2xTrain\",\n",
      "        \"numWorkers\": 1,\n",
      "        \"frameworkCmdGenerator\": \"disttensorflowCmdGen.pyc\",\n",
      "        \"distributeStrategy\": \"MultiWorkerMirroredStrategy\",\n",
      "        \"numPs\": 1,\n",
      "        \"backend\": \"TensorFlow\",\n",
      "        \"frameworkVersion\": \"2.4.3\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"distPyTorch\",\n",
      "        \"description\": \"\",\n",
      "        \"desc\": [\n",
      "            \"Distributed PyTorch\",\n",
      "            \"You only have to specify the number of workers as shown in the example below,\",\n",
      "            \"and the following environment variables will be available when the model runs:\",\n",
      "            \"MASTER_ADDR, MASTER_PORT, WORLD_SIZE, RANK\",\n",
      "            \"Use --gpuPerWorker flag to specify number of GPUs per worker.\",\n",
      "            \"The maximum number of worker is 1024.\",\n",
      "            \"The maximum number of GPUs per worker is 2.\",\n",
      "            \"Examples:\",\n",
      "            \"$ python dlicmd.py --exec-start distPyTorch <connection-options> --model-main mnist.py --numWorker 2 --gpuPerWorker 1\",\n",
      "            \"\",\n",
      "            \"Prebuilt Models:\",\n",
      "            \"  $ python dlicmd.py --exec-start PyTorch <connection-options> <prebuilt-model-params>\",\n",
      "            \"\",\n",
      "            \"  where:\",\n",
      "            \"    <prebuilt-model-params>:\",\n",
      "            \"      --pbmodel-cmd <command>: <command> is 'train'\",\n",
      "            \"        Specify 'train' to train the prebuilt models below with fake data\",\n",
      "            \"      --pbmodel-name <name>:\",\n",
      "            \"        For 'train', <name> can be either 'AlexNet', 'VGG16', or 'ResNet18'\",\n",
      "            \"      --epochs. Optional for 'train'. Default 10\",\n",
      "            \"      --batch-size. Optional for 'train'. Default 20\",\n",
      "            \"      --lr. Learning rate. Optional for 'train'. Default 0.01\",\n",
      "            \"      --momentum. Optional for 'train'. Default 0.9\",\n",
      "            \"  Examples:\",\n",
      "            \"    $ python dlicmd.py --exec-start distPyTorch --rest-host localhost --numWorker 2 --pbmodel-cmd train --pbmodel-name AlexNet --epochs 10 --batch-size 12\"\n",
      "        ],\n",
      "        \"deployMode\": \"cluster\",\n",
      "        \"hasTask0\": \"false\",\n",
      "        \"execMode\": \"distributed\",\n",
      "        \"appName\": \"DistributePyTorchTrain\",\n",
      "        \"exports\": {\n",
      "            \"GLOO_SOCKET_IFNAME\": \"eth0\"\n",
      "        },\n",
      "        \"numWorkers\": 1,\n",
      "        \"frameworkCmdGenerator\": \"distPyTorchCmdGen.pyc\",\n",
      "        \"backend\": \"PyTorch\",\n",
      "        \"prebuiltModelMain\": \"PyTorchPrebuiltMain.pyc\",\n",
      "        \"prebuiltModelDir\": \"PyTorch\",\n",
      "        \"frameworkVersion\": \"1.7.1\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"PyTorch\",\n",
      "        \"description\": \"\",\n",
      "        \"desc\": [\n",
      "            \"PyTorch\",\n",
      "            \"Examples:\",\n",
      "            \"$ python dlicmd.py --exec-start PyTorch <connection-options> --cs-datastore-meta type=fs,data_path=mnist --model-main mnist.py\",\n",
      "            \"\",\n",
      "            \"Prebuilt Models:\",\n",
      "            \"  $ python dlicmd.py --exec-start PyTorch <connection-options> <prebuilt-model-params>\",\n",
      "            \"\",\n",
      "            \"  where:\",\n",
      "            \"    <prebuilt-model-params>:\",\n",
      "            \"      --pbmodel-cmd <command>: <command> is 'train'\",\n",
      "            \"        Specify 'train' to train the prebuilt models below with fake data\",\n",
      "            \"      --pbmodel-name <name>:\",\n",
      "            \"        For 'train', <name> can be either 'AlexNet', 'VGG16', or 'ResNet18'\",\n",
      "            \"      --epochs. Optional for 'train'. Default 10\",\n",
      "            \"      --batch-size. Optional for 'train'. Default 20\",\n",
      "            \"      --lr. Learning rate. Optional for 'train'. Default 0.01\",\n",
      "            \"      --momentum. Optional for 'train'. Default 0.9\",\n",
      "            \"  Examples:\",\n",
      "            \"    $ python dlicmd.py --exec-start PyTorch --rest-host localhost --pbmodel-cmd train --pbmodel-name AlexNet --epochs 10 --batch-size 12\"\n",
      "        ],\n",
      "        \"deployMode\": \"cluster\",\n",
      "        \"hasTask0\": \"\",\n",
      "        \"execMode\": \"single\",\n",
      "        \"appName\": \"SingleNodePytorchTrain\",\n",
      "        \"numWorkers\": 1,\n",
      "        \"maxWorkers\": 1,\n",
      "        \"frameworkCmdGenerator\": \"PyTorchCmdGen.pyc\",\n",
      "        \"backend\": \"PyTorch\",\n",
      "        \"prebuiltModelMain\": \"PyTorchPrebuiltMain.pyc\",\n",
      "        \"prebuiltModelDir\": \"PyTorch\",\n",
      "        \"frameworkVersion\": \"1.7.1\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(dl_rest_url+'/execs/frameworks', headers=commonHeaders, verify=False).json()\n",
    "# Using the raw json, easier to see the examples given\n",
    "print(json.dumps(r, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e76666b7-a071-4519-a6cc-25cef0cd53ad"
   },
   "source": [
    "## Submit job via API\n",
    "[Back to top](#Contents)\n",
    "\n",
    "Now we need to structure our API job submission. There are various elements to this process as seen in the diagram below. Note that **this** Jupyter notebook is the one referred to below. A [static version](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/shared-images/4_api_setup.png) is also available.\n",
    "\n",
    "![code](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/shared-images/4_api_setup.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cc83c579-0f06-46ab-a33e-8c35deeb12da"
   },
   "source": [
    "framework_name = 'edtPyTorch' # DL Framework to use, from list given above\n",
    "local_dir_containing_your_code = 'resnet-wmla'\n",
    "number_of_GPU = '2' # number of GPUs for elastic distribution\n",
    "name_of_your_code_file = 'elastic-main.py' # Main model file as opened locally above\n",
    "\n",
    "\n",
    "args = '--exec-start {} \\\n",
    "        --cs-datastore-meta type=fs\\\n",
    "        --model-dir {} \\\n",
    "        --numWorker={} \\\n",
    "        --model-main {} \\\n",
    "        '.format(framework_name, local_dir_containing_your_code, number_of_GPU, name_of_your_code_file)\n",
    "\n",
    "print (\"args: \" + args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "889c26ff-603e-4495-b5a7-bbf2511da9c0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tempFile: /tmp/1000660000/tmppozce2dn.modelDir.tar\n"
     ]
    }
   ],
   "source": [
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
    "\n",
    "\n",
    "MODEL_DIR_SUFFIX = \".modelDir.tar\"\n",
    "tempFile = tempfile.mktemp(MODEL_DIR_SUFFIX)\n",
    "\n",
    "make_tarfile(tempFile, model_dir)\n",
    "\n",
    "print(\" tempFile: \" + tempFile)\n",
    "files = {'file': open(tempFile, 'rb')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "bc124fcb-9810-4cb6-b10a-fd7688a351e4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args: --exec-start edtPyTorch --cs-datastore-meta type=fs  --numWorker 1 --workerDeviceNum 2         --workerMemory 16g --model-main elastic-main.py --model-dir resnet-wmla\n"
     ]
    }
   ],
   "source": [
    "framework_name = 'edtPyTorch' # DL Framework to use, from list given above\n",
    "#dataset_location = 'pytorch-mnist' # relative path of your data set under $DLI_DATA_FS\n",
    "local_dir_containing_your_code = 'resnet-wmla'\n",
    "number_of_GPUs_per_worker = '2' \n",
    "number_of_workers_max = '1'\n",
    "name_of_your_code_file = 'elastic-main.py' # Main model file as opened locally above\n",
    "worker_memory = '16g'\n",
    "\n",
    "args = f'--exec-start {framework_name} --cs-datastore-meta type=fs  --numWorker {number_of_workers_max} --workerDeviceNum {number_of_GPUs_per_worker}\\\n",
    "         --workerMemory {worker_memory} --model-main {name_of_your_code_file} --model-dir {local_dir_containing_your_code}'\n",
    "\n",
    "    \n",
    "print (\"args: \" + args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f309d31e-6236-4482-ab3d-b3145586a48a"
   },
   "source": [
    "## Monitor running job\n",
    "[Back to top](#Contents)\n",
    "\n",
    "Once the job is submitted successfully we can monitor the running job. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "602f7c42-4ac5-46be-8ff2-a7f3162b3d0d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing every 5 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>args</th>\n",
       "      <th>submissionId</th>\n",
       "      <th>creator</th>\n",
       "      <th>state</th>\n",
       "      <th>appId</th>\n",
       "      <th>schedulerUrl</th>\n",
       "      <th>modelFileOwnerName</th>\n",
       "      <th>workDir</th>\n",
       "      <th>appName</th>\n",
       "      <th>createTime</th>\n",
       "      <th>elastic</th>\n",
       "      <th>nameSpace</th>\n",
       "      <th>numWorker</th>\n",
       "      <th>framework</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wmla-ns-21</td>\n",
       "      <td>--exec-start edtPyTorch --cs-datastore-meta type=fs  --numWorker 1 --workerDeviceNum 2         --workerMemory 16g --...</td>\n",
       "      <td>wmla-ns-21</td>\n",
       "      <td>wendy</td>\n",
       "      <td>LAUNCHING</td>\n",
       "      <td>wmla-ns-21</td>\n",
       "      <td>https://wmla-mss:9080</td>\n",
       "      <td>wmla</td>\n",
       "      <td>/gpfs/myresultfs/wendy/batchworkdir/wmla-ns-21/_submitted_code/resnet-wmla</td>\n",
       "      <td>ElasticPyTorchTrain</td>\n",
       "      <td>2021-12-01T05:03:15Z</td>\n",
       "      <td>True</td>\n",
       "      <td>wmla-ns</td>\n",
       "      <td>1</td>\n",
       "      <td>edtPyTorch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  \\\n",
       "0  wmla-ns-21   \n",
       "\n",
       "                                                                                                                      args  \\\n",
       "0  --exec-start edtPyTorch --cs-datastore-meta type=fs  --numWorker 1 --workerDeviceNum 2         --workerMemory 16g --...   \n",
       "\n",
       "  submissionId creator      state       appId           schedulerUrl  \\\n",
       "0   wmla-ns-21   wendy  LAUNCHING  wmla-ns-21  https://wmla-mss:9080   \n",
       "\n",
       "  modelFileOwnerName  \\\n",
       "0               wmla   \n",
       "\n",
       "                                                                      workDir  \\\n",
       "0  /gpfs/myresultfs/wendy/batchworkdir/wmla-ns-21/_submitted_code/resnet-wmla   \n",
       "\n",
       "               appName            createTime  elastic nameSpace  numWorker  \\\n",
       "0  ElasticPyTorchTrain  2021-12-01T05:03:15Z     True   wmla-ns          1   \n",
       "\n",
       "    framework  \n",
       "0  edtPyTorch  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'appId': 'wmla-ns-21',\n",
      "  'appName': 'ElasticPyTorchTrain',\n",
      "  'args': '--exec-start edtPyTorch --cs-datastore-meta type=fs  --numWorker 1 '\n",
      "          '--workerDeviceNum 2         --workerMemory 16g --model-main '\n",
      "          'elastic-main.py --model-dir resnet-wmla ',\n",
      "  'createTime': '2021-12-01T05:03:15Z',\n",
      "  'creator': 'wendy',\n",
      "  'elastic': True,\n",
      "  'framework': 'edtPyTorch',\n",
      "  'id': 'wmla-ns-21',\n",
      "  'modelFileOwnerName': 'wmla',\n",
      "  'nameSpace': 'wmla-ns',\n",
      "  'numWorker': 1,\n",
      "  'schedulerUrl': 'https://wmla-mss:9080',\n",
      "  'state': 'LAUNCHING',\n",
      "  'submissionId': 'wmla-ns-21',\n",
      "  'workDir': '/gpfs/myresultfs/wendy/batchworkdir/wmla-ns-21/_submitted_code/resnet-wmla'}\n"
     ]
    }
   ],
   "source": [
    "r = requests.post(dl_rest_url+'/execs?args='+args, files=files, \n",
    "                  headers=commonHeaders, verify=False)\n",
    "\n",
    "\n",
    "if not r.ok:\n",
    "    print('submit job failed: code=%s, %s'%(r.status_code, r.content))\n",
    "\n",
    "\n",
    "job_status = query_job_status(r.json(),refresh_rate=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2893375-3b42-4f98-8c2d-539575002006"
   },
   "source": [
    "## Training metrics and logs\n",
    "\n",
    "#### Retrieve and display the model training metrics:\n",
    "[Back to top](#Contents)\n",
    "\n",
    "After the job completes then we can retrieve the output, logs and saved models. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "9168ef55-0e21-4e15-afac-6657af4bcb2a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wmla-console-wmla-ns.apps.cpolab.ibm.com/platform/rest/deeplearning/v1/scheduler/applications/wmla-ns-20/executor/1/logs/stdout?lastlines=1000\n",
      "<> DATA_DIR /gpfs/mydatafs/<> DATA_DIR /gpfs/mydatafs/\n",
      "\n",
      "<> RESULT_DIR /gpfs/myresultfs/wendy/batchworkdir/wmla-ns-20\n",
      "<> RESULT_DIR /gpfs/myresultfs/wendy/batchworkdir/wmla-ns-20\n",
      "<> LOG_DIR /gpfs/myresultfs/wendy/batchworkdir/wmla-ns-20/log\n",
      "<> SAVED_MODEL_DIR /gpfs/myresultfs/wendy/batchworkdir/wmla-ns-20/model<> LOG_DIR /gpfs/myresultfs/wendy/batchworkdir/wmla-ns-20/log\n",
      "\n",
      "<> SAVED_MODEL_DIR /gpfs/myresultfs/wendy/batchworkdir/wmla-ns-20/model\n",
      "libunformatworker.so loaded successfully\n",
      "libunformatworker.so loaded successfully\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "UnformatedEseWorker created.\n",
      "work ptr is: 0x55cad73f3740\n",
      "LRScheduleCallback epoch=0, learning_rate=0.4\n",
      "UnformatedEseWorker created.\n",
      "work ptr is: 0x55cad7879630\n",
      "LRScheduleCallback epoch=0, learning_rate=0.4\n",
      "handler: pull weight over storage\n",
      "handler: push weight over storage\n",
      "Timestamp 1638333266045, Iteration 1\n",
      "batches :1 7.049193382263184\n",
      "Iteration 3: tag train_accuracy, simple_value 0.0\n",
      "Iteration 3: tag train_loss, simple_value 7.04919\n",
      "Timestamp 1638333266264, Iteration 2\n",
      "batches :2 6.913943767547607\n",
      "Iteration 4: tag train_accuracy, simple_value 0.0\n",
      "Iteration 4: tag train_loss, simple_value 6.91394\n",
      "Timestamp 1638333266481, Iteration 3\n",
      "batches :3 7.03933048248291\n",
      "Iteration 5: tag train_accuracy, simple_value 0.0\n",
      "Iteration 5: tag train_loss, simple_value 7.03933\n",
      "Timestamp 1638333266696, Iteration 4\n",
      "batches :4 6.96364688873291\n",
      "Iteration 6: tag train_accuracy, simple_value 0.0\n",
      "Iteration 6: tag train_loss, simple_value 6.96365\n",
      "Timestamp 1638333266906, Iteration 5\n",
      "batches :5 6.945241928100586\n",
      "Iteration 7: tag train_accuracy, simple_value 0.0\n",
      "Iteration 7: tag train_loss, simple_value 6.94524\n",
      "Timestamp 1638333267119, Iteration 6\n",
      "batches :6 6.985552787780762\n",
      "Iteration 8: tag train_accuracy, simple_value 0.0\n",
      "Iteration 8: tag train_loss, simple_value 6.98555\n",
      "Timestamp 1638333267326, Iteration 7\n",
      "batches :7 7.05058479309082\n",
      "Iteration 9: tag train_accuracy, simple_value 0.0\n",
      "Iteration 9: tag train_loss, simple_value 7.05058\n",
      "Timestamp 1638333267618, Iteration 8\n",
      "batches :8 6.918735504150391\n",
      "Iteration 10: tag train_accuracy, simple_value 0.0\n",
      "Iteration 10: tag train_loss, simple_value 6.91874\n",
      "Timestamp 1638333267835, Iteration 9\n",
      "batches :9 7.0123724937438965\n",
      "Iteration 11: tag train_accuracy, simple_value 0.0\n",
      "Iteration 11: tag train_loss, simple_value 7.01237\n",
      "Timestamp 1638333268041, Iteration 10\n",
      "batches :10 7.032445907592773\n",
      "Iteration 12: tag train_accuracy, simple_value 0.0\n",
      "Iteration 12: tag train_loss, simple_value 7.03245\n",
      "Timestamp 1638333268248, Iteration 11\n",
      "batches :11 6.930966377258301\n",
      "Iteration 13: tag train_accuracy, simple_value 0.0\n",
      "Iteration 13: tag train_loss, simple_value 6.93097\n",
      "Timestamp 1638333268465, Iteration 12\n",
      "batches :12 7.114292621612549\n",
      "Iteration 14: tag train_accuracy, simple_value 0.0\n",
      "Iteration 14: tag train_loss, simple_value 7.11429\n",
      "Timestamp 1638333268675, Iteration 13\n",
      "batches :13 7.038506031036377\n",
      "Iteration 15: tag train_accuracy, simple_value 0.0\n",
      "Iteration 15: tag train_loss, simple_value 7.03851\n",
      "Timestamp 1638333268930, Iteration 14\n",
      "batches :14 7.179214954376221\n",
      "Iteration 16: tag train_accuracy, simple_value 0.0\n",
      "Iteration 16: tag train_loss, simple_value 7.17921\n",
      "wmla-ns-20-task12n-2rz9v:377:377 [0] NCCL INFO Bootstrap : Using [0]eth0:10.128.8.242<0>\n",
      "wmla-ns-20-task12n-2rz9v:377:377 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\n",
      "wmla-ns-20-task12n-2rz9v:377:377 [0] NCCL INFO NET/IB : No device found.\n",
      "wmla-ns-20-task12n-2rz9v:377:377 [0] NCCL INFO NET/Socket : Using [0]eth0:10.128.8.242<0>\n",
      "wmla-ns-20-task12n-2rz9v:377:377 [0] NCCL INFO Using network Socket\n",
      "wmla-ns-20-task12n-2rz9v:376:376 [0] NCCL INFO Bootstrap : Using [0]eth0:10.128.8.242<0>\n",
      "wmla-ns-20-task12n-2rz9v:376:376 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\n",
      "wmla-ns-20-task12n-2rz9v:376:376 [0] NCCL INFO NET/IB : No device found.\n",
      "wmla-ns-20-task12n-2rz9v:376:376 [0] NCCL INFO NET/Socket : Using [0]eth0:10.128.8.242<0>\n",
      "wmla-ns-20-task12n-2rz9v:376:376 [0] NCCL INFO Using network Socket\n",
      "NCCL version 2.7.8+cuda11.0\n",
      "wmla-ns-20-task12n-2rz9v:376:376 [0] NCCL INFO Channel 00/02 :    0   1\n",
      "wmla-ns-20-task12n-2rz9v:376:376 [0] NCCL INFO Channel 01/02 :    0   1\n",
      "wmla-ns-20-task12n-2rz9v:377:377 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64\n",
      "wmla-ns-20-task12n-2rz9v:377:377 [0] NCCL INFO Trees [0] -1/-1/-1->1->0|0->1->-1/-1/-1 [1] -1/-1/-1->1->0|0->1->-1/-1/-1\n",
      "wmla-ns-20-task12n-2rz9v:376:376 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64\n",
      "wmla-ns-20-task12n-2rz9v:376:376 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1|-1->0->1/-1/-1 [1] 1/-1/-1->0->-1|-1->0->1/-1/-1\n",
      "wmla-ns-20-task12n-2rz9v:377:377 [0] NCCL INFO Channel 00 : 1[1b000] -> 0[13000] via P2P/IPC\n",
      "wmla-ns-20-task12n-2rz9v:376:376 [0] NCCL INFO Channel 00 : 0[13000] -> 1[1b000] via P2P/IPC\n",
      "\n",
      "wmla-ns-20-task12n-2rz9v:377:377 [0] transport/p2p.cc:238 NCCL WARN failed to open CUDA IPC handle : 217 peer access is not supported between these two devices\n",
      "wmla-ns-20-task12n-2rz9v:377:377 [0] NCCL INFO transport.cc:68 -> 1\n",
      "wmla-ns-20-task12n-2rz9v:377:377 [0] NCCL INFO init.cc:766 -> 1\n",
      "wmla-ns-20-task12n-2rz9v:377:377 [0] NCCL INFO init.cc:840 -> 1\n",
      "wmla-ns-20-task12n-2rz9v:377:377 [0] NCCL INFO init.cc:876 -> 1\n",
      "wmla-ns-20-task12n-2rz9v:377:377 [0] NCCL INFO init.cc:887 -> 1\n",
      "\n",
      "wmla-ns-20-task12n-2rz9v:376:376 [0] transport/p2p.cc:238 NCCL WARN failed to open CUDA IPC handle : 217 peer access is not supported between these two devices\n",
      "wmla-ns-20-task12n-2rz9v:376:376 [0] NCCL INFO transport.cc:68 -> 1\n",
      "wmla-ns-20-task12n-2rz9v:376:376 [0] NCCL INFO init.cc:766 -> 1\n",
      "wmla-ns-20-task12n-2rz9v:376:376 [0] NCCL INFO init.cc:840 -> 1\n",
      "wmla-ns-20-task12n-2rz9v:376:376 [0] NCCL INFO init.cc:876 -> 1\n",
      "wmla-ns-20-task12n-2rz9v:376:376 [0] NCCL INFO init.cc:887 -> 1\n",
      "intput worker ptr is: 0x55cad7879630\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 16 reduce Alpha = 0.0625\n",
      "Before ddl allreduce\n",
      "\n",
      "wmla-ns-20-task12n-2rz9v:376:376 [0] misc/argcheck.cc:30 NCCL WARN AllReduce : comm argument is NULL\n",
      "wmla-ns-20-task12n-2rz9v:376:376 [0] NCCL INFO enqueue.cc:555 -> 4\n",
      "\n",
      "wmla-ns-20-task12n-2rz9v:376:376 [0] misc/argcheck.cc:30 NCCL WARN AllReduce : comm argument is NULL\n",
      "wmla-ns-20-task12n-2rz9v:376:376 [0] NCCL INFO enqueue.cc:555 -> 4\n",
      "DDL failure: 1\n",
      "intput worker ptr is: 0x55cad73f3740\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 16 reduce Alpha = 0.0625\n",
      "Before ddl allreduce\n",
      "\n",
      "wmla-ns-20-task12n-2rz9v:377:377 [0] misc/argcheck.cc:30 NCCL WARN AllReduce : comm argument is NULL\n",
      "wmla-ns-20-task12n-2rz9v:377:377 [0] NCCL INFO enqueue.cc:555 -> 4\n",
      "\n",
      "wmla-ns-20-task12n-2rz9v:377:377 [0] misc/argcheck.cc:30 NCCL WARN AllReduce : comm argument is NULL\n",
      "wmla-ns-20-task12n-2rz9v:377:377 [0] NCCL INFO enqueue.cc:555 -> 4\n",
      "DDL failure: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_executor_stdout_log(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d48050c6-da3a-4350-9328-90d05a721c47"
   },
   "source": [
    "## Download trained model from Watson Machine Learning Accelerator \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "45e81331-63f8-4239-80f1-f3191be98965",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wmla-console-wmla-ns.apps.cpolab.ibm.com/platform/rest/deeplearning/v1/execs/wmla-ns-12/result\n",
      "Save model:  ./resnet-wmla/wmla-ns-12.zip\n"
     ]
    }
   ],
   "source": [
    "download_trained_model(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58e90fb7-94fd-4a3e-988e-e864a1c85e2f"
   },
   "source": [
    "## Further information and useful links\n",
    "[Back to top](#Contents)\n",
    "\n",
    "**WML Accelerator Introduction videos:**\n",
    "- WML Accelerator overview video (1 minute): http://ibm.biz/wmla-video\n",
    "- Overview of adapting your code for Elastic Distributed Training via API: [video](https://youtu.be/RnZtYNX6meM) | [PDF](docs/wmla_api_pieces.pdf) (screenshot below)\n",
    "\n",
    "**Further WML Accelerator information & documentation**\n",
    "- [Learning path: Get started with Watson Machine Learning Accelerator](http://ibm.biz/wmla-learning-path)\n",
    "- [IBM Documentation on Watson Machine Learning Accelerator](https://www.ibm.com/docs/en/wmla/2.2.0)\n",
    "- [Blog: Expert Q&A: Accelerate deep learning on IBM Cloud Pak for Data](https://www.ibm.com/blogs/journey-to-ai/2020/10/expert-qa-accelerate-deep-learning-on-ibm-cloud-pak-for-data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "605142fc-5745-45ce-b4d7-ac6f4f5bb7c5"
   },
   "source": [
    "## Appendix\n",
    "[Back to top](#Contents)\n",
    "\n",
    "\n",
    "#### This is version 1.0 and its content is copyright of IBM.   All rights reserved.   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a84e3f7e-71f8-402e-89ec-81a2ed43633f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 + GPU with applications",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
