{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02ea1f6f090f46e5966468336006c630"
   },
   "source": [
    "# Accelerate Deep Learning Model training with Watson Machine Learning Accelerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bc04e069c7c4c86ac640afdecdefbad"
   },
   "source": [
    "### Notebook created by Kelvin Lui,  Xue Yin Zhuang in Jan 2021\n",
    "\n",
    "### In this notebook, you will learn how to use the Watson Machine Learning Accelerator (WML-A) API and accelerate deep learning model training on GPU with Watson Machine Learning Accelerator.\n",
    "\n",
    "This notebook uses the PyTorch Resnet18 model, which performs image classification using a basic computer vision image classification example. The model will be trained both on CPU and GPU to demonstrate that training models on GPU hardware deliver faster result times.\n",
    "\n",
    "\n",
    "This notebook covers the following sections:\n",
    "\n",
    "1. [Setting up required packages](#setup)<br>\n",
    "\n",
    "2. [Configuring your environment and project details](#configure)<br>\n",
    "\n",
    "3. [Training the model on CPU](#cpu)<br>\n",
    "\n",
    "4. [Training the model on GPU with Watson Machine Learning Accelerator](#gpu)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fcde3deab74495e871e4da0f83fdfac"
   },
   "source": [
    "<a id = \"setup\"></a>\n",
    "## Step 1: Setting up required packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4020a53992743179053ecb345e637a4"
   },
   "source": [
    "#### First, install torchvision which is required to train the PyTorch Resnet18 model on CPU.\n",
    "Note: You will need to create a custom environment with 16VCPU and 32GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bd69fac3990e4ef28a38a29b0b83b614",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (0.8.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from torchvision) (1.19.2)\n",
      "Requirement already satisfied: torch in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from torchvision) (1.7.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from torchvision) (8.3.1)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from torch->torchvision) (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "199618d2bbe1488d8820cd8a62402e99",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01c07e0d7cc44bf787719d848a58c1f3"
   },
   "source": [
    "#### Next, define helper methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "975a773eb5f7406085b30b08bc3d9922",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# import tarfile\n",
    "import tempfile\n",
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import pandas as pd\n",
    "from IPython.display import display, FileLink, clear_output\n",
    "\n",
    "import requests\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%pylab inline\n",
    "\n",
    "import base64\n",
    "import json\n",
    "import time\n",
    "import urllib\n",
    "import tarfile\n",
    "\n",
    "\n",
    "def query_job_status(job_id,refresh_rate=3) :\n",
    "\n",
    "    execURL = dl_rest_url  +'/execs/'+ job_id['id']\n",
    "    pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "    keep_running=True\n",
    "    res=None\n",
    "    while(keep_running):\n",
    "        res = req.get(execURL, headers=commonHeaders, verify=False)\n",
    "        monitoring = pd.DataFrame(res.json(), index=[0])\n",
    "        pd.set_option('max_colwidth', 120)\n",
    "        clear_output()\n",
    "        print(\"Refreshing every {} seconds\".format(refresh_rate))\n",
    "        display(monitoring)\n",
    "        pp.pprint(res.json())\n",
    "        if(res.json()['state'] not in ['PENDING_CRD_SCHEDULER', 'SUBMITTED','RUNNING']) :\n",
    "            keep_running=False\n",
    "        time.sleep(refresh_rate)\n",
    "    return res\n",
    "\n",
    "def query_executor_stdout_log(job_id) :\n",
    "\n",
    "    execURL = dl_rest_url  +'/scheduler/applications/'+ job_id['id'] + '/executor/1/logs/stdout?lastlines=1000'\n",
    "    #'https://{}/platform/rest/deeplearning/v1/scheduler/applications/wmla-267/driver/logs/stderr?lastlines=10'.format(hostname)\n",
    "    commonHeaders2={'accept': 'text/plain', 'X-Auth-Token': access_token}\n",
    "    print (execURL)\n",
    "    res = req.get(execURL, headers=commonHeaders2, verify=False)\n",
    "    print(res.text)\n",
    "    \n",
    "    \n",
    "def query_train_metric(job_id) :\n",
    "\n",
    "    #execURL = dl_rest_url  +'/execs/'+ job_id['id'] + '/log'\n",
    "    execURL = dl_rest_url  +'/execs/'+ job_id['id'] + '/log'\n",
    "    #'https://{}/platform/rest/deeplearning/v1/scheduler/applications/wmla-267/driver/logs/stderr?lastlines=10'.format(hostname)\n",
    "    commonHeaders2={'accept': 'text/plain', 'X-Auth-Token': access_token}\n",
    "    print (execURL)\n",
    "    res = req.get(execURL, headers=commonHeaders2, verify=False)\n",
    "    print(res.text)\n",
    "\n",
    "    # save result file    \n",
    "def download_trained_model(job_id) :\n",
    "\n",
    "    from IPython.display import display, FileLink\n",
    "\n",
    "    # save result file\n",
    "    commonHeaders3={'accept': 'application/octet-stream', 'X-Auth-Token': access_token}\n",
    "    execURL = dl_rest_url  +'/execs/'+ r.json()['id'] + '/result'\n",
    "    res = req.get(execURL, headers=commonHeaders3, verify=False, stream=True)\n",
    "    print (execURL)\n",
    "\n",
    "    tmpfile = '/project_data/data_asset/' +  r.json()['id'] +'.zip'\n",
    "    print ('Save model: ', tmpfile )\n",
    "    with open(tmpfile,'wb') as f:\n",
    "        f.write(res.content)\n",
    "        f.close()\n",
    "\n",
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41e72dbf695c4f258de5e6d85685c7c2"
   },
   "source": [
    "<a id = \"configure\"></a>\n",
    "## Step 2: Configuring your environment and project details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fe8c33b48a9a4f10aeb34eabc2e1f1d3"
   },
   "source": [
    "To set up your project details, provide your credentials in this cell. You must include your cluster URL, username, and password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4fa3be661bab44b58d8e86d383a4df65",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d2VuZHk6UGFzc3cwcmQyMDIx\n",
      "https://wmla-console-wmla-ns.apps.cpolab.ibm.com/auth/v1/logon\n",
      "eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6IkViSEpBdDhiVTNIUW1HeGw5ZmVJMXRuMWhkQTBSekV6TUNIZ3lsOTdfMVUifQ.eyJ1c2VybmFtZSI6IndlbmR5Iiwic3ViIjoid2VuZHkiLCJpc3MiOiJLTk9YU1NPIiwiYXVkIjoiRFNYIiwicm9sZSI6IkFkbWluIiwicGVybWlzc2lvbnMiOlsiYWRtaW5pc3RyYXRvciIsImNhbl9wcm92aXNpb24iLCJtYW5hZ2VfY2F0YWxvZyIsImNyZWF0ZV9wcm9qZWN0IiwiY3JlYXRlX3NwYWNlIiwibWFuYWdlX3F1YWxpdHkiLCJtYW5hZ2VfaW5mb3JtYXRpb25fYXNzZXRzIiwibWFuYWdlX2Rpc2NvdmVyeSIsIm1hbmFnZV9tZXRhZGF0YV9pbXBvcnQiLCJhdXRob3JfZ292ZXJuYW5jZV9hcnRpZmFjdHMiLCJtYW5hZ2VfZ292ZXJuYW5jZV93b3JrZmxvdyIsInZpZXdfZ292ZXJuYW5jZV9hcnRpZmFjdHMiLCJtYW5hZ2VfY2F0ZWdvcmllcyIsImFjY2Vzc19jYXRhbG9nIiwidmlld19xdWFsaXR5Il0sImdyb3VwcyI6WzEwMDAwXSwidWlkIjoiMTAwMDMzMTAwMiIsImF1dGhlbnRpY2F0b3IiOiJkZWZhdWx0IiwiZGlzcGxheV9uYW1lIjoiV2VuZHkgV2FuZyIsImNhbl9yZWZyZXNoX3VudGlsIjoxNjM4NDI5NzM1MjM4LCJjc3JmX3Rva2VuIjoiNWVmZThkYzcwNTYyMmU1ZDM5NTg2ZWU5MTE2MTlhOGYiLCJzZXNzaW9uX2lkIjoiZDVhYWM4ZTQtYjhjNC00ZDYxLWI2NDktYWYzODEzZmFlYjM4IiwiaWF0IjoxNjM4Mzg2NTcxLCJleHAiOjE2Mzg0Mjk3NzF9.hYsFO9T8U1rMKlMr6TVNQ1vLzr_SqEJf12uzCdr5hG_8Qp2N8BULFewlUiPOkRjPlTkitPWCIdcvAlrYsKri3034Rq6UD7lZgn_cVQQojfWIQA9vTc_-yfFNPr4ouEuWiB0xSbSwI5puYA505KyaaNerK0FeRS3wJMpzQtRtFvc_gafL_3ROua16IXcHXUxMvKDvNG1HreyfiXiJ7ZUCJTMB-AMaPikdPLzELDQZjd1hHbkvn4l5GncUITHeUfAk-4aIkCo3OVl3r9siggM-KYnK84IpfYnEWHrHtVABB9hcQu2P_3gw8iEPWcmBl9XHQznE_5wUUyPRikXLAcCEpw\n"
     ]
    }
   ],
   "source": [
    "hostname='wmla-console-wmla-ns.apps.cpolab.ibm.com'  # please enter Watson Machine Learning Accelerator host name\n",
    "login='wendy:Passw0rd2021' # please enter the login and password\n",
    "es = base64.b64encode(login.encode('utf-8')).decode(\"utf-8\")\n",
    "print(es)\n",
    "commonHeaders={'Authorization': 'Basic '+es}\n",
    "req = requests.Session()\n",
    "auth_url = 'https://{}/auth/v1/logon'.format(hostname)\n",
    "print(auth_url)\n",
    "a=requests.get(auth_url,headers=commonHeaders, verify=False)\n",
    "access_token=a.json()['accessToken']\n",
    "print(access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "24d476b80c6c4f7d8abe927b3656e1de",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dl_rest_url = 'https://{}/platform/rest/deeplearning/v1'.format(hostname)\n",
    "commonHeaders={'accept': 'application/json', 'X-Auth-Token': access_token}\n",
    "req = requests.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de65166a99d0485aa885f938ffcec89f"
   },
   "source": [
    "<a id = \"cpu\"></a>\n",
    "## Step 3: Training the model on CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8aea74018e6d4c26920e02a00637f4c6"
   },
   "source": [
    "#### Prepare the model files for running on CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3e3692cc27324c4680410dffcec5cd73",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "DATA_DIR='/project_data/data_asset/pytorch-resnet/data'\n",
    "RESULT_DIR='/project_data/data_asset/pytorch-resnet/result'\n",
    "model_dir = f'/project_data/data_asset/pytorch-resnet/resnet' \n",
    "model_main = f'main.py'\n",
    "model_resnet = f'resnet.py'\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "a89ecd024ebf4acf9338ea0157415a61",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /project_data/data_asset/pytorch-resnet/resnet/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {model_dir}/{model_main}\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # Image Classification Using PyTorch Resnet with Watson Machine Learning Accelerator Notebook\n",
    "# This asset details the process of performing a basic computer vision image classification example using the notebook functionality within Watson Machine Learning Accelerator. In this asset, you will learn how to accelerate your training with pytorch resnet model upon the cifar10 dataset.\n",
    "#\n",
    "# Please refer to [Resnet Introduction](https://arxiv.org/abs/1512.03385) for more details.\n",
    "\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.models as models\n",
    "#from resnet import resnet18\n",
    "import time\n",
    "import numpy\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "\n",
    "log_interval = 10\n",
    "\n",
    "seed = 1\n",
    "use_cuda = False\n",
    "completed_batch =0\n",
    "completed_test_batch =0\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Tensorflow MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=32, metavar='N',\n",
    "                    help='input batch size for training (default: 128)')\n",
    "parser.add_argument('--epochs', type=int, default=5, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "args = parser.parse_args()\n",
    "print(args)\n",
    "\n",
    "\n",
    "# ## Create the Resnet18 model\n",
    "print(\"Use cuda: \", use_cuda)\n",
    "\n",
    "# ## Download the Cifar10 dataset\n",
    "# If you set download=True, the CIFAR-10 [CIFAR-10 python version](https://www.cs.toronto.edu/~kriz/cifar.html) dataset is automatically downloaded and used by the Notebook. \n",
    "# If you want to use a different dataset or have previously downloaded a dataset, \n",
    "# set download=False and specify the directory that contains the dataset\n",
    "\n",
    "# An exmpale to dowload the CIFAR-10 dataset:\n",
    "# > mkdir ${DATA_DIR}/cifar10\n",
    "# > cd ${DATA_DIR}/cifar10\n",
    "# > wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "# > tar -zxf cifar-10-python.tar.gz\n",
    "\n",
    "\n",
    "DATA_DIR='/project_data/data_asset/pytorch-resnet/data'\n",
    "RESULT_DIR='/project_data/data_asset/pytorch-resnet/result'\n",
    "model_dir = f'/project_data/data_asset/pytorch-resnet/resnet' \n",
    "\n",
    "def getDatasets():\n",
    "    train_data_dir = DATA_DIR + '/cifar10'\n",
    "    test_data_dir = DATA_DIR + '/cifar10'\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        #transforms.RandomCrop(self.resolution, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    return (torchvision.datasets.CIFAR10(root=train_data_dir, train=True, download=True, transform = transform_train),\n",
    "            torchvision.datasets.CIFAR10(root=test_data_dir, train=False, download=True, transform = transform_test)\n",
    "            )\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print ('device:', device)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "train_dataset, test_dataset = getDatasets()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "# ## Implement the customized train and test loop\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    global completed_batch\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "\n",
    "        completed_batch += 1\n",
    "\n",
    "        print ('Train - batches : {}, average loss: {:.4f}, accuracy: {}/{} ({:.0f}%)'.format(\n",
    "           completed_batch, train_loss/(batch_idx+1), correct, total, 100.*correct/total))\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, epoch):\n",
    "    global completed_test_batch\n",
    "    global completed_batch\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    completed_test_batch = completed_batch -  len(test_loader)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            test_loss += loss.item() # sum up batch loss\n",
    "            _, pred = output.max(1) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "            completed_test_batch += 1\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "    # Output test info for per epoch\n",
    "    print('Test - batches: {}, average loss: {:.4f}, accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        completed_batch, test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "# ## Create the Resnet18 model\n",
    "#use_cuda = not args.no_cuda\n",
    "print(\"Use cuda: \", use_cuda)\n",
    "\n",
    "\n",
    "model_type = \"resnet18\"\n",
    "print(\"=> using pytorch build-in model '{}'\".format(model_type))\n",
    "\n",
    "model = models.resnet18()\n",
    "#model = models.resnet50()\n",
    "\n",
    "\n",
    "# Using pytorch built-in resnet18 model, the model is pre-trained on the ImageNet dataset,\n",
    "# which has 1000 classifications. To transfer it to cifar10 dataset, we can modify the last fully-connected layer output size to 10\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True  # set False if you only want to train the last layer using pretrained model\n",
    "    # Replace the last fully-connected layer\n",
    "    # Parameters of newly constructed modules have requires_grad=True by default\n",
    "    model.fc = nn.Linear(512, 10)\n",
    "\n",
    "\n",
    "# (Optional) To use wmla pretrained resnet18 model for cifar10, load the model weight file. The pretrained model weight file can be downloaded [here](https://?).\n",
    "\n",
    "weightfile = DATA_DIR + \"/checkpoint/model_epoch_final.pth\"\n",
    "if os.path.exists(weightfile):\n",
    "    print (\"Initial weight file is \" + weightfile)\n",
    "    model.load_state_dict(torch.load(weightfile, map_location=lambda storage, loc: storage))\n",
    "\n",
    "\n",
    "# ## Run the model trainings\n",
    "#print(model)\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "epochs = args.epochs\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, 30, 0.1, last_epoch=-1)\n",
    "\n",
    "# Output total iterations info for deep learning insights\n",
    "print(\"Total iterations: %s\" % (len(train_loader) * epochs))\n",
    "\n",
    "#print(\"RESULT_DIR: \" + os.getenv(\"RESULT_DIR\"))\n",
    "#RESULT_DIR = os.getenv(\"RESULT_DIR\")\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(\"\\nRunning epoch %s ... It might take several minutes for each epoch to run.\" % epoch)\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader, epoch)\n",
    "    scheduler.step()\n",
    "\n",
    "    torch.save(model.state_dict(),  RESULT_DIR + \"/model_epoch_%d.pth\"%(epoch))\n",
    "\n",
    "torch.save(model.state_dict(), RESULT_DIR + \"/model_epoch_final.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4f3ad43f9efb42a68057fac608bd21c8"
   },
   "source": [
    "## Training results on CPU\n",
    "\n",
    "#### Training was run from a Cloud Pak for Data Notebook utilizing a CPU kernel. \n",
    "\n",
    "\n",
    "In the custom environment that was created with **16vCPU** and **32GB**, it took **1560 seconds** (or approximately **26 minutes**) to complete 1 EPOCH training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "789d930b39b14945bf4fa4850b0d2475",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=32, cuda=False, epochs=1, lr=0.01)\n",
      "Use cuda:  False\n",
      "device: cpu\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /project_data/data_asset/pytorch-resnet/data/cifar10/cifar-10-python.tar.gz\n",
      "100%|███████████████████████▉| 170237952/170498071 [00:45<00:00, 2229642.38it/s]Extracting /project_data/data_asset/pytorch-resnet/data/cifar10/cifar-10-python.tar.gz to /project_data/data_asset/pytorch-resnet/data/cifar10\n",
      "Files already downloaded and verified\n",
      "Use cuda:  False\n",
      "=> using pytorch build-in model 'resnet18'\n",
      "Total iterations: 1563\n",
      "\n",
      "Running epoch 1 ... It might take several minutes for each epoch to run.\n",
      "Train - batches : 1, average loss: 2.4301, accuracy: 2/32 (6%)\n",
      "Train - batches : 2, average loss: 2.4637, accuracy: 6/64 (9%)\n",
      "170500096it [01:00, 2229642.38it/s]                                             Train - batches : 3, average loss: 2.4328, accuracy: 9/96 (9%)\n",
      "Train - batches : 4, average loss: 2.4104, accuracy: 12/128 (9%)\n",
      "Train - batches : 5, average loss: 2.3993, accuracy: 15/160 (9%)\n",
      "Train - batches : 6, average loss: 2.3674, accuracy: 21/192 (11%)\n",
      "Train - batches : 7, average loss: 2.3450, accuracy: 27/224 (12%)\n",
      "Train - batches : 8, average loss: 2.3538, accuracy: 29/256 (11%)\n",
      "Train - batches : 9, average loss: 2.3450, accuracy: 33/288 (11%)\n",
      "Train - batches : 10, average loss: 2.3344, accuracy: 37/320 (12%)\n",
      "Train - batches : 11, average loss: 2.3275, accuracy: 42/352 (12%)\n",
      "Train - batches : 12, average loss: 2.3182, accuracy: 47/384 (12%)\n",
      "Train - batches : 13, average loss: 2.3157, accuracy: 53/416 (13%)\n",
      "Train - batches : 14, average loss: 2.3114, accuracy: 57/448 (13%)\n",
      "Train - batches : 15, average loss: 2.3047, accuracy: 61/480 (13%)\n",
      "Train - batches : 16, average loss: 2.3076, accuracy: 64/512 (12%)\n",
      "Train - batches : 17, average loss: 2.3080, accuracy: 68/544 (12%)\n",
      "Train - batches : 18, average loss: 2.3040, accuracy: 73/576 (13%)\n",
      "Train - batches : 19, average loss: 2.3039, accuracy: 77/608 (13%)\n",
      "Train - batches : 20, average loss: 2.3013, accuracy: 85/640 (13%)\n",
      "Train - batches : 21, average loss: 2.2980, accuracy: 90/672 (13%)\n",
      "Train - batches : 22, average loss: 2.2915, accuracy: 97/704 (14%)\n",
      "Train - batches : 23, average loss: 2.2908, accuracy: 104/736 (14%)\n",
      "Train - batches : 24, average loss: 2.2838, accuracy: 109/768 (14%)\n",
      "Train - batches : 25, average loss: 2.2793, accuracy: 113/800 (14%)\n",
      "Train - batches : 26, average loss: 2.2771, accuracy: 116/832 (14%)\n",
      "Train - batches : 27, average loss: 2.2731, accuracy: 124/864 (14%)\n",
      "Train - batches : 28, average loss: 2.2709, accuracy: 131/896 (15%)\n",
      "Train - batches : 29, average loss: 2.2655, accuracy: 138/928 (15%)\n",
      "Train - batches : 30, average loss: 2.2593, accuracy: 144/960 (15%)\n",
      "Train - batches : 31, average loss: 2.2537, accuracy: 153/992 (15%)\n",
      "Train - batches : 32, average loss: 2.2493, accuracy: 159/1024 (16%)\n",
      "Train - batches : 33, average loss: 2.2427, accuracy: 166/1056 (16%)\n",
      "Train - batches : 34, average loss: 2.2373, accuracy: 175/1088 (16%)\n",
      "Train - batches : 35, average loss: 2.2328, accuracy: 184/1120 (16%)\n",
      "Train - batches : 36, average loss: 2.2318, accuracy: 189/1152 (16%)\n",
      "Train - batches : 37, average loss: 2.2281, accuracy: 193/1184 (16%)\n",
      "Train - batches : 38, average loss: 2.2259, accuracy: 201/1216 (17%)\n",
      "Train - batches : 39, average loss: 2.2227, accuracy: 208/1248 (17%)\n",
      "Train - batches : 40, average loss: 2.2219, accuracy: 214/1280 (17%)\n",
      "Train - batches : 41, average loss: 2.2213, accuracy: 218/1312 (17%)\n",
      "Train - batches : 42, average loss: 2.2175, accuracy: 223/1344 (17%)\n",
      "Train - batches : 43, average loss: 2.2121, accuracy: 229/1376 (17%)\n",
      "Train - batches : 44, average loss: 2.2096, accuracy: 237/1408 (17%)\n",
      "Train - batches : 45, average loss: 2.2056, accuracy: 248/1440 (17%)\n",
      "Train - batches : 46, average loss: 2.2060, accuracy: 253/1472 (17%)\n",
      "Train - batches : 47, average loss: 2.2011, accuracy: 262/1504 (17%)\n",
      "Train - batches : 48, average loss: 2.1970, accuracy: 272/1536 (18%)\n",
      "Train - batches : 49, average loss: 2.1955, accuracy: 280/1568 (18%)\n",
      "Train - batches : 50, average loss: 2.1929, accuracy: 286/1600 (18%)\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/project_data/data_asset/pytorch-resnet/resnet/main.py\", line 209, in <module>\n",
      "    train(model, device, train_loader, optimizer, epoch)\n",
      "  File \"/project_data/data_asset/pytorch-resnet/resnet/main.py\", line 117, in train\n",
      "    output = model(data)\n",
      "  File \"/opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages/torchvision/models/resnet.py\", line 220, in forward\n",
      "    return self._forward_impl(x)\n",
      "  File \"/opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages/torchvision/models/resnet.py\", line 208, in _forward_impl\n",
      "    x = self.layer1(x)\n",
      "  File \"/opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages/torch/nn/modules/container.py\", line 117, in forward\n",
      "    input = module(input)\n",
      "  File \"/opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages/torchvision/models/resnet.py\", line 63, in forward\n",
      "    out = self.conv2(out)\n",
      "  File \"/opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages/torch/nn/modules/conv.py\", line 423, in forward\n",
      "    return self._conv_forward(input, self.weight)\n",
      "  File \"/opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages/torch/nn/modules/conv.py\", line 420, in _conv_forward\n",
      "    self.padding, self.dilation, self.groups)\n",
      "KeyboardInterrupt\n",
      "170500096it [03:46, 752584.63it/s] \n",
      "Training cost:  227  seconds.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "starttime = datetime.datetime.now()\n",
    "\n",
    "! python /project_data/data_asset/pytorch-resnet/resnet/main.py --epochs 1 \n",
    "\n",
    "endtime = datetime.datetime.now()\n",
    "print(\"Training cost: \", (endtime - starttime).seconds, \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bd858908a5714a46881266091e1442a1"
   },
   "source": [
    "<a id = \"gpu\"></a>\n",
    "## Step 4: Training the model on GPU with Watson Machine Learning Accelerator\n",
    "\n",
    "#### Prepare the model files for running on GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "b7cf774caa0c4902a2bd0719b5b955d0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "model_dir = f'/project_data/data_asset/pytorch-resnet/resnet-wmla' \n",
    "model_main = f'main.py'\n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "64a1d4d199a9464aa0c589f9791eb66e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /project_data/data_asset/pytorch-resnet/resnet-wmla/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {model_dir}/{model_main}\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # Image Classification Using PyTorch Resnet with Watson Machine Learning Accelerator Notebook\n",
    "# This asset details the process of performing a basic computer vision image classification example using the notebook functionality within Watson Machine Learning Accelerator. In this asset, you will learn how to accelerate your training with pytorch resnet model upon the cifar10 dataset.\n",
    "#\n",
    "# Please refer to [Resnet Introduction](https://arxiv.org/abs/1512.03385) for more details.\n",
    "\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.models as models\n",
    "import time\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "\n",
    "log_interval = 10\n",
    "\n",
    "seed = 1\n",
    "use_cuda = False\n",
    "completed_batch =0\n",
    "completed_test_batch =0\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Tensorflow MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
    "                    help='input batch size for training (default: 128)')\n",
    "parser.add_argument('--epochs', type=int, default=1, metavar='N',\n",
    "                    help='number of epochs to train (default: 1)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('cuda', action='store_true', default=True,\n",
    "                    help='enables CUDA training')\n",
    "args = parser.parse_args()\n",
    "print(args)\n",
    "\n",
    "\n",
    "# ## Create the Resnet18 model\n",
    "use_cuda = args.cuda\n",
    "print(\"Use cuda: \", use_cuda)\n",
    "\n",
    "# ## Download the Cifar10 dataset\n",
    "# If you set download=True, the CIFAR-10 [CIFAR-10 python version](https://www.cs.toronto.edu/~kriz/cifar.html) dataset is automatically downloaded and used by the Notebook. \n",
    "# If you want to use a different dataset or have previously downloaded a dataset, \n",
    "# set download=False and specify the directory that contains the dataset\n",
    "\n",
    "# An exmpale to dowload the CIFAR-10 dataset:\n",
    "# > mkdir ${DATA_DIR}/cifar10\n",
    "# > cd ${DATA_DIR}/cifar10\n",
    "# > wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "# > tar -zxf cifar-10-python.tar.gz\n",
    "\n",
    "print(\"DATA_DIR: \" + os.getenv(\"DATA_DIR\"))\n",
    "DATA_DIR = os.getenv(\"DATA_DIR\")\n",
    "\n",
    "def getDatasets():\n",
    "    train_data_dir = DATA_DIR + \"/cifar10\"\n",
    "    test_data_dir = DATA_DIR + \"/cifar10\"\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        #transforms.RandomCrop(self.resolution, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    return (torchvision.datasets.CIFAR10(root=train_data_dir, train=True, download=True, transform = transform_train),\n",
    "            torchvision.datasets.CIFAR10(root=test_data_dir, train=False, download=True, transform = transform_test)\n",
    "            )\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "train_dataset, test_dataset = getDatasets()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "# ## Implement the customized train and test loop\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    global completed_batch\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "\n",
    "        completed_batch += 1\n",
    "\n",
    "        print ('Train - batches : {}, average loss: {:.4f}, accuracy: {}/{} ({:.0f}%)'.format(\n",
    "           completed_batch, train_loss/(batch_idx+1), correct, total, 100.*correct/total))\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, epoch):\n",
    "    global completed_test_batch\n",
    "    global completed_batch\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    completed_test_batch = completed_batch -  len(test_loader)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            test_loss += loss.item() # sum up batch loss\n",
    "            _, pred = output.max(1) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "            completed_test_batch += 1\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "    # Output test info for per epoch\n",
    "    print('Test - batches: {}, average loss: {:.4f}, accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        completed_batch, test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "# ## Create the Resnet18 model\n",
    "\n",
    "model_type = \"resnet18\"\n",
    "#model_type = \"resnet50\"\n",
    "print(\"=> using pytorch build-in model '{}'\".format(model_type))\n",
    "\n",
    "model = models.resnet18()\n",
    "\n",
    "\n",
    "# Using pytorch build-in resnet18 model, the model is pre-trained on the ImageNet dataset,\n",
    "# which has 1000 classifications. To transfer it to cifar10 dataset, we can modify the last fully-connected layer output size to 10\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True  # set False if you only want to train the last layer using pretrained model\n",
    "    # Replace the last fully-connected layer\n",
    "    # Parameters of newly constructed modules have requires_grad=True by default\n",
    "    model.fc = nn.Linear(512, 10)\n",
    "\n",
    "\n",
    "# (Optional) To use wmla pretrained resnet18 model for cifar10, load the model weight file. The pretrained model weight file can be downloaded [here](https://?).\n",
    "\n",
    "weightfile = DATA_DIR + \"/checkpoint/model_epoch_final.pth\"\n",
    "if os.path.exists(weightfile):\n",
    "    print (\"Initial weight file is \" + weightfile)\n",
    "    model.load_state_dict(torch.load(weightfile, map_location=lambda storage, loc: storage))\n",
    "\n",
    "\n",
    "# ## Run the model trainings\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "epochs = args.epochs\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, 30, 0.1, last_epoch=-1)\n",
    "\n",
    "# Output total iterations info for deep learning insights\n",
    "print(\"Total iterations: %s\" % (len(train_loader) * epochs))\n",
    "\n",
    "print(\"RESULT_DIR: \" + os.getenv(\"RESULT_DIR\"))\n",
    "RESULT_DIR = os.getenv(\"RESULT_DIR\")\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(\"\\nRunning epoch %s ... It might take several minutes for each epoch to run.\" % epoch)\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader, epoch)\n",
    "    scheduler.step()\n",
    "\n",
    "    torch.save(model.state_dict(),  RESULT_DIR + \"/model/model_epoch_%d.pth\"%(epoch))\n",
    "\n",
    "torch.save(model.state_dict(), RESULT_DIR + \"/model/model_epoch_final.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "512529be85064a589196e893c24cc1e8"
   },
   "source": [
    "## Training results on GPU\n",
    "\n",
    "#### Training was run from a Cloud Pak for Data Notebook utilizing a GPU kernel. \n",
    "\n",
    "\n",
    "In the custom environment that was created with **16vCPU** and **32GB**, it took **147seconds** (or approximately **2.5 minutes**) to complete 1 EPOCH training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "80e85d2d-ddd3-4c3f-8d8b-67ca9498b402",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://wmla-console-wmla-ns.apps.cpolab.ibm.com/platform/rest/deeplearning/v1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_rest_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "9b428dc3dcef44bf94f7afe59e68f5d8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "files = {'file': open('/project_data/data_asset/pytorch-resnet/resnet-wmla/main.py', 'rb')}\n",
    "\n",
    "args = '--exec-start PyTorch --cs-datastore-meta type=fs \\\n",
    "                     --numWorker 2 \\\n",
    "                     --workerDeviceNum 1 \\\n",
    "                     --model-main main.py --epochs 1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "12744932929149de91df0af4d20bfd50",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit job failed: code=500, b'Error 500: Error: [Error]: Maximum number of workers is 1. \\n [Error]: Maximum number of workers is 1\\n'\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a6ea5c48832b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submit job failed: code=%s, %s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mjob_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrefresh_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    898\u001b[0m                     \u001b[0;31m# used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/Python-3.7-CUDA/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/Python-3.7-CUDA/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/Python-3.7-CUDA/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "starttime = datetime.datetime.now()\n",
    "\n",
    "r = requests.post(dl_rest_url+'/execs?args='+args, files=files,\n",
    "                  headers=commonHeaders, verify=False)\n",
    "if not r.ok:\n",
    "    print('submit job failed: code=%s, %s'%(r.status_code, r.content))\n",
    "    \n",
    "job_status = query_job_status(r.json(),refresh_rate=5)\n",
    "\n",
    "endtime = datetime.datetime.now()\n",
    "\n",
    "print(\"\\nTraining cost: \", (endtime - starttime).seconds, \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cdf692a2d9740209a8484619f409a0a"
   },
   "source": [
    "## Training metrics and logs\n",
    "\n",
    "#### Retrieve and display the model training metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "28bd037ad580460aa3545c65c19c47dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wmla-console-wmla.apps.cpd35-beta.cpolab.ibm.com/platform/rest/deeplearning/v1/execs/wmla-327/log\n",
      "Namespace(batch_size=128, cuda=True, epochs=1, lr=0.01)\n",
      "Use cuda:  True\n",
      "DATA_DIR: /gpfs/mydatafs\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "=> using pytorch build-in model 'resnet18'\n",
      "Total iterations: 391\n",
      "RESULT_DIR: /gpfs/myresultfs/dse_user/batchworkdir/wmla-327\n",
      "\n",
      "Running epoch 1 ... It might take several minutes for each epoch to run.\n",
      "Train - batches : 1, average loss: 2.4147, accuracy: 15/128 (12%)\n",
      "Train - batches : 2, average loss: 2.3836, accuracy: 27/256 (11%)\n",
      "Train - batches : 3, average loss: 2.3746, accuracy: 40/384 (10%)\n",
      "Train - batches : 4, average loss: 2.3545, accuracy: 54/512 (11%)\n",
      "Train - batches : 5, average loss: 2.3369, accuracy: 71/640 (11%)\n",
      "Train - batches : 6, average loss: 2.3242, accuracy: 95/768 (12%)\n",
      "Train - batches : 7, average loss: 2.3176, accuracy: 110/896 (12%)\n",
      "Train - batches : 8, average loss: 2.3134, accuracy: 127/1024 (12%)\n",
      "Train - batches : 9, average loss: 2.3101, accuracy: 139/1152 (12%)\n",
      "Train - batches : 10, average loss: 2.3040, accuracy: 159/1280 (12%)\n",
      "Train - batches : 11, average loss: 2.3012, accuracy: 174/1408 (12%)\n",
      "Train - batches : 12, average loss: 2.2949, accuracy: 196/1536 (13%)\n",
      "Train - batches : 13, average loss: 2.2906, accuracy: 213/1664 (13%)\n",
      "Train - batches : 14, average loss: 2.2838, accuracy: 240/1792 (13%)\n",
      "Train - batches : 15, average loss: 2.2752, accuracy: 276/1920 (14%)\n",
      "Train - batches : 16, average loss: 2.2711, accuracy: 298/2048 (15%)\n",
      "Train - batches : 17, average loss: 2.2682, accuracy: 322/2176 (15%)\n",
      "Train - batches : 18, average loss: 2.2607, accuracy: 356/2304 (15%)\n",
      "Train - batches : 19, average loss: 2.2536, accuracy: 389/2432 (16%)\n",
      "Train - batches : 20, average loss: 2.2498, accuracy: 414/2560 (16%)\n",
      "Train - batches : 21, average loss: 2.2437, accuracy: 444/2688 (17%)\n",
      "Train - batches : 22, average loss: 2.2398, accuracy: 471/2816 (17%)\n",
      "Train - batches : 23, average loss: 2.2339, accuracy: 511/2944 (17%)\n",
      "Train - batches : 24, average loss: 2.2304, accuracy: 541/3072 (18%)\n",
      "Train - batches : 25, average loss: 2.2269, accuracy: 566/3200 (18%)\n",
      "Train - batches : 26, average loss: 2.2214, accuracy: 602/3328 (18%)\n",
      "Train - batches : 27, average loss: 2.2155, accuracy: 639/3456 (18%)\n",
      "Train - batches : 28, average loss: 2.2104, accuracy: 676/3584 (19%)\n",
      "Train - batches : 29, average loss: 2.2086, accuracy: 700/3712 (19%)\n",
      "Train - batches : 30, average loss: 2.2037, accuracy: 736/3840 (19%)\n",
      "Train - batches : 31, average loss: 2.2000, accuracy: 767/3968 (19%)\n",
      "Train - batches : 32, average loss: 2.1952, accuracy: 806/4096 (20%)\n",
      "Train - batches : 33, average loss: 2.1892, accuracy: 850/4224 (20%)\n",
      "Train - batches : 34, average loss: 2.1861, accuracy: 885/4352 (20%)\n",
      "Train - batches : 35, average loss: 2.1828, accuracy: 908/4480 (20%)\n",
      "Train - batches : 36, average loss: 2.1799, accuracy: 933/4608 (20%)\n",
      "Train - batches : 37, average loss: 2.1764, accuracy: 970/4736 (20%)\n",
      "Train - batches : 38, average loss: 2.1737, accuracy: 1000/4864 (21%)\n",
      "Train - batches : 39, average loss: 2.1705, accuracy: 1035/4992 (21%)\n",
      "Train - batches : 40, average loss: 2.1682, accuracy: 1066/5120 (21%)\n",
      "Train - batches : 41, average loss: 2.1640, accuracy: 1099/5248 (21%)\n",
      "Train - batches : 42, average loss: 2.1621, accuracy: 1139/5376 (21%)\n",
      "Train - batches : 43, average loss: 2.1594, accuracy: 1164/5504 (21%)\n",
      "Train - batches : 44, average loss: 2.1540, accuracy: 1210/5632 (21%)\n",
      "Train - batches : 45, average loss: 2.1496, accuracy: 1247/5760 (22%)\n",
      "Train - batches : 46, average loss: 2.1457, accuracy: 1286/5888 (22%)\n",
      "Train - batches : 47, average loss: 2.1434, accuracy: 1319/6016 (22%)\n",
      "Train - batches : 48, average loss: 2.1392, accuracy: 1355/6144 (22%)\n",
      "Train - batches : 49, average loss: 2.1369, accuracy: 1385/6272 (22%)\n",
      "Train - batches : 50, average loss: 2.1359, accuracy: 1418/6400 (22%)\n",
      "Train - batches : 51, average loss: 2.1325, accuracy: 1459/6528 (22%)\n",
      "Train - batches : 52, average loss: 2.1299, accuracy: 1494/6656 (22%)\n",
      "Train - batches : 53, average loss: 2.1260, accuracy: 1532/6784 (23%)\n",
      "Train - batches : 54, average loss: 2.1218, accuracy: 1568/6912 (23%)\n",
      "Train - batches : 55, average loss: 2.1203, accuracy: 1597/7040 (23%)\n",
      "Train - batches : 56, average loss: 2.1184, accuracy: 1626/7168 (23%)\n",
      "Train - batches : 57, average loss: 2.1165, accuracy: 1664/7296 (23%)\n",
      "Train - batches : 58, average loss: 2.1144, accuracy: 1701/7424 (23%)\n",
      "Train - batches : 59, average loss: 2.1116, accuracy: 1733/7552 (23%)\n",
      "Train - batches : 60, average loss: 2.1090, accuracy: 1766/7680 (23%)\n",
      "Train - batches : 61, average loss: 2.1061, accuracy: 1804/7808 (23%)\n",
      "Train - batches : 62, average loss: 2.1047, accuracy: 1832/7936 (23%)\n",
      "Train - batches : 63, average loss: 2.1018, accuracy: 1869/8064 (23%)\n",
      "Train - batches : 64, average loss: 2.0990, accuracy: 1906/8192 (23%)\n",
      "Train - batches : 65, average loss: 2.0960, accuracy: 1942/8320 (23%)\n",
      "Train - batches : 66, average loss: 2.0936, accuracy: 1982/8448 (23%)\n",
      "Train - batches : 67, average loss: 2.0929, accuracy: 2012/8576 (23%)\n",
      "Train - batches : 68, average loss: 2.0898, accuracy: 2052/8704 (24%)\n",
      "Train - batches : 69, average loss: 2.0866, accuracy: 2098/8832 (24%)\n",
      "Train - batches : 70, average loss: 2.0842, accuracy: 2136/8960 (24%)\n",
      "Train - batches : 71, average loss: 2.0824, accuracy: 2179/9088 (24%)\n",
      "Train - batches : 72, average loss: 2.0795, accuracy: 2219/9216 (24%)\n",
      "Train - batches : 73, average loss: 2.0776, accuracy: 2255/9344 (24%)\n",
      "Train - batches : 74, average loss: 2.0765, accuracy: 2287/9472 (24%)\n",
      "Train - batches : 75, average loss: 2.0741, accuracy: 2328/9600 (24%)\n",
      "Train - batches : 76, average loss: 2.0719, accuracy: 2362/9728 (24%)\n",
      "Train - batches : 77, average loss: 2.0691, accuracy: 2402/9856 (24%)\n",
      "Train - batches : 78, average loss: 2.0682, accuracy: 2429/9984 (24%)\n",
      "Train - batches : 79, average loss: 2.0659, accuracy: 2470/10112 (24%)\n",
      "Train - batches : 80, average loss: 2.0636, accuracy: 2504/10240 (24%)\n",
      "Train - batches : 81, average loss: 2.0605, accuracy: 2544/10368 (25%)\n",
      "Train - batches : 82, average loss: 2.0582, accuracy: 2582/10496 (25%)\n",
      "Train - batches : 83, average loss: 2.0548, accuracy: 2625/10624 (25%)\n",
      "Train - batches : 84, average loss: 2.0522, accuracy: 2668/10752 (25%)\n",
      "Train - batches : 85, average loss: 2.0507, accuracy: 2711/10880 (25%)\n",
      "Train - batches : 86, average loss: 2.0477, accuracy: 2756/11008 (25%)\n",
      "Train - batches : 87, average loss: 2.0460, accuracy: 2796/11136 (25%)\n",
      "Train - batches : 88, average loss: 2.0447, accuracy: 2831/11264 (25%)\n",
      "Train - batches : 89, average loss: 2.0429, accuracy: 2859/11392 (25%)\n",
      "Train - batches : 90, average loss: 2.0410, accuracy: 2895/11520 (25%)\n",
      "Train - batches : 91, average loss: 2.0385, accuracy: 2934/11648 (25%)\n",
      "Train - batches : 92, average loss: 2.0357, accuracy: 2979/11776 (25%)\n",
      "Train - batches : 93, average loss: 2.0337, accuracy: 3018/11904 (25%)\n",
      "Train - batches : 94, average loss: 2.0312, accuracy: 3057/12032 (25%)\n",
      "Train - batches : 95, average loss: 2.0304, accuracy: 3088/12160 (25%)\n",
      "Train - batches : 96, average loss: 2.0276, accuracy: 3131/12288 (25%)\n",
      "Train - batches : 97, average loss: 2.0258, accuracy: 3173/12416 (26%)\n",
      "Train - batches : 98, average loss: 2.0242, accuracy: 3213/12544 (26%)\n",
      "Train - batches : 99, average loss: 2.0223, accuracy: 3250/12672 (26%)\n",
      "Train - batches : 100, average loss: 2.0206, accuracy: 3289/12800 (26%)\n",
      "Train - batches : 101, average loss: 2.0187, accuracy: 3330/12928 (26%)\n",
      "Train - batches : 102, average loss: 2.0168, accuracy: 3370/13056 (26%)\n",
      "Train - batches : 103, average loss: 2.0146, accuracy: 3410/13184 (26%)\n",
      "Train - batches : 104, average loss: 2.0124, accuracy: 3451/13312 (26%)\n",
      "Train - batches : 105, average loss: 2.0111, accuracy: 3487/13440 (26%)\n",
      "Train - batches : 106, average loss: 2.0087, accuracy: 3525/13568 (26%)\n",
      "Train - batches : 107, average loss: 2.0073, accuracy: 3561/13696 (26%)\n",
      "Train - batches : 108, average loss: 2.0060, accuracy: 3600/13824 (26%)\n",
      "Train - batches : 109, average loss: 2.0037, accuracy: 3646/13952 (26%)\n",
      "Train - batches : 110, average loss: 2.0022, accuracy: 3689/14080 (26%)\n",
      "Train - batches : 111, average loss: 2.0008, accuracy: 3738/14208 (26%)\n",
      "Train - batches : 112, average loss: 1.9991, accuracy: 3780/14336 (26%)\n",
      "Train - batches : 113, average loss: 1.9978, accuracy: 3822/14464 (26%)\n",
      "Train - batches : 114, average loss: 1.9958, accuracy: 3864/14592 (26%)\n",
      "Train - batches : 115, average loss: 1.9941, accuracy: 3905/14720 (27%)\n",
      "Train - batches : 116, average loss: 1.9932, accuracy: 3947/14848 (27%)\n",
      "Train - batches : 117, average loss: 1.9914, accuracy: 3994/14976 (27%)\n",
      "Train - batches : 118, average loss: 1.9900, accuracy: 4034/15104 (27%)\n",
      "Train - batches : 119, average loss: 1.9883, accuracy: 4086/15232 (27%)\n",
      "Train - batches : 120, average loss: 1.9872, accuracy: 4127/15360 (27%)\n",
      "Train - batches : 121, average loss: 1.9861, accuracy: 4169/15488 (27%)\n",
      "Train - batches : 122, average loss: 1.9844, accuracy: 4215/15616 (27%)\n",
      "Train - batches : 123, average loss: 1.9838, accuracy: 4260/15744 (27%)\n",
      "Train - batches : 124, average loss: 1.9832, accuracy: 4294/15872 (27%)\n",
      "Train - batches : 125, average loss: 1.9822, accuracy: 4327/16000 (27%)\n",
      "Train - batches : 126, average loss: 1.9810, accuracy: 4366/16128 (27%)\n",
      "Train - batches : 127, average loss: 1.9794, accuracy: 4412/16256 (27%)\n",
      "Train - batches : 128, average loss: 1.9773, accuracy: 4457/16384 (27%)\n",
      "Train - batches : 129, average loss: 1.9755, accuracy: 4505/16512 (27%)\n",
      "Train - batches : 130, average loss: 1.9731, accuracy: 4564/16640 (27%)\n",
      "Train - batches : 131, average loss: 1.9718, accuracy: 4610/16768 (27%)\n",
      "Train - batches : 132, average loss: 1.9699, accuracy: 4658/16896 (28%)\n",
      "Train - batches : 133, average loss: 1.9688, accuracy: 4696/17024 (28%)\n",
      "Train - batches : 134, average loss: 1.9668, accuracy: 4744/17152 (28%)\n",
      "Train - batches : 135, average loss: 1.9658, accuracy: 4782/17280 (28%)\n",
      "Train - batches : 136, average loss: 1.9646, accuracy: 4831/17408 (28%)\n",
      "Train - batches : 137, average loss: 1.9627, accuracy: 4876/17536 (28%)\n",
      "Train - batches : 138, average loss: 1.9621, accuracy: 4916/17664 (28%)\n",
      "Train - batches : 139, average loss: 1.9600, accuracy: 4966/17792 (28%)\n",
      "Train - batches : 140, average loss: 1.9584, accuracy: 5009/17920 (28%)\n",
      "Train - batches : 141, average loss: 1.9566, accuracy: 5057/18048 (28%)\n",
      "Train - batches : 142, average loss: 1.9552, accuracy: 5101/18176 (28%)\n",
      "Train - batches : 143, average loss: 1.9541, accuracy: 5147/18304 (28%)\n",
      "Train - batches : 144, average loss: 1.9529, accuracy: 5187/18432 (28%)\n",
      "Train - batches : 145, average loss: 1.9518, accuracy: 5233/18560 (28%)\n",
      "Train - batches : 146, average loss: 1.9511, accuracy: 5272/18688 (28%)\n",
      "Train - batches : 147, average loss: 1.9501, accuracy: 5316/18816 (28%)\n",
      "Train - batches : 148, average loss: 1.9486, accuracy: 5358/18944 (28%)\n",
      "Train - batches : 149, average loss: 1.9473, accuracy: 5404/19072 (28%)\n",
      "Train - batches : 150, average loss: 1.9455, accuracy: 5444/19200 (28%)\n",
      "Train - batches : 151, average loss: 1.9444, accuracy: 5488/19328 (28%)\n",
      "Train - batches : 152, average loss: 1.9434, accuracy: 5535/19456 (28%)\n",
      "Train - batches : 153, average loss: 1.9426, accuracy: 5575/19584 (28%)\n",
      "Train - batches : 154, average loss: 1.9424, accuracy: 5618/19712 (29%)\n",
      "Train - batches : 155, average loss: 1.9415, accuracy: 5654/19840 (28%)\n",
      "Train - batches : 156, average loss: 1.9410, accuracy: 5692/19968 (29%)\n",
      "Train - batches : 157, average loss: 1.9404, accuracy: 5739/20096 (29%)\n",
      "Train - batches : 158, average loss: 1.9395, accuracy: 5778/20224 (29%)\n",
      "Train - batches : 159, average loss: 1.9383, accuracy: 5825/20352 (29%)\n",
      "Train - batches : 160, average loss: 1.9371, accuracy: 5869/20480 (29%)\n",
      "Train - batches : 161, average loss: 1.9362, accuracy: 5913/20608 (29%)\n",
      "Train - batches : 162, average loss: 1.9350, accuracy: 5960/20736 (29%)\n",
      "Train - batches : 163, average loss: 1.9341, accuracy: 6007/20864 (29%)\n",
      "Train - batches : 164, average loss: 1.9333, accuracy: 6053/20992 (29%)\n",
      "Train - batches : 165, average loss: 1.9319, accuracy: 6102/21120 (29%)\n",
      "Train - batches : 166, average loss: 1.9309, accuracy: 6143/21248 (29%)\n",
      "Train - batches : 167, average loss: 1.9298, accuracy: 6194/21376 (29%)\n",
      "Train - batches : 168, average loss: 1.9295, accuracy: 6235/21504 (29%)\n",
      "Train - batches : 169, average loss: 1.9289, accuracy: 6278/21632 (29%)\n",
      "Train - batches : 170, average loss: 1.9273, accuracy: 6323/21760 (29%)\n",
      "Train - batches : 171, average loss: 1.9263, accuracy: 6369/21888 (29%)\n",
      "Train - batches : 172, average loss: 1.9254, accuracy: 6412/22016 (29%)\n",
      "Train - batches : 173, average loss: 1.9252, accuracy: 6448/22144 (29%)\n",
      "Train - batches : 174, average loss: 1.9244, accuracy: 6491/22272 (29%)\n",
      "Train - batches : 175, average loss: 1.9232, accuracy: 6542/22400 (29%)\n",
      "Train - batches : 176, average loss: 1.9225, accuracy: 6588/22528 (29%)\n",
      "Train - batches : 177, average loss: 1.9219, accuracy: 6628/22656 (29%)\n",
      "Train - batches : 178, average loss: 1.9212, accuracy: 6676/22784 (29%)\n",
      "Train - batches : 179, average loss: 1.9203, accuracy: 6723/22912 (29%)\n",
      "Train - batches : 180, average loss: 1.9190, accuracy: 6779/23040 (29%)\n",
      "Train - batches : 181, average loss: 1.9179, accuracy: 6827/23168 (29%)\n",
      "Train - batches : 182, average loss: 1.9166, accuracy: 6876/23296 (30%)\n",
      "Train - batches : 183, average loss: 1.9160, accuracy: 6914/23424 (30%)\n",
      "Train - batches : 184, average loss: 1.9153, accuracy: 6956/23552 (30%)\n",
      "Train - batches : 185, average loss: 1.9141, accuracy: 7004/23680 (30%)\n",
      "Train - batches : 186, average loss: 1.9128, accuracy: 7052/23808 (30%)\n",
      "Train - batches : 187, average loss: 1.9120, accuracy: 7094/23936 (30%)\n",
      "Train - batches : 188, average loss: 1.9110, accuracy: 7149/24064 (30%)\n",
      "Train - batches : 189, average loss: 1.9102, accuracy: 7189/24192 (30%)\n",
      "Train - batches : 190, average loss: 1.9095, accuracy: 7234/24320 (30%)\n",
      "Train - batches : 191, average loss: 1.9078, accuracy: 7297/24448 (30%)\n",
      "Train - batches : 192, average loss: 1.9070, accuracy: 7337/24576 (30%)\n",
      "Train - batches : 193, average loss: 1.9056, accuracy: 7390/24704 (30%)\n",
      "Train - batches : 194, average loss: 1.9047, accuracy: 7438/24832 (30%)\n",
      "Train - batches : 195, average loss: 1.9038, accuracy: 7489/24960 (30%)\n",
      "Train - batches : 196, average loss: 1.9029, accuracy: 7538/25088 (30%)\n",
      "Train - batches : 197, average loss: 1.9016, accuracy: 7593/25216 (30%)\n",
      "Train - batches : 198, average loss: 1.9010, accuracy: 7645/25344 (30%)\n",
      "Train - batches : 199, average loss: 1.9002, accuracy: 7697/25472 (30%)\n",
      "Train - batches : 200, average loss: 1.8991, accuracy: 7745/25600 (30%)\n",
      "Train - batches : 201, average loss: 1.8980, accuracy: 7794/25728 (30%)\n",
      "Train - batches : 202, average loss: 1.8969, accuracy: 7834/25856 (30%)\n",
      "Train - batches : 203, average loss: 1.8961, accuracy: 7882/25984 (30%)\n",
      "Train - batches : 204, average loss: 1.8953, accuracy: 7929/26112 (30%)\n",
      "Train - batches : 205, average loss: 1.8944, accuracy: 7973/26240 (30%)\n",
      "Train - batches : 206, average loss: 1.8936, accuracy: 8024/26368 (30%)\n",
      "Train - batches : 207, average loss: 1.8925, accuracy: 8073/26496 (30%)\n",
      "Train - batches : 208, average loss: 1.8928, accuracy: 8108/26624 (30%)\n",
      "Train - batches : 209, average loss: 1.8917, accuracy: 8159/26752 (30%)\n",
      "Train - batches : 210, average loss: 1.8911, accuracy: 8207/26880 (31%)\n",
      "Train - batches : 211, average loss: 1.8893, accuracy: 8267/27008 (31%)\n",
      "Train - batches : 212, average loss: 1.8885, accuracy: 8316/27136 (31%)\n",
      "Train - batches : 213, average loss: 1.8877, accuracy: 8363/27264 (31%)\n",
      "Train - batches : 214, average loss: 1.8864, accuracy: 8425/27392 (31%)\n",
      "Train - batches : 215, average loss: 1.8852, accuracy: 8483/27520 (31%)\n",
      "Train - batches : 216, average loss: 1.8840, accuracy: 8535/27648 (31%)\n",
      "Train - batches : 217, average loss: 1.8830, accuracy: 8585/27776 (31%)\n",
      "Train - batches : 218, average loss: 1.8818, accuracy: 8637/27904 (31%)\n",
      "Train - batches : 219, average loss: 1.8813, accuracy: 8684/28032 (31%)\n",
      "Train - batches : 220, average loss: 1.8803, accuracy: 8727/28160 (31%)\n",
      "Train - batches : 221, average loss: 1.8794, accuracy: 8777/28288 (31%)\n",
      "Train - batches : 222, average loss: 1.8782, accuracy: 8824/28416 (31%)\n",
      "Train - batches : 223, average loss: 1.8773, accuracy: 8871/28544 (31%)\n",
      "Train - batches : 224, average loss: 1.8771, accuracy: 8918/28672 (31%)\n",
      "Train - batches : 225, average loss: 1.8767, accuracy: 8961/28800 (31%)\n",
      "Train - batches : 226, average loss: 1.8762, accuracy: 9007/28928 (31%)\n",
      "Train - batches : 227, average loss: 1.8760, accuracy: 9049/29056 (31%)\n",
      "Train - batches : 228, average loss: 1.8749, accuracy: 9104/29184 (31%)\n",
      "Train - batches : 229, average loss: 1.8742, accuracy: 9145/29312 (31%)\n",
      "Train - batches : 230, average loss: 1.8734, accuracy: 9195/29440 (31%)\n",
      "Train - batches : 231, average loss: 1.8728, accuracy: 9240/29568 (31%)\n",
      "Train - batches : 232, average loss: 1.8718, accuracy: 9299/29696 (31%)\n",
      "Train - batches : 233, average loss: 1.8713, accuracy: 9347/29824 (31%)\n",
      "Train - batches : 234, average loss: 1.8707, accuracy: 9396/29952 (31%)\n",
      "Train - batches : 235, average loss: 1.8697, accuracy: 9441/30080 (31%)\n",
      "Train - batches : 236, average loss: 1.8687, accuracy: 9493/30208 (31%)\n",
      "Train - batches : 237, average loss: 1.8675, accuracy: 9540/30336 (31%)\n",
      "Train - batches : 238, average loss: 1.8666, accuracy: 9592/30464 (31%)\n",
      "Train - batches : 239, average loss: 1.8659, accuracy: 9643/30592 (32%)\n",
      "Train - batches : 240, average loss: 1.8653, accuracy: 9687/30720 (32%)\n",
      "Train - batches : 241, average loss: 1.8649, accuracy: 9737/30848 (32%)\n",
      "Train - batches : 242, average loss: 1.8645, accuracy: 9785/30976 (32%)\n",
      "Train - batches : 243, average loss: 1.8635, accuracy: 9830/31104 (32%)\n",
      "Train - batches : 244, average loss: 1.8624, accuracy: 9893/31232 (32%)\n",
      "Train - batches : 245, average loss: 1.8616, accuracy: 9941/31360 (32%)\n",
      "Train - batches : 246, average loss: 1.8611, accuracy: 9994/31488 (32%)\n",
      "Train - batches : 247, average loss: 1.8605, accuracy: 10040/31616 (32%)\n",
      "Train - batches : 248, average loss: 1.8600, accuracy: 10086/31744 (32%)\n",
      "Train - batches : 249, average loss: 1.8592, accuracy: 10128/31872 (32%)\n",
      "Train - batches : 250, average loss: 1.8586, accuracy: 10164/32000 (32%)\n",
      "Train - batches : 251, average loss: 1.8578, accuracy: 10218/32128 (32%)\n",
      "Train - batches : 252, average loss: 1.8569, accuracy: 10268/32256 (32%)\n",
      "Train - batches : 253, average loss: 1.8557, accuracy: 10319/32384 (32%)\n",
      "Train - batches : 254, average loss: 1.8549, accuracy: 10364/32512 (32%)\n",
      "Train - batches : 255, average loss: 1.8539, accuracy: 10414/32640 (32%)\n",
      "Train - batches : 256, average loss: 1.8533, accuracy: 10464/32768 (32%)\n",
      "Train - batches : 257, average loss: 1.8520, accuracy: 10525/32896 (32%)\n",
      "Train - batches : 258, average loss: 1.8519, accuracy: 10568/33024 (32%)\n",
      "Train - batches : 259, average loss: 1.8512, accuracy: 10624/33152 (32%)\n",
      "Train - batches : 260, average loss: 1.8503, accuracy: 10673/33280 (32%)\n",
      "Train - batches : 261, average loss: 1.8494, accuracy: 10725/33408 (32%)\n",
      "Train - batches : 262, average loss: 1.8487, accuracy: 10773/33536 (32%)\n",
      "Train - batches : 263, average loss: 1.8483, accuracy: 10818/33664 (32%)\n",
      "Train - batches : 264, average loss: 1.8475, accuracy: 10864/33792 (32%)\n",
      "Train - batches : 265, average loss: 1.8468, accuracy: 10909/33920 (32%)\n",
      "Train - batches : 266, average loss: 1.8462, accuracy: 10962/34048 (32%)\n",
      "Train - batches : 267, average loss: 1.8456, accuracy: 11009/34176 (32%)\n",
      "Train - batches : 268, average loss: 1.8448, accuracy: 11060/34304 (32%)\n",
      "Train - batches : 269, average loss: 1.8439, accuracy: 11115/34432 (32%)\n",
      "Train - batches : 270, average loss: 1.8429, accuracy: 11169/34560 (32%)\n",
      "Train - batches : 271, average loss: 1.8428, accuracy: 11219/34688 (32%)\n",
      "Train - batches : 272, average loss: 1.8417, accuracy: 11277/34816 (32%)\n",
      "Train - batches : 273, average loss: 1.8409, accuracy: 11319/34944 (32%)\n",
      "Train - batches : 274, average loss: 1.8401, accuracy: 11371/35072 (32%)\n",
      "Train - batches : 275, average loss: 1.8394, accuracy: 11420/35200 (32%)\n",
      "Train - batches : 276, average loss: 1.8390, accuracy: 11460/35328 (32%)\n",
      "Train - batches : 277, average loss: 1.8383, accuracy: 11517/35456 (32%)\n",
      "Train - batches : 278, average loss: 1.8378, accuracy: 11564/35584 (32%)\n",
      "Train - batches : 279, average loss: 1.8369, accuracy: 11622/35712 (33%)\n",
      "Train - batches : 280, average loss: 1.8364, accuracy: 11666/35840 (33%)\n",
      "Train - batches : 281, average loss: 1.8357, accuracy: 11710/35968 (33%)\n",
      "Train - batches : 282, average loss: 1.8348, accuracy: 11768/36096 (33%)\n",
      "Train - batches : 283, average loss: 1.8342, accuracy: 11828/36224 (33%)\n",
      "Train - batches : 284, average loss: 1.8336, accuracy: 11875/36352 (33%)\n",
      "Train - batches : 285, average loss: 1.8326, accuracy: 11928/36480 (33%)\n",
      "Train - batches : 286, average loss: 1.8320, accuracy: 11979/36608 (33%)\n",
      "Train - batches : 287, average loss: 1.8312, accuracy: 12032/36736 (33%)\n",
      "Train - batches : 288, average loss: 1.8304, accuracy: 12078/36864 (33%)\n",
      "Train - batches : 289, average loss: 1.8301, accuracy: 12119/36992 (33%)\n",
      "Train - batches : 290, average loss: 1.8292, accuracy: 12171/37120 (33%)\n",
      "Train - batches : 291, average loss: 1.8287, accuracy: 12219/37248 (33%)\n",
      "Train - batches : 292, average loss: 1.8275, accuracy: 12280/37376 (33%)\n",
      "Train - batches : 293, average loss: 1.8267, accuracy: 12339/37504 (33%)\n",
      "Train - batches : 294, average loss: 1.8263, accuracy: 12385/37632 (33%)\n",
      "Train - batches : 295, average loss: 1.8254, accuracy: 12437/37760 (33%)\n",
      "Train - batches : 296, average loss: 1.8247, accuracy: 12495/37888 (33%)\n",
      "Train - batches : 297, average loss: 1.8246, accuracy: 12531/38016 (33%)\n",
      "Train - batches : 298, average loss: 1.8243, accuracy: 12574/38144 (33%)\n",
      "Train - batches : 299, average loss: 1.8237, accuracy: 12626/38272 (33%)\n",
      "Train - batches : 300, average loss: 1.8228, accuracy: 12681/38400 (33%)\n",
      "Train - batches : 301, average loss: 1.8224, accuracy: 12732/38528 (33%)\n",
      "Train - batches : 302, average loss: 1.8219, accuracy: 12781/38656 (33%)\n",
      "Train - batches : 303, average loss: 1.8211, accuracy: 12837/38784 (33%)\n",
      "Train - batches : 304, average loss: 1.8205, accuracy: 12884/38912 (33%)\n",
      "Train - batches : 305, average loss: 1.8200, accuracy: 12928/39040 (33%)\n",
      "Train - batches : 306, average loss: 1.8196, accuracy: 12969/39168 (33%)\n",
      "Train - batches : 307, average loss: 1.8189, accuracy: 13023/39296 (33%)\n",
      "Train - batches : 308, average loss: 1.8179, accuracy: 13083/39424 (33%)\n",
      "Train - batches : 309, average loss: 1.8170, accuracy: 13139/39552 (33%)\n",
      "Train - batches : 310, average loss: 1.8163, accuracy: 13192/39680 (33%)\n",
      "Train - batches : 311, average loss: 1.8158, accuracy: 13243/39808 (33%)\n",
      "Train - batches : 312, average loss: 1.8153, accuracy: 13297/39936 (33%)\n",
      "Train - batches : 313, average loss: 1.8143, accuracy: 13349/40064 (33%)\n",
      "Train - batches : 314, average loss: 1.8132, accuracy: 13402/40192 (33%)\n",
      "Train - batches : 315, average loss: 1.8123, accuracy: 13454/40320 (33%)\n",
      "Train - batches : 316, average loss: 1.8116, accuracy: 13511/40448 (33%)\n",
      "Train - batches : 317, average loss: 1.8112, accuracy: 13561/40576 (33%)\n",
      "Train - batches : 318, average loss: 1.8108, accuracy: 13614/40704 (33%)\n",
      "Train - batches : 319, average loss: 1.8102, accuracy: 13667/40832 (33%)\n",
      "Train - batches : 320, average loss: 1.8097, accuracy: 13724/40960 (34%)\n",
      "Train - batches : 321, average loss: 1.8095, accuracy: 13766/41088 (34%)\n",
      "Train - batches : 322, average loss: 1.8089, accuracy: 13812/41216 (34%)\n",
      "Train - batches : 323, average loss: 1.8087, accuracy: 13859/41344 (34%)\n",
      "Train - batches : 324, average loss: 1.8080, accuracy: 13911/41472 (34%)\n",
      "Train - batches : 325, average loss: 1.8075, accuracy: 13960/41600 (34%)\n",
      "Train - batches : 326, average loss: 1.8068, accuracy: 14012/41728 (34%)\n",
      "Train - batches : 327, average loss: 1.8065, accuracy: 14055/41856 (34%)\n",
      "Train - batches : 328, average loss: 1.8062, accuracy: 14106/41984 (34%)\n",
      "Train - batches : 329, average loss: 1.8056, accuracy: 14162/42112 (34%)\n",
      "Train - batches : 330, average loss: 1.8048, accuracy: 14222/42240 (34%)\n",
      "Train - batches : 331, average loss: 1.8041, accuracy: 14271/42368 (34%)\n",
      "Train - batches : 332, average loss: 1.8031, accuracy: 14329/42496 (34%)\n",
      "Train - batches : 333, average loss: 1.8025, accuracy: 14379/42624 (34%)\n",
      "Train - batches : 334, average loss: 1.8019, accuracy: 14433/42752 (34%)\n",
      "Train - batches : 335, average loss: 1.8008, accuracy: 14490/42880 (34%)\n",
      "Train - batches : 336, average loss: 1.7999, accuracy: 14545/43008 (34%)\n",
      "Train - batches : 337, average loss: 1.7991, accuracy: 14601/43136 (34%)\n",
      "Train - batches : 338, average loss: 1.7989, accuracy: 14653/43264 (34%)\n",
      "Train - batches : 339, average loss: 1.7984, accuracy: 14709/43392 (34%)\n",
      "Train - batches : 340, average loss: 1.7979, accuracy: 14763/43520 (34%)\n",
      "Train - batches : 341, average loss: 1.7972, accuracy: 14814/43648 (34%)\n",
      "Train - batches : 342, average loss: 1.7962, accuracy: 14881/43776 (34%)\n",
      "Train - batches : 343, average loss: 1.7963, accuracy: 14925/43904 (34%)\n",
      "Train - batches : 344, average loss: 1.7960, accuracy: 14974/44032 (34%)\n",
      "Train - batches : 345, average loss: 1.7957, accuracy: 15014/44160 (34%)\n",
      "Train - batches : 346, average loss: 1.7953, accuracy: 15067/44288 (34%)\n",
      "Train - batches : 347, average loss: 1.7950, accuracy: 15118/44416 (34%)\n",
      "Train - batches : 348, average loss: 1.7945, accuracy: 15167/44544 (34%)\n",
      "Train - batches : 349, average loss: 1.7940, accuracy: 15217/44672 (34%)\n",
      "Train - batches : 350, average loss: 1.7936, accuracy: 15265/44800 (34%)\n",
      "Train - batches : 351, average loss: 1.7930, accuracy: 15327/44928 (34%)\n",
      "Train - batches : 352, average loss: 1.7921, accuracy: 15385/45056 (34%)\n",
      "Train - batches : 353, average loss: 1.7916, accuracy: 15428/45184 (34%)\n",
      "Train - batches : 354, average loss: 1.7909, accuracy: 15483/45312 (34%)\n",
      "Train - batches : 355, average loss: 1.7906, accuracy: 15525/45440 (34%)\n",
      "Train - batches : 356, average loss: 1.7898, accuracy: 15577/45568 (34%)\n",
      "Train - batches : 357, average loss: 1.7890, accuracy: 15630/45696 (34%)\n",
      "Train - batches : 358, average loss: 1.7882, accuracy: 15683/45824 (34%)\n",
      "Train - batches : 359, average loss: 1.7875, accuracy: 15739/45952 (34%)\n",
      "Train - batches : 360, average loss: 1.7869, accuracy: 15786/46080 (34%)\n",
      "Train - batches : 361, average loss: 1.7864, accuracy: 15839/46208 (34%)\n",
      "Train - batches : 362, average loss: 1.7859, accuracy: 15886/46336 (34%)\n",
      "Train - batches : 363, average loss: 1.7851, accuracy: 15943/46464 (34%)\n",
      "Train - batches : 364, average loss: 1.7844, accuracy: 15993/46592 (34%)\n",
      "Train - batches : 365, average loss: 1.7839, accuracy: 16049/46720 (34%)\n",
      "Train - batches : 366, average loss: 1.7835, accuracy: 16094/46848 (34%)\n",
      "Train - batches : 367, average loss: 1.7831, accuracy: 16145/46976 (34%)\n",
      "Train - batches : 368, average loss: 1.7824, accuracy: 16202/47104 (34%)\n",
      "Train - batches : 369, average loss: 1.7818, accuracy: 16262/47232 (34%)\n",
      "Train - batches : 370, average loss: 1.7812, accuracy: 16306/47360 (34%)\n",
      "Train - batches : 371, average loss: 1.7804, accuracy: 16369/47488 (34%)\n",
      "Train - batches : 372, average loss: 1.7797, accuracy: 16420/47616 (34%)\n",
      "Train - batches : 373, average loss: 1.7793, accuracy: 16469/47744 (34%)\n",
      "Train - batches : 374, average loss: 1.7787, accuracy: 16526/47872 (35%)\n",
      "Train - batches : 375, average loss: 1.7783, accuracy: 16579/48000 (35%)\n",
      "Train - batches : 376, average loss: 1.7776, accuracy: 16638/48128 (35%)\n",
      "Train - batches : 377, average loss: 1.7772, accuracy: 16694/48256 (35%)\n",
      "Train - batches : 378, average loss: 1.7769, accuracy: 16742/48384 (35%)\n",
      "Train - batches : 379, average loss: 1.7763, accuracy: 16791/48512 (35%)\n",
      "Train - batches : 380, average loss: 1.7759, accuracy: 16846/48640 (35%)\n",
      "Train - batches : 381, average loss: 1.7755, accuracy: 16901/48768 (35%)\n",
      "Train - batches : 382, average loss: 1.7753, accuracy: 16950/48896 (35%)\n",
      "Train - batches : 383, average loss: 1.7747, accuracy: 17004/49024 (35%)\n",
      "Train - batches : 384, average loss: 1.7742, accuracy: 17063/49152 (35%)\n",
      "Train - batches : 385, average loss: 1.7739, accuracy: 17115/49280 (35%)\n",
      "Train - batches : 386, average loss: 1.7734, accuracy: 17173/49408 (35%)\n",
      "Train - batches : 387, average loss: 1.7728, accuracy: 17225/49536 (35%)\n",
      "Train - batches : 388, average loss: 1.7723, accuracy: 17276/49664 (35%)\n",
      "Train - batches : 389, average loss: 1.7716, accuracy: 17326/49792 (35%)\n",
      "Train - batches : 390, average loss: 1.7713, accuracy: 17375/49920 (35%)\n",
      "Train - batches : 391, average loss: 1.7706, accuracy: 17412/50000 (35%)\n",
      "Test - batches: 391, average loss: 0.0129, accuracy: 3919/10000 (39%)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_train_metric(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "132ffd64d79c4b81837a565291dc67da"
   },
   "source": [
    "#### Retrieve and display the model training logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "87c7cd6ac5d64afa8947783fb5f854e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wmla-console-wmla.apps.cpd35-beta.cpolab.ibm.com/platform/rest/deeplearning/v1/scheduler/applications/wmla-327/executor/1/logs/stdout?lastlines=1000\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:33.321944 39 INFO Create log direcotry /wmla-logging/dli/wmla-327/dli/./app.wmla-327-task12n-jbbhg\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:33.333660 39 INFO Running on kubernetes.\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:33.346049 39 INFO List GPUs\n",
      "*Task <1> SubProcess*: Wed Feb 10 16:39:33 2021       \n",
      "*Task <1> SubProcess*: +-----------------------------------------------------------------------------+\n",
      "*Task <1> SubProcess*: | NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "*Task <1> SubProcess*: |-------------------------------+----------------------+----------------------+\n",
      "*Task <1> SubProcess*: | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "*Task <1> SubProcess*: | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "*Task <1> SubProcess*: |                               |                      |               MIG M. |\n",
      "*Task <1> SubProcess*: |===============================+======================+======================|\n",
      "*Task <1> SubProcess*: |   0  Tesla V100-PCIE...  On   | 00000000:0B:00.0 Off |                    0 |\n",
      "*Task <1> SubProcess*: | N/A   32C    P0    26W / 250W |      0MiB / 32510MiB |      0%      Default |\n",
      "*Task <1> SubProcess*: |                               |                      |                  N/A |\n",
      "*Task <1> SubProcess*: +-------------------------------+----------------------+----------------------+\n",
      "*Task <1> SubProcess*:                                                                                \n",
      "*Task <1> SubProcess*: +-----------------------------------------------------------------------------+\n",
      "*Task <1> SubProcess*: | Processes:                                                                  |\n",
      "*Task <1> SubProcess*: |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "*Task <1> SubProcess*: |        ID   ID                                                   Usage      |\n",
      "*Task <1> SubProcess*: |=============================================================================|\n",
      "*Task <1> SubProcess*: |  No running processes found                                                 |\n",
      "*Task <1> SubProcess*: +-----------------------------------------------------------------------------+\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:33.497944 39 INFO NVIDIA_VISIBLE_DEVICES=GPU-a2ca4f2a-c061-0bbe-bd05-dc2c837f7554\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:33.510250 39 INFO NVIDIA_DRIVER_CAPABILITIES=compute,utility\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:33.537340 39 INFO Init enviroment with /opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/spark-env.sh\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:33.550030 39 INFO Enter into spark-env.sh\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:33.574787 39 INFO Path variables: WMLA_TOP=/opt/ibm/spectrumcomputing, FABRIC_HOME=/opt/ibm/spectrumcomputing/dli/2.2.0/fabric, DLI_DATA_FS=/gpfs/mydatafs, DLI_RESULT_FS=/gpfs/myresultfs, DLI_DEFAULT_CONDA_HOME=/opt/anaconda3, DLI_DEFAULT_CONDA_ENV_NAME=dlipy3\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:33.618331 39 INFO Setup environment on node wmla-327-task12n-jbbhg\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:34.298652 39 INFO Succeed to activate conda environment dlipy3\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:34.324946 39 INFO Conda variables: conda_home=/opt/anaconda3, conda_env=dlipy3, gpu_enabled=Y, python=/opt/anaconda3/envs/dlipy3/bin/python\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:34.335163 39 INFO Check python version\n",
      "*Task <1> SubProcess*: 3.7.9 (default, Aug 31 2020, 12:42:55) \n",
      "*Task <1> SubProcess*: [GCC 7.3.0]\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:34.482661 39 INFO DLI_LOGGER_LEVEL=info\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:34.501104 39 INFO Trace envrionment\n",
      "*Task <1> SubProcess*: CUDA_VISIBLE_DEVICES=GPU-a2ca4f2a-c061-0bbe-bd05-dc2c837f7554\n",
      "*Task <1> SubProcess*: PATH=/opt/anaconda3/envs/dlipy3/bin:/opt/anaconda3/condabin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/anaconda3/bin\n",
      "*Task <1> SubProcess*: PYTHONPATH=/opt/wmla-mk/sdk/python::/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/dli_utils/:/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/dataset/:/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/compression/:/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/3rdparty/py4j-0.10.4-src.zip:/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/3rdparty/pyspark.zip:/gpfs/myresultfs/dse_user:/gpfs/myresultfs/dse_user/lib/python3.7/site-packages\n",
      "*Task <1> SubProcess*: LD_LIBRARY_PATH=/opt/wmla-mk/sdk/python:/usr/local/nvidia/lib:/usr/local/nvidia/lib64::/opt/anaconda3/envs/dlipy3/cuda/lib::/opt/anaconda3/envs/dlipy3/lib/python3.7/site-packages/tensorflow_core:/opt/wmla-mk/sdk/python:/usr/local/nvidia/lib:/usr/local/nvidia/lib64::/opt/anaconda3/envs/dlipy3/cuda/lib::/opt/anaconda3/envs/dlipy3/lib/python3.7/site-packages/tensorflow_core:/opt/anaconda3/envs/dlipy3/lib\n",
      "*Task <1> SubProcess*: WMLA_TOP=/opt/ibm/spectrumcomputing\n",
      "*Task <1> SubProcess*: DLI_DATA_FS=/gpfs/mydatafs\n",
      "*Task <1> SubProcess*: DLI_RESULT_FS=/gpfs/myresultfs\n",
      "*Task <1> SubProcess*: IS_K8S=ON\n",
      "*Task <1> SubProcess*: IS_TASK0=\n",
      "*Task <1> SubProcess*: DLI_EXECID=wmla-327\n",
      "*Task <1> SubProcess*: DLI_WORK_DIR=/gpfs/myresultfs/dse_user/batchworkdir/wmla-327/_submitted_code\n",
      "*Task <1> SubProcess*: DLI_CS_DATASTORE_META=type=fs\n",
      "*Task <1> SubProcess*: FRAME_WORK=PyTorch\n",
      "*Task <1> SubProcess*: EXEC_TYPE=\n",
      "*Task <1> SubProcess*: DLI_IS_ELASTIC=false\n",
      "*Task <1> SubProcess*: FABRIC_HOME=/opt/ibm/spectrumcomputing/dli/2.2.0/fabric\n",
      "*Task <1> SubProcess*: NCCL_SOCKET_IFNAME=\n",
      "*Task <1> SubProcess*: NCCL_P2P_DISABLE=\n",
      "*Task <1> SubProcess*: DLIM_MK_COND_HOME=\n",
      "*Task <1> SubProcess*: HPO_PLUGIN_CONDA_HOME=\n",
      "*Task <1> SubProcess*: DLI_CONDA_HOME=\n",
      "*Task <1> SubProcess*: DLI_DEFAULT_CONDA_HOME=/opt/anaconda3\n",
      "*Task <1> SubProcess*: DLIM_MK_COND_ENV_NAME=\n",
      "*Task <1> SubProcess*: HPO_PLUGIN_CONDA_ENV=\n",
      "*Task <1> SubProcess*: DLI_CONDA_ENV_NAME=\n",
      "*Task <1> SubProcess*: DLI_DEFAULT_CONDA_ENV_NAME=dlipy3\n",
      "*Task <1> SubProcess*: DLI_SC_RUNASUSER=1000590000\n",
      "*Task <1> SubProcess*: DLI_SC_RUNASGROUP=1000590000\n",
      "*Task <1> SubProcess*: DLI_SC_FSGROUP=1000590000\n",
      "*Task <1> SubProcess*: DLI_SUBMIT_USERNAME=dse_user\n",
      "*Task <1> SubProcess*: CURRENT_RUNASUSER=1000590000\n",
      "*Task <1> SubProcess*: CURRENT_RUNASGROUP=1000590000\n",
      "*Task <1> SubProcess*: DATA_DIR=\n",
      "*Task <1> SubProcess*: RESULT_DIR=\n",
      "*Task <1> SubProcess*: LOG_DIR=\n",
      "*Task <1> SubProcess*: SAVED_MODEL_DIR=\n",
      "*Task <1> SubProcess*: CHECKPOINT_DIR=\n",
      "*Task <1> SubProcess*: DLI_LOGGER_LEVEL=info\n",
      "*Task <1> SubProcess*: APP_ID=wmla-327\n",
      "*Task <1> SubProcess*: SPACE_ID=\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:34.517163 39 INFO Finish environment setup.\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:34.531177 39 INFO Entering working directory /gpfs/myresultfs/dse_user/batchworkdir/wmla-327/_submitted_code\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:34.541664 39 INFO Execute command: /opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/launcher/launcher.py --exec_mode=single --work_dir=/gpfs/myresultfs/dse_user/batchworkdir/wmla-327/_submitted_code --app_type=executable --model=/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/dl_plugins/pytorch_wrapper.sh -- /gpfs/myresultfs/dse_user/batchworkdir/wmla-327/_submitted_code/main.py --epochs 1\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:34.551191 39 INFO Job log files under /wmla-logging/dli/wmla-327/dli/./app.wmla-327-task12n-jbbhg\n",
      "*Task <1> SubProcess*: /opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/launcher/launcher.py:21: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "*Task <1> SubProcess*:   from imp import find_module\n",
      "*Task <1> SubProcess*: DLI_LOGGER_LEVEL=info\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:34.707066 309 {'options': <Values at 0x7f57b78f9c90: {'app': '/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/dl_plugins/pytorch_wrapper.sh', 'num_of_worker_hosts': 1, 'num_of_ps_hosts': 1, 'customized_port': 0, 'app_type': 'executable', 'work_dir': '/gpfs/myresultfs/dse_user/batchworkdir/wmla-327/_submitted_code', 'exec_engine': 'msd', 'exec_mode': 'single'}>, 'args': ['/gpfs/myresultfs/dse_user/batchworkdir/wmla-327/_submitted_code/main.py', '--epochs', '1']}\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:34.707141 309 wrap_app is:  /opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/dl_plugins/pytorch_wrapper.sh\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:34.707186 309 calling \"on_task_invoke\"\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:34.707210 309 callback on_task_invoke_begin is started\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:34.869464 309 callback on_task_invoke_begin is finished\n",
      "*Task <1> SubProcess*: DLI_LOGGER_LEVEL=info\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:34.886129 309 load file: executable, /opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/launcher/plugins/executable.py\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:34.886678 309 find class: executable.Executable\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:34.891354 309 Original id is 1, cluster detail is 10.128.4.26\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:34.891412 309 The current task id is 0\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:34.891425 309 partition index: 0, host: 10.128.4.26 \n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:34.891493 309 f=/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/dl_plugins/pytorch_wrapper.sh &&                   if [ ! -x $f ] && head -n1 $f | grep -sq \"^#!\"; then chmod a+x $f || true; fi &&                   if [ -x $f ]; then p=env; else p=python; fi && $p $f  /gpfs/myresultfs/dse_user/batchworkdir/wmla-327/_submitted_code/main.py --epochs 1\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:34.891517 309 calling \"buildCmd\"\n",
      "*Task <1> SubProcess*: WMLA_AUTH_REST_SERVICE_HOST=172.30.191.50\n",
      "*Task <1> SubProcess*: WMLA_EDI_PORT_9000_TCP_PORT=9000\n",
      "*Task <1> SubProcess*: WMLA_AUTH_REST_SERVICE_PORT_HTTPS=3000\n",
      "*Task <1> SubProcess*: WMLA_PROMETHEUS_PORT_9090_TCP=tcp://172.30.244.81:9090\n",
      "*Task <1> SubProcess*: EMETRICS_STREAMING=ON\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_API_PORT_8001_TCP_ADDR=172.30.226.243\n",
      "*Task <1> SubProcess*: WMLA_DLPD_PORT_9243_TCP_PORT=9243\n",
      "*Task <1> SubProcess*: HOSTNAME=wmla-327-task12n-jbbhg\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_PUBLIC_PORT_89_TCP_PORT=89\n",
      "*Task <1> SubProcess*: WMLA_INGRESS_PORT_30746_TCP_PORT=30746\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_API_SERVICE_HOST=172.30.226.243\n",
      "*Task <1> SubProcess*: WMLA_EDI_PORT_9010_TCP_ADDR=172.30.191.192\n",
      "*Task <1> SubProcess*: WMLA_EDI_PORT_9010_TCP_PROTO=tcp\n",
      "*Task <1> SubProcess*: KUBERNETES_PORT_443_TCP_PORT=443\n",
      "*Task <1> SubProcess*: KUBERNETES_PORT=tcp://172.30.0.1:443\n",
      "*Task <1> SubProcess*: TERM=xterm-256color\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_HUB_SERVICE_PORT=8081\n",
      "*Task <1> SubProcess*: WMLA_LOGSTASH_SERVICE_SERVICE_PORT=5043\n",
      "*Task <1> SubProcess*: WMLA_MSS_PORT_9080_TCP=tcp://172.30.6.5:9080\n",
      "*Task <1> SubProcess*: CONDA_SHLVL=1\n",
      "*Task <1> SubProcess*: WMLA_EDI_SERVICE_PORT_ADMIN=8889\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_PUBLIC_PORT_89_TCP=tcp://172.30.53.6:89\n",
      "*Task <1> SubProcess*: WMLA_INFERENCE_SERVICE_PORT_REST=9000\n",
      "*Task <1> SubProcess*: KUBERNETES_SERVICE_PORT=443\n",
      "*Task <1> SubProcess*: WMLA_INGRESS_PORT_30746_TCP=tcp://172.30.177.20:30746\n",
      "*Task <1> SubProcess*: PYTHONUNBUFFERED=x\n",
      "*Task <1> SubProcess*: CONDA_PROMPT_MODIFIER=(dlipy3) \n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_API_PORT_8001_TCP=tcp://172.30.226.243:8001\n",
      "*Task <1> SubProcess*: WMLA_GUI_PORT_8443_TCP_ADDR=172.30.21.215\n",
      "*Task <1> SubProcess*: LOG_FILE_PATH=/wmla-logging/dli/wmla-327/wmla-327-task12n-jbbhg\n",
      "*Task <1> SubProcess*: TF_INCLUDE_DIR=/opt/anaconda3/envs/dlipy3/lib/python3.7/site-packages/tensorflow_core/include\n",
      "*Task <1> SubProcess*: OLDPWD=/gpfs/myresultfs/dse_user/batchworkdir/wmla-327/_submitted_code\n",
      "*Task <1> SubProcess*: WMLA_GUI_SERVICE_PORT_HTTP=8443\n",
      "*Task <1> SubProcess*: WMLA_ETCD_PORT_2379_TCP_ADDR=172.30.48.201\n",
      "*Task <1> SubProcess*: WMLA_INFERENCE_PORT_9000_TCP_PROTO=tcp\n",
      "*Task <1> SubProcess*: WMLA_ETCD_SERVICE_PORT=2379\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_ENTERPRISE_GATEWAY_PORT_8888_TCP=tcp://172.30.125.158:8888\n",
      "*Task <1> SubProcess*: KUBERNETES_SERVICE_HOST=172.30.0.1\n",
      "*Task <1> SubProcess*: WMLA_INGRESS_SERVICE_PORT=30746\n",
      "*Task <1> SubProcess*: WMLA_MSS_PORT_9080_TCP_PORT=9080\n",
      "*Task <1> SubProcess*: DLI_OBJECT_NAME=wmla-327\n",
      "*Task <1> SubProcess*: WMLA_ETCD_SERVICE_PORT_CLIENT=2379\n",
      "*Task <1> SubProcess*: WMLA_DLPD_PORT_27017_TCP_PORT=27017\n",
      "*Task <1> SubProcess*: WMLA_GUI_SERVICE_HOST=172.30.21.215\n",
      "*Task <1> SubProcess*: WMLA_DLPD_PORT_9243_TCP_ADDR=172.30.236.207\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_PUBLIC_SERVICE_HOST=172.30.53.6\n",
      "*Task <1> SubProcess*: DLI_LOGGER_LEVEL=info\n",
      "*Task <1> SubProcess*: CAFFE_BIN=/opt/anaconda3/envs/dlipy3/bin\n",
      "*Task <1> SubProcess*: DLI_VERSION=2.2.0\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_HUB_PORT_8081_TCP_PROTO=tcp\n",
      "*Task <1> SubProcess*: MSD_POD_IP=10.128.4.26\n",
      "*Task <1> SubProcess*: DLI_EXECID=wmla-327\n",
      "*Task <1> SubProcess*: TASK12N_DEVICE_TYPE=ngpus\n",
      "*Task <1> SubProcess*: MSD_MK_CONN_TIMEOUT=15\n",
      "*Task <1> SubProcess*: PYTHONUSERBASE=/gpfs/myresultfs/dse_user\n",
      "*Task <1> SubProcess*: TASK_ID=1\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_ENTERPRISE_GATEWAY_PORT_8888_TCP_PORT=8888\n",
      "*Task <1> SubProcess*: WMLA_INFOSERVICE_PORT=tcp://172.30.196.5:8892\n",
      "*Task <1> SubProcess*: OPAL_PREFIX=/opt/anaconda3/envs/dlipy3\n",
      "*Task <1> SubProcess*: LD_LIBRARY_PATH=/opt/wmla-mk/sdk/python:/usr/local/nvidia/lib:/usr/local/nvidia/lib64::/opt/anaconda3/envs/dlipy3/cuda/lib::/opt/anaconda3/envs/dlipy3/lib/python3.7/site-packages/tensorflow_core:/opt/wmla-mk/sdk/python:/usr/local/nvidia/lib:/usr/local/nvidia/lib64::/opt/anaconda3/envs/dlipy3/cuda/lib::/opt/anaconda3/envs/dlipy3/lib/python3.7/site-packages/tensorflow_core:/opt/anaconda3/envs/dlipy3/lib\n",
      "*Task <1> SubProcess*: WMLA_INFERENCE_SERVICE_HOST=172.30.204.123\n",
      "*Task <1> SubProcess*: WMLA_ETCD_PORT_2379_TCP=tcp://172.30.48.201:2379\n",
      "*Task <1> SubProcess*: WMLA_EDI_PORT_8889_TCP_PORT=8889\n",
      "*Task <1> SubProcess*: WMLA_INGRESS_SERVICE_PORT_HTTPS_PROXY=30746\n",
      "*Task <1> SubProcess*: NVIDIA_VISIBLE_DEVICES=GPU-a2ca4f2a-c061-0bbe-bd05-dc2c837f7554\n",
      "*Task <1> SubProcess*: MK_SERVER_ADDRESS=127.0.0.1:39423\n",
      "*Task <1> SubProcess*: WMLA_MSS_SERVICE_PORT=9080\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_PUBLIC_SERVICE_PORT_HTTP=89\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_HUB_PORT_8081_TCP_ADDR=172.30.130.151\n",
      "*Task <1> SubProcess*: DLI_WORK_DIR=/gpfs/myresultfs/dse_user/batchworkdir/wmla-327/_submitted_code\n",
      "*Task <1> SubProcess*: WMLA_DLPD_PORT_27017_TCP=tcp://172.30.236.207:27017\n",
      "*Task <1> SubProcess*: WMLA_MSD_PORT_10000_TCP_PORT=10000\n",
      "*Task <1> SubProcess*: WMLA_ETCD_PORT=tcp://172.30.48.201:2379\n",
      "*Task <1> SubProcess*: CONDA_EXE=/opt/anaconda3/bin/conda\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_ENTERPRISE_GATEWAY_SERVICE_HOST=172.30.125.158\n",
      "*Task <1> SubProcess*: WMLA_PROMETHEUS_SERVICE_PORT_PROMUI=9090\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_PUBLIC_PORT_493_TCP_ADDR=172.30.53.6\n",
      "*Task <1> SubProcess*: WMLA_MSD_PORT_10000_TCP_PROTO=tcp\n",
      "*Task <1> SubProcess*: WMLA_INFERENCE_SERVICE_PORT=9000\n",
      "*Task <1> SubProcess*: WMLA_EDI_SERVICE_PORT_STREAM=9010\n",
      "*Task <1> SubProcess*: WMLA_MSS_PORT_9080_TCP_PROTO=tcp\n",
      "*Task <1> SubProcess*: WMLA_EDI_PORT_9000_TCP=tcp://172.30.191.192:9000\n",
      "*Task <1> SubProcess*: WMLA_EDI_PORT_8889_TCP_ADDR=172.30.191.192\n",
      "*Task <1> SubProcess*: DLI_DATA_FS=/gpfs/mydatafs\n",
      "*Task <1> SubProcess*: MSD_POD_NAME=wmla-327-task12n-jbbhg\n",
      "*Task <1> SubProcess*: WMLA_INFERENCE_PORT_9000_TCP=tcp://172.30.204.123:9000\n",
      "*Task <1> SubProcess*: WMLA_DLPD_PORT_27017_TCP_PROTO=tcp\n",
      "*Task <1> SubProcess*: NVIDIA_DRIVER_CAPABILITIES=compute,utility\n",
      "*Task <1> SubProcess*: DLI_SUBMIT_USERNAME=dse_user\n",
      "*Task <1> SubProcess*: WMLA_GRAFANA_PORT_3000_TCP=tcp://172.30.4.186:3000\n",
      "*Task <1> SubProcess*: WMLA_MSD_PORT_9080_TCP_PROTO=tcp\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_ENTERPRISE_GATEWAY_PORT_8888_TCP_ADDR=172.30.125.158\n",
      "*Task <1> SubProcess*: POWERAI_SAVE_OMP_NUM_THREADS=16\n",
      "*Task <1> SubProcess*: WMLA_INFOSERVICE_SERVICE_HOST=172.30.196.5\n",
      "*Task <1> SubProcess*: CLUSTERADMIN=wmla\n",
      "*Task <1> SubProcess*: WMLA_AUTH_REST_PORT_3000_TCP_ADDR=172.30.191.50\n",
      "*Task <1> SubProcess*: WMLA_GRAFANA_SERVICE_PORT_GRAFANA=3000\n",
      "*Task <1> SubProcess*: WMLA_AUTH_REST_PORT=tcp://172.30.191.50:3000\n",
      "*Task <1> SubProcess*: WMLA_GRAFANA_PORT_3000_TCP_PROTO=tcp\n",
      "*Task <1> SubProcess*: DLI_DEFAULT_CONDA_HOME=/opt/anaconda3\n",
      "*Task <1> SubProcess*: _CE_CONDA=\n",
      "*Task <1> SubProcess*: TASK_JOB_ID=wmla-327\n",
      "*Task <1> SubProcess*: WMLA_ETCD_PORT_2379_TCP_PROTO=tcp\n",
      "*Task <1> SubProcess*: WMLA_EDI_PORT_9010_TCP_PORT=9010\n",
      "*Task <1> SubProcess*: WMLA_DLPD_SERVICE_HOST=172.30.236.207\n",
      "*Task <1> SubProcess*: PATH=/opt/anaconda3/envs/dlipy3/bin:/opt/anaconda3/condabin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/anaconda3/bin\n",
      "*Task <1> SubProcess*: WMLA_PROMETHEUS_SERVICE_PORT=9090\n",
      "*Task <1> SubProcess*: WMLA_PROMETHEUS_PORT_9090_TCP_PROTO=tcp\n",
      "*Task <1> SubProcess*: WMLA_GUI_PORT_8443_TCP_PORT=8443\n",
      "*Task <1> SubProcess*: WMLA_EDI_PORT_9000_TCP_ADDR=172.30.191.192\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_API_PORT_8001_TCP_PROTO=tcp\n",
      "*Task <1> SubProcess*: MSD_POD_AFFINITY_RULE=preferred\n",
      "*Task <1> SubProcess*: DLIMSD_LOG_DIR=/wmla-logging/dli/wmla-327/dli/./app.wmla-327-task12n-jbbhg\n",
      "*Task <1> SubProcess*: CONDA_PREFIX=/opt/anaconda3/envs/dlipy3\n",
      "*Task <1> SubProcess*: PWD=/gpfs/myresultfs/dse_user/batchworkdir/wmla-327/_submitted_code\n",
      "*Task <1> SubProcess*: WMLA_TOP=/opt/ibm/spectrumcomputing\n",
      "*Task <1> SubProcess*: WMLA_INFOSERVICE_PORT_8892_TCP_PORT=8892\n",
      "*Task <1> SubProcess*: WMLA_GRAFANA_SERVICE_HOST=172.30.4.186\n",
      "*Task <1> SubProcess*: WMLA_EDI_SERVICE_HOST=172.30.191.192\n",
      "*Task <1> SubProcess*: WMLA_MSD_SERVICE_HOST=172.30.135.197\n",
      "*Task <1> SubProcess*: POWERAI_SAVE_CAFFE_BIN=unset\n",
      "*Task <1> SubProcess*: WMLA_LOGSTASH_SERVICE_PORT_5043_TCP_PORT=5043\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_ENTERPRISE_GATEWAY_SERVICE_PORT=8888\n",
      "*Task <1> SubProcess*: SPACE_ID=\n",
      "*Task <1> SubProcess*: CUDA_VISIBLE_DEVICES=GPU-a2ca4f2a-c061-0bbe-bd05-dc2c837f7554\n",
      "*Task <1> SubProcess*: WMLA_AUTH_REST_PORT_3000_TCP=tcp://172.30.191.50:3000\n",
      "*Task <1> SubProcess*: WMLA_INFERENCE_PORT_9000_TCP_PORT=9000\n",
      "*Task <1> SubProcess*: LANG=en_US.UTF-8\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_PUBLIC_SERVICE_PORT=89\n",
      "*Task <1> SubProcess*: WMLA_INGRESS_PORT_30746_TCP_ADDR=172.30.177.20\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_PUBLIC_PORT_89_TCP_ADDR=172.30.53.6\n",
      "*Task <1> SubProcess*: WMLA_INFOSERVICE_SERVICE_PORT_HISTORY_REST=8892\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_PUBLIC_PORT_89_TCP_PROTO=tcp\n",
      "*Task <1> SubProcess*: WMLA_INGRESS_PORT_30746_TCP_PROTO=tcp\n",
      "*Task <1> SubProcess*: WMLA_LOGSTASH_SERVICE_PORT_5043_TCP_ADDR=172.30.46.129\n",
      "*Task <1> SubProcess*: MSD_CONTAINER_USER=1000590000\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_HUB_PORT=tcp://172.30.130.151:8081\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_API_PORT_8001_TCP_PORT=8001\n",
      "*Task <1> SubProcess*: WMLA_ETCD_SERVICE_HOST=172.30.48.201\n",
      "*Task <1> SubProcess*: WMLA_LOGSTASH_SERVICE_PORT_5043_TCP=tcp://172.30.46.129:5043\n",
      "*Task <1> SubProcess*: WMLA_AUTH_REST_PORT_3000_TCP_PROTO=tcp\n",
      "*Task <1> SubProcess*: WMLA_MSS_SERVICE_HOST=172.30.6.5\n",
      "*Task <1> SubProcess*: WMLA_MSD_PORT_9080_TCP_PORT=9080\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_PUBLIC_PORT_493_TCP=tcp://172.30.53.6:493\n",
      "*Task <1> SubProcess*: WMLA_INFOSERVICE_PORT_8892_TCP_ADDR=172.30.196.5\n",
      "*Task <1> SubProcess*: MSD_POD_AFFINITY_TOPOLOGY_KEY=kubernetes.io/hostname\n",
      "*Task <1> SubProcess*: WMLA_INFOSERVICE_PORT_8892_TCP_PROTO=tcp\n",
      "*Task <1> SubProcess*: WMLA_PROMETHEUS_PORT=tcp://172.30.244.81:9090\n",
      "*Task <1> SubProcess*: SIMPLIFIEDWEM=N\n",
      "*Task <1> SubProcess*: WMLA_LOGSTASH_SERVICE_PORT_5043_TCP_PROTO=tcp\n",
      "*Task <1> SubProcess*: DLI_RESULT_FS=/gpfs/myresultfs\n",
      "*Task <1> SubProcess*: DLI_OBJECT_ID=wmla-327\n",
      "*Task <1> SubProcess*: APP_ID=wmla-327\n",
      "*Task <1> SubProcess*: _CE_M=\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_PUBLIC_PORT=tcp://172.30.53.6:89\n",
      "*Task <1> SubProcess*: WMLA_DLPD_PORT_9243_TCP_PROTO=tcp\n",
      "*Task <1> SubProcess*: REDHARE_LD_LIBRARY_PATH=/opt/wmla-mk/sdk/python\n",
      "*Task <1> SubProcess*: SHLVL=4\n",
      "*Task <1> SubProcess*: HOME=/\n",
      "*Task <1> SubProcess*: WMLA_LOGSTASH_SERVICE_PORT=tcp://172.30.46.129:5043\n",
      "*Task <1> SubProcess*: WMLA_PROMETHEUS_PORT_9090_TCP_ADDR=172.30.244.81\n",
      "*Task <1> SubProcess*: WMLA_INFOSERVICE_SERVICE_PORT=8892\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_PUBLIC_PORT_493_TCP_PORT=493\n",
      "*Task <1> SubProcess*: LANGUAGE=en_US:en\n",
      "*Task <1> SubProcess*: WMLA_MSD_SERVICE_PORT=10000\n",
      "*Task <1> SubProcess*: WMLA_MSD_PORT_9080_TCP=tcp://172.30.135.197:9080\n",
      "*Task <1> SubProcess*: WMLA_GRAFANA_SERVICE_PORT=3000\n",
      "*Task <1> SubProcess*: WMLA_EDI_SERVICE_PORT=9000\n",
      "*Task <1> SubProcess*: CURRENT_RUNASGROUP=1000590000\n",
      "*Task <1> SubProcess*: TF_LIBRARY_DIR=/opt/anaconda3/envs/dlipy3/lib/python3.7/site-packages/tensorflow_core\n",
      "*Task <1> SubProcess*: WMLA_AUTH_REST_PORT_3000_TCP_PORT=3000\n",
      "*Task <1> SubProcess*: WMLA_INFERENCE_PORT_9000_TCP_ADDR=172.30.204.123\n",
      "*Task <1> SubProcess*: WMLA_MSS_SERVICE_PORT_ENTRANCE_HTTP=9080\n",
      "*Task <1> SubProcess*: KUBERNETES_PORT_443_TCP_PROTO=tcp\n",
      "*Task <1> SubProcess*: REDHARE_MK_MSD_ADDR=172.30.135.197:10000\n",
      "*Task <1> SubProcess*: FABRIC_HOME=/opt/ibm/spectrumcomputing/dli/2.2.0/fabric\n",
      "*Task <1> SubProcess*: KUBERNETES_SERVICE_PORT_HTTPS=443\n",
      "*Task <1> SubProcess*: DLI_CS_DATASTORE_META=type=fs\n",
      "*Task <1> SubProcess*: DLIM_MK_LOG_PATH=/wmla-logging/dli\n",
      "*Task <1> SubProcess*: CURRENT_RUNASUSER=1000590000\n",
      "*Task <1> SubProcess*: WMLA_GUI_SERVICE_PORT=8443\n",
      "*Task <1> SubProcess*: WMLA_EDI_PORT_9000_TCP_PROTO=tcp\n",
      "*Task <1> SubProcess*: WMLA_INGRESS_PORT=tcp://172.30.177.20:30746\n",
      "*Task <1> SubProcess*: WMLA_PROMETHEUS_SERVICE_HOST=172.30.244.81\n",
      "*Task <1> SubProcess*: mss_cluster_spec=10.128.4.26\n",
      "*Task <1> SubProcess*: WMLA_GRAFANA_PORT_3000_TCP_PORT=3000\n",
      "*Task <1> SubProcess*: WMLA_DLPD_PORT_27017_TCP_ADDR=172.30.236.207\n",
      "*Task <1> SubProcess*: DLI_RESULT_FS_SUBDIR_PERMISSION_ENFORCE=true\n",
      "*Task <1> SubProcess*: PYTHONPATH=/opt/wmla-mk/sdk/python::/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/dli_utils/:/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/dataset/:/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/compression/:/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/3rdparty/py4j-0.10.4-src.zip:/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/3rdparty/pyspark.zip:/gpfs/myresultfs/dse_user:/gpfs/myresultfs/dse_user/lib/python3.7/site-packages\n",
      "*Task <1> SubProcess*: WMLA_MSS_PORT=tcp://172.30.6.5:9080\n",
      "*Task <1> SubProcess*: WMLA_GUI_PORT_8443_TCP=tcp://172.30.21.215:8443\n",
      "*Task <1> SubProcess*: JOB_ID=wmla-327\n",
      "*Task <1> SubProcess*: NSS_SDB_USE_CACHE=no\n",
      "*Task <1> SubProcess*: CONDA_PYTHON_EXE=/opt/anaconda3/bin/python\n",
      "*Task <1> SubProcess*: MPI_ROOT=/opt/anaconda3/envs/dlipy3\n",
      "*Task <1> SubProcess*: WMLA_MSD_PORT_10000_TCP_ADDR=172.30.135.197\n",
      "*Task <1> SubProcess*: REDHARE_UNLIMITED_RETRY=false\n",
      "*Task <1> SubProcess*: JOB_LOG_PATH=/wmla-logging/dli/wmla-327\n",
      "*Task <1> SubProcess*: WMLA_MSS_PORT_9080_TCP_ADDR=172.30.6.5\n",
      "*Task <1> SubProcess*: WMLA_GUI_PORT=tcp://172.30.21.215:8443\n",
      "*Task <1> SubProcess*: WMLA_PROMETHEUS_PORT_9090_TCP_PORT=9090\n",
      "*Task <1> SubProcess*: JOB_UUID=ce2ba1fb-e62c-4172-81f5-9731ca0adf8c\n",
      "*Task <1> SubProcess*: IS_K8S=ON\n",
      "*Task <1> SubProcess*: REDHARE_PYTHONPATH=/opt/wmla-mk/sdk/python\n",
      "*Task <1> SubProcess*: MSD_MK_ID=ce2ba1fb-e62c-4172-81f5-9731ca0adf8c\n",
      "*Task <1> SubProcess*: WMLA_DLPD_PORT=tcp://172.30.236.207:27017\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_PUBLIC_PORT_493_TCP_PROTO=tcp\n",
      "*Task <1> SubProcess*: WMLA_LOGSTASH_SERVICE_SERVICE_HOST=172.30.46.129\n",
      "*Task <1> SubProcess*: WMLA_GRAFANA_PORT_3000_TCP_ADDR=172.30.4.186\n",
      "*Task <1> SubProcess*: WMLA_MSD_SERVICE_PORT_RPC=10000\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_HUB_PORT_8081_TCP_PORT=8081\n",
      "*Task <1> SubProcess*: WMLA_DLPD_PORT_9243_TCP=tcp://172.30.236.207:9243\n",
      "*Task <1> SubProcess*: LOGSTASH_INSTANCE_URL=wmla-logstash-service:5043\n",
      "*Task <1> SubProcess*: MSD_POD_HOST_IP=10.11.14.126\n",
      "*Task <1> SubProcess*: CONDA_DEFAULT_ENV=dlipy3\n",
      "*Task <1> SubProcess*: OMP_NUM_THREADS=16\n",
      "*Task <1> SubProcess*: WMLA_MSD_PORT_9080_TCP_ADDR=172.30.135.197\n",
      "*Task <1> SubProcess*: WMLA_EDI_PORT_8889_TCP=tcp://172.30.191.192:8889\n",
      "*Task <1> SubProcess*: WMLA_DLPD_SERVICE_PORT_DL_REST_PORT=9243\n",
      "*Task <1> SubProcess*: WMLA_GUI_PORT_8443_TCP_PROTO=tcp\n",
      "*Task <1> SubProcess*: WMLA_INFERENCE_PORT=tcp://172.30.204.123:9000\n",
      "*Task <1> SubProcess*: WMLA_DLPD_SERVICE_PORT_MONGODB_PORT=27017\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_ENTERPRISE_GATEWAY_SERVICE_PORT_HTTP=8888\n",
      "*Task <1> SubProcess*: WMLA_EDI_PORT_9010_TCP=tcp://172.30.191.192:9010\n",
      "*Task <1> SubProcess*: DLI_SC_RUNASGROUP=1000590000\n",
      "*Task <1> SubProcess*: FRAME_WORK=PyTorch\n",
      "*Task <1> SubProcess*: DLI_SC_RUNASUSER=1000590000\n",
      "*Task <1> SubProcess*: DLI_SC_FSGROUP=1000590000\n",
      "*Task <1> SubProcess*: WMLA_MSD_PORT=tcp://172.30.135.197:10000\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_PUBLIC_SERVICE_PORT_HTTPS=493\n",
      "*Task <1> SubProcess*: WMLA_GRAFANA_PORT=tcp://172.30.4.186:3000\n",
      "*Task <1> SubProcess*: KUBERNETES_PORT_443_TCP_ADDR=172.30.0.1\n",
      "*Task <1> SubProcess*: WMLA_INGRESS_SERVICE_HOST=172.30.177.20\n",
      "*Task <1> SubProcess*: WMLA_DLPD_SERVICE_PORT=27017\n",
      "*Task <1> SubProcess*: WMLA_MSD_PORT_10000_TCP=tcp://172.30.135.197:10000\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_ENTERPRISE_GATEWAY_PORT_8888_TCP_PROTO=tcp\n",
      "*Task <1> SubProcess*: WMLA_EDI_PORT=tcp://172.30.191.192:9000\n",
      "*Task <1> SubProcess*: WMLA_EDI_SERVICE_PORT_REST=9000\n",
      "*Task <1> SubProcess*: WMLA_EDI_PORT_8889_TCP_PROTO=tcp\n",
      "*Task <1> SubProcess*: WMLA_MSD_SERVICE_PORT_REST=9080\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_HUB_PORT_8081_TCP=tcp://172.30.130.151:8081\n",
      "*Task <1> SubProcess*: WMLA_AUTH_REST_SERVICE_PORT=3000\n",
      "*Task <1> SubProcess*: DLI_DEFAULT_CONDA_ENV_NAME=dlipy3\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_HUB_SERVICE_HOST=172.30.130.151\n",
      "*Task <1> SubProcess*: WMLA_ETCD_PORT_2379_TCP_PORT=2379\n",
      "*Task <1> SubProcess*: KUBERNETES_PORT_443_TCP=tcp://172.30.0.1:443\n",
      "*Task <1> SubProcess*: DLI_IS_ELASTIC=false\n",
      "*Task <1> SubProcess*: container=oci\n",
      "*Task <1> SubProcess*: WMLA_INFOSERVICE_PORT_8892_TCP=tcp://172.30.196.5:8892\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_ENTERPRISE_GATEWAY_PORT=tcp://172.30.125.158:8888\n",
      "*Task <1> SubProcess*: DLI_LAUNCHER_CALLBACK=/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/msd/msdcallback\n",
      "*Task <1> SubProcess*: BASE_LOG_PATH=/wmla-logging/dli\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_API_SERVICE_PORT=8001\n",
      "*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_API_PORT=tcp://172.30.226.243:8001\n",
      "*Task <1> SubProcess*: MSD_POD_GPU_DEVICES=GPU-a2ca4f2a-c061-0bbe-bd05-dc2c837f7554\n",
      "*Task <1> SubProcess*: _=/usr/bin/env\n",
      "*Task <1> SubProcess*: DLI_TASK_ID=0\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:34.960854 434 INFO Enter into pytorch_wrapper.sh\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:34.970053 434 INFO Check pytorch version\n",
      "*Task <1> SubProcess*: 1.3.1\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:36.137522 434 INFO Initialize\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:36.153248 434 INFO Setup file system datastore\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:36.184577 434 INFO RESULT_DIR=/gpfs/myresultfs/dse_user/batchworkdir/wmla-327, DATA_DIR=/gpfs/mydatafs, LOG_DIR=/gpfs/myresultfs/dse_user/batchworkdir/wmla-327/log/app.wmla-327-task12n-jbbhg, MODEL_FILE_LOCATION=/gpfs/myresultfs/dse_user/batchworkdir/wmla-327/_submitted_code\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:36.197517 434 INFO List files under DATA_DIR\n",
      "*Task <1> SubProcess*: $ls -l /gpfs/mydatafs\n",
      "*Task <1> SubProcess*: total 214776\n",
      "*Task <1> SubProcess*: drwxr-xr-x.  3 1000590000 1000590000        63 Dec 28 05:54 cifar10\n",
      "*Task <1> SubProcess*: drwxr-xr-x.  3 root       root              19 Dec 15 22:37 dataset\n",
      "*Task <1> SubProcess*: drwxr-xr-x.  3 1000590000 1000590000        18 Jan  7 22:28 java\n",
      "*Task <1> SubProcess*: drwxr-xr-x.  4 1000590000 1000590000        34 Dec 14 19:12 MNIST\n",
      "*Task <1> SubProcess*: drwxrwxrwx.  3 1000590000 1000590000        36 Dec 15 03:26 msdtool\n",
      "*Task <1> SubProcess*: drwxr-xr-x.  3 root       root              19 Dec 11 22:31 pytorch-mnist\n",
      "*Task <1> SubProcess*: drwxr-xr-x.  2       1001       1001        43 Dec 15 20:27 samaya\n",
      "*Task <1> SubProcess*: drwxrwxr-x.  3 1000590000 1000590000        53 Jan 12 23:28 simclr\n",
      "*Task <1> SubProcess*: drwxr-xr-x. 13 1000590000 1000590000       211 Jan  5 18:42 spark\n",
      "*Task <1> SubProcess*: -rw-r--r--.  1 root       root       219929956 Jan  5 18:38 spark-3.0.1-bin-hadoop2.7.tgz\n",
      "*Task <1> SubProcess*: drwxr-xr-x.  2 root       root               6 Dec 15 22:36 xgboost\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:36.226017 434 INFO Save log files under /gpfs/myresultfs/dse_user/batchworkdir/wmla-327/log/app.wmla-327-task12n-jbbhg\n",
      "*Task <1> SubProcess*: 2021-02-10 16:39:36.245737 434 INFO Start running user model\n",
      "*Task <1> SubProcess*: /gpfs/myresultfs/dse_user/batchworkdir/wmla-327/_submitted_code/main.py --epochs 1\n",
      "*Task <1> SubProcess*: Namespace(batch_size=128, cuda=True, epochs=1, lr=0.01)\n",
      "*Task <1> SubProcess*: Use cuda:  True\n",
      "*Task <1> SubProcess*: DATA_DIR: /gpfs/mydatafs\n",
      "*Task <1> SubProcess*: Files already downloaded and verified\n",
      "*Task <1> SubProcess*: Files already downloaded and verified\n",
      "*Task <1> SubProcess*: => using pytorch build-in model 'resnet18'\n",
      "*Task <1> SubProcess*: Total iterations: 391\n",
      "*Task <1> SubProcess*: RESULT_DIR: /gpfs/myresultfs/dse_user/batchworkdir/wmla-327\n",
      "*Task <1> SubProcess*: Running epoch 1 ... It might take several minutes for each epoch to run.\n",
      "*Task <1> SubProcess*: Train - batches : 1, average loss: 2.4147, accuracy: 15/128 (12%)\n",
      "*Task <1> SubProcess*: Train - batches : 2, average loss: 2.3836, accuracy: 27/256 (11%)\n",
      "*Task <1> SubProcess*: Train - batches : 3, average loss: 2.3746, accuracy: 40/384 (10%)\n",
      "*Task <1> SubProcess*: Train - batches : 4, average loss: 2.3545, accuracy: 54/512 (11%)\n",
      "*Task <1> SubProcess*: Train - batches : 5, average loss: 2.3369, accuracy: 71/640 (11%)\n",
      "*Task <1> SubProcess*: Train - batches : 6, average loss: 2.3242, accuracy: 95/768 (12%)\n",
      "*Task <1> SubProcess*: Train - batches : 7, average loss: 2.3176, accuracy: 110/896 (12%)\n",
      "*Task <1> SubProcess*: Train - batches : 8, average loss: 2.3134, accuracy: 127/1024 (12%)\n",
      "*Task <1> SubProcess*: Train - batches : 9, average loss: 2.3101, accuracy: 139/1152 (12%)\n",
      "*Task <1> SubProcess*: Train - batches : 10, average loss: 2.3040, accuracy: 159/1280 (12%)\n",
      "*Task <1> SubProcess*: Train - batches : 11, average loss: 2.3012, accuracy: 174/1408 (12%)\n",
      "*Task <1> SubProcess*: Train - batches : 12, average loss: 2.2949, accuracy: 196/1536 (13%)\n",
      "*Task <1> SubProcess*: Train - batches : 13, average loss: 2.2906, accuracy: 213/1664 (13%)\n",
      "*Task <1> SubProcess*: Train - batches : 14, average loss: 2.2838, accuracy: 240/1792 (13%)\n",
      "*Task <1> SubProcess*: Train - batches : 15, average loss: 2.2752, accuracy: 276/1920 (14%)\n",
      "*Task <1> SubProcess*: Train - batches : 16, average loss: 2.2711, accuracy: 298/2048 (15%)\n",
      "*Task <1> SubProcess*: Train - batches : 17, average loss: 2.2682, accuracy: 322/2176 (15%)\n",
      "*Task <1> SubProcess*: Train - batches : 18, average loss: 2.2607, accuracy: 356/2304 (15%)\n",
      "*Task <1> SubProcess*: Train - batches : 19, average loss: 2.2536, accuracy: 389/2432 (16%)\n",
      "*Task <1> SubProcess*: Train - batches : 20, average loss: 2.2498, accuracy: 414/2560 (16%)\n",
      "*Task <1> SubProcess*: Train - batches : 21, average loss: 2.2437, accuracy: 444/2688 (17%)\n",
      "*Task <1> SubProcess*: Train - batches : 22, average loss: 2.2398, accuracy: 471/2816 (17%)\n",
      "*Task <1> SubProcess*: Train - batches : 23, average loss: 2.2339, accuracy: 511/2944 (17%)\n",
      "*Task <1> SubProcess*: Train - batches : 24, average loss: 2.2304, accuracy: 541/3072 (18%)\n",
      "*Task <1> SubProcess*: Train - batches : 25, average loss: 2.2269, accuracy: 566/3200 (18%)\n",
      "*Task <1> SubProcess*: Train - batches : 26, average loss: 2.2214, accuracy: 602/3328 (18%)\n",
      "*Task <1> SubProcess*: Train - batches : 27, average loss: 2.2155, accuracy: 639/3456 (18%)\n",
      "*Task <1> SubProcess*: Train - batches : 28, average loss: 2.2104, accuracy: 676/3584 (19%)\n",
      "*Task <1> SubProcess*: Train - batches : 29, average loss: 2.2086, accuracy: 700/3712 (19%)\n",
      "*Task <1> SubProcess*: Train - batches : 30, average loss: 2.2037, accuracy: 736/3840 (19%)\n",
      "*Task <1> SubProcess*: Train - batches : 31, average loss: 2.2000, accuracy: 767/3968 (19%)\n",
      "*Task <1> SubProcess*: Train - batches : 32, average loss: 2.1952, accuracy: 806/4096 (20%)\n",
      "*Task <1> SubProcess*: Train - batches : 33, average loss: 2.1892, accuracy: 850/4224 (20%)\n",
      "*Task <1> SubProcess*: Train - batches : 34, average loss: 2.1861, accuracy: 885/4352 (20%)\n",
      "*Task <1> SubProcess*: Train - batches : 35, average loss: 2.1828, accuracy: 908/4480 (20%)\n",
      "*Task <1> SubProcess*: Train - batches : 36, average loss: 2.1799, accuracy: 933/4608 (20%)\n",
      "*Task <1> SubProcess*: Train - batches : 37, average loss: 2.1764, accuracy: 970/4736 (20%)\n",
      "*Task <1> SubProcess*: Train - batches : 38, average loss: 2.1737, accuracy: 1000/4864 (21%)\n",
      "*Task <1> SubProcess*: Train - batches : 39, average loss: 2.1705, accuracy: 1035/4992 (21%)\n",
      "*Task <1> SubProcess*: Train - batches : 40, average loss: 2.1682, accuracy: 1066/5120 (21%)\n",
      "*Task <1> SubProcess*: Train - batches : 41, average loss: 2.1640, accuracy: 1099/5248 (21%)\n",
      "*Task <1> SubProcess*: Train - batches : 42, average loss: 2.1621, accuracy: 1139/5376 (21%)\n",
      "*Task <1> SubProcess*: Train - batches : 43, average loss: 2.1594, accuracy: 1164/5504 (21%)\n",
      "*Task <1> SubProcess*: Train - batches : 44, average loss: 2.1540, accuracy: 1210/5632 (21%)\n",
      "*Task <1> SubProcess*: Train - batches : 45, average loss: 2.1496, accuracy: 1247/5760 (22%)\n",
      "*Task <1> SubProcess*: Train - batches : 46, average loss: 2.1457, accuracy: 1286/5888 (22%)\n",
      "*Task <1> SubProcess*: Train - batches : 47, average loss: 2.1434, accuracy: 1319/6016 (22%)\n",
      "*Task <1> SubProcess*: Train - batches : 48, average loss: 2.1392, accuracy: 1355/6144 (22%)\n",
      "*Task <1> SubProcess*: Train - batches : 49, average loss: 2.1369, accuracy: 1385/6272 (22%)\n",
      "*Task <1> SubProcess*: Train - batches : 50, average loss: 2.1359, accuracy: 1418/6400 (22%)\n",
      "*Task <1> SubProcess*: Train - batches : 51, average loss: 2.1325, accuracy: 1459/6528 (22%)\n",
      "*Task <1> SubProcess*: Train - batches : 52, average loss: 2.1299, accuracy: 1494/6656 (22%)\n",
      "*Task <1> SubProcess*: Train - batches : 53, average loss: 2.1260, accuracy: 1532/6784 (23%)\n",
      "*Task <1> SubProcess*: Train - batches : 54, average loss: 2.1218, accuracy: 1568/6912 (23%)\n",
      "*Task <1> SubProcess*: Train - batches : 55, average loss: 2.1203, accuracy: 1597/7040 (23%)\n",
      "*Task <1> SubProcess*: Train - batches : 56, average loss: 2.1184, accuracy: 1626/7168 (23%)\n",
      "*Task <1> SubProcess*: Train - batches : 57, average loss: 2.1165, accuracy: 1664/7296 (23%)\n",
      "*Task <1> SubProcess*: Train - batches : 58, average loss: 2.1144, accuracy: 1701/7424 (23%)\n",
      "*Task <1> SubProcess*: Train - batches : 59, average loss: 2.1116, accuracy: 1733/7552 (23%)\n",
      "*Task <1> SubProcess*: Train - batches : 60, average loss: 2.1090, accuracy: 1766/7680 (23%)\n",
      "*Task <1> SubProcess*: Train - batches : 61, average loss: 2.1061, accuracy: 1804/7808 (23%)\n",
      "*Task <1> SubProcess*: Train - batches : 62, average loss: 2.1047, accuracy: 1832/7936 (23%)\n",
      "*Task <1> SubProcess*: Train - batches : 63, average loss: 2.1018, accuracy: 1869/8064 (23%)\n",
      "*Task <1> SubProcess*: Train - batches : 64, average loss: 2.0990, accuracy: 1906/8192 (23%)\n",
      "*Task <1> SubProcess*: Train - batches : 65, average loss: 2.0960, accuracy: 1942/8320 (23%)\n",
      "*Task <1> SubProcess*: Train - batches : 66, average loss: 2.0936, accuracy: 1982/8448 (23%)\n",
      "*Task <1> SubProcess*: Train - batches : 67, average loss: 2.0929, accuracy: 2012/8576 (23%)\n",
      "*Task <1> SubProcess*: Train - batches : 68, average loss: 2.0898, accuracy: 2052/8704 (24%)\n",
      "*Task <1> SubProcess*: Train - batches : 69, average loss: 2.0866, accuracy: 2098/8832 (24%)\n",
      "*Task <1> SubProcess*: Train - batches : 70, average loss: 2.0842, accuracy: 2136/8960 (24%)\n",
      "*Task <1> SubProcess*: Train - batches : 71, average loss: 2.0824, accuracy: 2179/9088 (24%)\n",
      "*Task <1> SubProcess*: Train - batches : 72, average loss: 2.0795, accuracy: 2219/9216 (24%)\n",
      "*Task <1> SubProcess*: Train - batches : 73, average loss: 2.0776, accuracy: 2255/9344 (24%)\n",
      "*Task <1> SubProcess*: Train - batches : 74, average loss: 2.0765, accuracy: 2287/9472 (24%)\n",
      "*Task <1> SubProcess*: Train - batches : 75, average loss: 2.0741, accuracy: 2328/9600 (24%)\n",
      "*Task <1> SubProcess*: Train - batches : 76, average loss: 2.0719, accuracy: 2362/9728 (24%)\n",
      "*Task <1> SubProcess*: Train - batches : 77, average loss: 2.0691, accuracy: 2402/9856 (24%)\n",
      "*Task <1> SubProcess*: Train - batches : 78, average loss: 2.0682, accuracy: 2429/9984 (24%)\n",
      "*Task <1> SubProcess*: Train - batches : 79, average loss: 2.0659, accuracy: 2470/10112 (24%)\n",
      "*Task <1> SubProcess*: Train - batches : 80, average loss: 2.0636, accuracy: 2504/10240 (24%)\n",
      "*Task <1> SubProcess*: Train - batches : 81, average loss: 2.0605, accuracy: 2544/10368 (25%)\n",
      "*Task <1> SubProcess*: Train - batches : 82, average loss: 2.0582, accuracy: 2582/10496 (25%)\n",
      "*Task <1> SubProcess*: Train - batches : 83, average loss: 2.0548, accuracy: 2625/10624 (25%)\n",
      "*Task <1> SubProcess*: Train - batches : 84, average loss: 2.0522, accuracy: 2668/10752 (25%)\n",
      "*Task <1> SubProcess*: Train - batches : 85, average loss: 2.0507, accuracy: 2711/10880 (25%)\n",
      "*Task <1> SubProcess*: Train - batches : 86, average loss: 2.0477, accuracy: 2756/11008 (25%)\n",
      "*Task <1> SubProcess*: Train - batches : 87, average loss: 2.0460, accuracy: 2796/11136 (25%)\n",
      "*Task <1> SubProcess*: Train - batches : 88, average loss: 2.0447, accuracy: 2831/11264 (25%)\n",
      "*Task <1> SubProcess*: Train - batches : 89, average loss: 2.0429, accuracy: 2859/11392 (25%)\n",
      "*Task <1> SubProcess*: Train - batches : 90, average loss: 2.0410, accuracy: 2895/11520 (25%)\n",
      "*Task <1> SubProcess*: Train - batches : 91, average loss: 2.0385, accuracy: 2934/11648 (25%)\n",
      "*Task <1> SubProcess*: Train - batches : 92, average loss: 2.0357, accuracy: 2979/11776 (25%)\n",
      "*Task <1> SubProcess*: Train - batches : 93, average loss: 2.0337, accuracy: 3018/11904 (25%)\n",
      "*Task <1> SubProcess*: Train - batches : 94, average loss: 2.0312, accuracy: 3057/12032 (25%)\n",
      "*Task <1> SubProcess*: Train - batches : 95, average loss: 2.0304, accuracy: 3088/12160 (25%)\n",
      "*Task <1> SubProcess*: Train - batches : 96, average loss: 2.0276, accuracy: 3131/12288 (25%)\n",
      "*Task <1> SubProcess*: Train - batches : 97, average loss: 2.0258, accuracy: 3173/12416 (26%)\n",
      "*Task <1> SubProcess*: Train - batches : 98, average loss: 2.0242, accuracy: 3213/12544 (26%)\n",
      "*Task <1> SubProcess*: Train - batches : 99, average loss: 2.0223, accuracy: 3250/12672 (26%)\n",
      "*Task <1> SubProcess*: Train - batches : 100, average loss: 2.0206, accuracy: 3289/12800 (26%)\n",
      "*Task <1> SubProcess*: Train - batches : 101, average loss: 2.0187, accuracy: 3330/12928 (26%)\n",
      "*Task <1> SubProcess*: Train - batches : 102, average loss: 2.0168, accuracy: 3370/13056 (26%)\n",
      "*Task <1> SubProcess*: Train - batches : 103, average loss: 2.0146, accuracy: 3410/13184 (26%)\n",
      "*Task <1> SubProcess*: Train - batches : 104, average loss: 2.0124, accuracy: 3451/13312 (26%)\n",
      "*Task <1> SubProcess*: Train - batches : 105, average loss: 2.0111, accuracy: 3487/13440 (26%)\n",
      "*Task <1> SubProcess*: Train - batches : 106, average loss: 2.0087, accuracy: 3525/13568 (26%)\n",
      "*Task <1> SubProcess*: Train - batches : 107, average loss: 2.0073, accuracy: 3561/13696 (26%)\n",
      "*Task <1> SubProcess*: Train - batches : 108, average loss: 2.0060, accuracy: 3600/13824 (26%)\n",
      "*Task <1> SubProcess*: Train - batches : 109, average loss: 2.0037, accuracy: 3646/13952 (26%)\n",
      "*Task <1> SubProcess*: Train - batches : 110, average loss: 2.0022, accuracy: 3689/14080 (26%)\n",
      "*Task <1> SubProcess*: Train - batches : 111, average loss: 2.0008, accuracy: 3738/14208 (26%)\n",
      "*Task <1> SubProcess*: Train - batches : 112, average loss: 1.9991, accuracy: 3780/14336 (26%)\n",
      "*Task <1> SubProcess*: Train - batches : 113, average loss: 1.9978, accuracy: 3822/14464 (26%)\n",
      "*Task <1> SubProcess*: Train - batches : 114, average loss: 1.9958, accuracy: 3864/14592 (26%)\n",
      "*Task <1> SubProcess*: Train - batches : 115, average loss: 1.9941, accuracy: 3905/14720 (27%)\n",
      "*Task <1> SubProcess*: Train - batches : 116, average loss: 1.9932, accuracy: 3947/14848 (27%)\n",
      "*Task <1> SubProcess*: Train - batches : 117, average loss: 1.9914, accuracy: 3994/14976 (27%)\n",
      "*Task <1> SubProcess*: Train - batches : 118, average loss: 1.9900, accuracy: 4034/15104 (27%)\n",
      "*Task <1> SubProcess*: Train - batches : 119, average loss: 1.9883, accuracy: 4086/15232 (27%)\n",
      "*Task <1> SubProcess*: Train - batches : 120, average loss: 1.9872, accuracy: 4127/15360 (27%)\n",
      "*Task <1> SubProcess*: Train - batches : 121, average loss: 1.9861, accuracy: 4169/15488 (27%)\n",
      "*Task <1> SubProcess*: Train - batches : 122, average loss: 1.9844, accuracy: 4215/15616 (27%)\n",
      "*Task <1> SubProcess*: Train - batches : 123, average loss: 1.9838, accuracy: 4260/15744 (27%)\n",
      "*Task <1> SubProcess*: Train - batches : 124, average loss: 1.9832, accuracy: 4294/15872 (27%)\n",
      "*Task <1> SubProcess*: Train - batches : 125, average loss: 1.9822, accuracy: 4327/16000 (27%)\n",
      "*Task <1> SubProcess*: Train - batches : 126, average loss: 1.9810, accuracy: 4366/16128 (27%)\n",
      "*Task <1> SubProcess*: Train - batches : 127, average loss: 1.9794, accuracy: 4412/16256 (27%)\n",
      "*Task <1> SubProcess*: Train - batches : 128, average loss: 1.9773, accuracy: 4457/16384 (27%)\n",
      "*Task <1> SubProcess*: Train - batches : 129, average loss: 1.9755, accuracy: 4505/16512 (27%)\n",
      "*Task <1> SubProcess*: Train - batches : 130, average loss: 1.9731, accuracy: 4564/16640 (27%)\n",
      "*Task <1> SubProcess*: Train - batches : 131, average loss: 1.9718, accuracy: 4610/16768 (27%)\n",
      "*Task <1> SubProcess*: Train - batches : 132, average loss: 1.9699, accuracy: 4658/16896 (28%)\n",
      "*Task <1> SubProcess*: Train - batches : 133, average loss: 1.9688, accuracy: 4696/17024 (28%)\n",
      "*Task <1> SubProcess*: Train - batches : 134, average loss: 1.9668, accuracy: 4744/17152 (28%)\n",
      "*Task <1> SubProcess*: Train - batches : 135, average loss: 1.9658, accuracy: 4782/17280 (28%)\n",
      "*Task <1> SubProcess*: Train - batches : 136, average loss: 1.9646, accuracy: 4831/17408 (28%)\n",
      "*Task <1> SubProcess*: Train - batches : 137, average loss: 1.9627, accuracy: 4876/17536 (28%)\n",
      "*Task <1> SubProcess*: Train - batches : 138, average loss: 1.9621, accuracy: 4916/17664 (28%)\n",
      "*Task <1> SubProcess*: Train - batches : 139, average loss: 1.9600, accuracy: 4966/17792 (28%)\n",
      "*Task <1> SubProcess*: Train - batches : 140, average loss: 1.9584, accuracy: 5009/17920 (28%)\n",
      "*Task <1> SubProcess*: Train - batches : 141, average loss: 1.9566, accuracy: 5057/18048 (28%)\n",
      "*Task <1> SubProcess*: Train - batches : 142, average loss: 1.9552, accuracy: 5101/18176 (28%)\n",
      "*Task <1> SubProcess*: Train - batches : 143, average loss: 1.9541, accuracy: 5147/18304 (28%)\n",
      "*Task <1> SubProcess*: Train - batches : 144, average loss: 1.9529, accuracy: 5187/18432 (28%)\n",
      "*Task <1> SubProcess*: Train - batches : 145, average loss: 1.9518, accuracy: 5233/18560 (28%)\n",
      "*Task <1> SubProcess*: Train - batches : 146, average loss: 1.9511, accuracy: 5272/18688 (28%)\n",
      "*Task <1> SubProcess*: Train - batches : 147, average loss: 1.9501, accuracy: 5316/18816 (28%)\n",
      "*Task <1> SubProcess*: Train - batches : 148, average loss: 1.9486, accuracy: 5358/18944 (28%)\n",
      "*Task <1> SubProcess*: Train - batches : 149, average loss: 1.9473, accuracy: 5404/19072 (28%)\n",
      "*Task <1> SubProcess*: Train - batches : 150, average loss: 1.9455, accuracy: 5444/19200 (28%)\n",
      "*Task <1> SubProcess*: Train - batches : 151, average loss: 1.9444, accuracy: 5488/19328 (28%)\n",
      "*Task <1> SubProcess*: Train - batches : 152, average loss: 1.9434, accuracy: 5535/19456 (28%)\n",
      "*Task <1> SubProcess*: Train - batches : 153, average loss: 1.9426, accuracy: 5575/19584 (28%)\n",
      "*Task <1> SubProcess*: Train - batches : 154, average loss: 1.9424, accuracy: 5618/19712 (29%)\n",
      "*Task <1> SubProcess*: Train - batches : 155, average loss: 1.9415, accuracy: 5654/19840 (28%)\n",
      "*Task <1> SubProcess*: Train - batches : 156, average loss: 1.9410, accuracy: 5692/19968 (29%)\n",
      "*Task <1> SubProcess*: Train - batches : 157, average loss: 1.9404, accuracy: 5739/20096 (29%)\n",
      "*Task <1> SubProcess*: Train - batches : 158, average loss: 1.9395, accuracy: 5778/20224 (29%)\n",
      "*Task <1> SubProcess*: Train - batches : 159, average loss: 1.9383, accuracy: 5825/20352 (29%)\n",
      "*Task <1> SubProcess*: Train - batches : 160, average loss: 1.9371, accuracy: 5869/20480 (29%)\n",
      "*Task <1> SubProcess*: Train - batches : 161, average loss: 1.9362, accuracy: 5913/20608 (29%)\n",
      "*Task <1> SubProcess*: Train - batches : 162, average loss: 1.9350, accuracy: 5960/20736 (29%)\n",
      "*Task <1> SubProcess*: Train - batches : 163, average loss: 1.9341, accuracy: 6007/20864 (29%)\n",
      "*Task <1> SubProcess*: Train - batches : 164, average loss: 1.9333, accuracy: 6053/20992 (29%)\n",
      "*Task <1> SubProcess*: Train - batches : 165, average loss: 1.9319, accuracy: 6102/21120 (29%)\n",
      "*Task <1> SubProcess*: Train - batches : 166, average loss: 1.9309, accuracy: 6143/21248 (29%)\n",
      "*Task <1> SubProcess*: Train - batches : 167, average loss: 1.9298, accuracy: 6194/21376 (29%)\n",
      "*Task <1> SubProcess*: Train - batches : 168, average loss: 1.9295, accuracy: 6235/21504 (29%)\n",
      "*Task <1> SubProcess*: Train - batches : 169, average loss: 1.9289, accuracy: 6278/21632 (29%)\n",
      "*Task <1> SubProcess*: Train - batches : 170, average loss: 1.9273, accuracy: 6323/21760 (29%)\n",
      "*Task <1> SubProcess*: Train - batches : 171, average loss: 1.9263, accuracy: 6369/21888 (29%)\n",
      "*Task <1> SubProcess*: Train - batches : 172, average loss: 1.9254, accuracy: 6412/22016 (29%)\n",
      "*Task <1> SubProcess*: Train - batches : 173, average loss: 1.9252, accuracy: 6448/22144 (29%)\n",
      "*Task <1> SubProcess*: Train - batches : 174, average loss: 1.9244, accuracy: 6491/22272 (29%)\n",
      "*Task <1> SubProcess*: Train - batches : 175, average loss: 1.9232, accuracy: 6542/22400 (29%)\n",
      "*Task <1> SubProcess*: Train - batches : 176, average loss: 1.9225, accuracy: 6588/22528 (29%)\n",
      "*Task <1> SubProcess*: Train - batches : 177, average loss: 1.9219, accuracy: 6628/22656 (29%)\n",
      "*Task <1> SubProcess*: Train - batches : 178, average loss: 1.9212, accuracy: 6676/22784 (29%)\n",
      "*Task <1> SubProcess*: Train - batches : 179, average loss: 1.9203, accuracy: 6723/22912 (29%)\n",
      "*Task <1> SubProcess*: Train - batches : 180, average loss: 1.9190, accuracy: 6779/23040 (29%)\n",
      "*Task <1> SubProcess*: Train - batches : 181, average loss: 1.9179, accuracy: 6827/23168 (29%)\n",
      "*Task <1> SubProcess*: Train - batches : 182, average loss: 1.9166, accuracy: 6876/23296 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 183, average loss: 1.9160, accuracy: 6914/23424 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 184, average loss: 1.9153, accuracy: 6956/23552 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 185, average loss: 1.9141, accuracy: 7004/23680 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 186, average loss: 1.9128, accuracy: 7052/23808 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 187, average loss: 1.9120, accuracy: 7094/23936 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 188, average loss: 1.9110, accuracy: 7149/24064 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 189, average loss: 1.9102, accuracy: 7189/24192 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 190, average loss: 1.9095, accuracy: 7234/24320 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 191, average loss: 1.9078, accuracy: 7297/24448 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 192, average loss: 1.9070, accuracy: 7337/24576 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 193, average loss: 1.9056, accuracy: 7390/24704 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 194, average loss: 1.9047, accuracy: 7438/24832 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 195, average loss: 1.9038, accuracy: 7489/24960 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 196, average loss: 1.9029, accuracy: 7538/25088 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 197, average loss: 1.9016, accuracy: 7593/25216 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 198, average loss: 1.9010, accuracy: 7645/25344 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 199, average loss: 1.9002, accuracy: 7697/25472 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 200, average loss: 1.8991, accuracy: 7745/25600 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 201, average loss: 1.8980, accuracy: 7794/25728 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 202, average loss: 1.8969, accuracy: 7834/25856 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 203, average loss: 1.8961, accuracy: 7882/25984 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 204, average loss: 1.8953, accuracy: 7929/26112 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 205, average loss: 1.8944, accuracy: 7973/26240 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 206, average loss: 1.8936, accuracy: 8024/26368 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 207, average loss: 1.8925, accuracy: 8073/26496 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 208, average loss: 1.8928, accuracy: 8108/26624 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 209, average loss: 1.8917, accuracy: 8159/26752 (30%)\n",
      "*Task <1> SubProcess*: Train - batches : 210, average loss: 1.8911, accuracy: 8207/26880 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 211, average loss: 1.8893, accuracy: 8267/27008 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 212, average loss: 1.8885, accuracy: 8316/27136 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 213, average loss: 1.8877, accuracy: 8363/27264 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 214, average loss: 1.8864, accuracy: 8425/27392 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 215, average loss: 1.8852, accuracy: 8483/27520 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 216, average loss: 1.8840, accuracy: 8535/27648 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 217, average loss: 1.8830, accuracy: 8585/27776 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 218, average loss: 1.8818, accuracy: 8637/27904 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 219, average loss: 1.8813, accuracy: 8684/28032 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 220, average loss: 1.8803, accuracy: 8727/28160 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 221, average loss: 1.8794, accuracy: 8777/28288 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 222, average loss: 1.8782, accuracy: 8824/28416 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 223, average loss: 1.8773, accuracy: 8871/28544 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 224, average loss: 1.8771, accuracy: 8918/28672 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 225, average loss: 1.8767, accuracy: 8961/28800 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 226, average loss: 1.8762, accuracy: 9007/28928 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 227, average loss: 1.8760, accuracy: 9049/29056 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 228, average loss: 1.8749, accuracy: 9104/29184 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 229, average loss: 1.8742, accuracy: 9145/29312 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 230, average loss: 1.8734, accuracy: 9195/29440 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 231, average loss: 1.8728, accuracy: 9240/29568 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 232, average loss: 1.8718, accuracy: 9299/29696 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 233, average loss: 1.8713, accuracy: 9347/29824 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 234, average loss: 1.8707, accuracy: 9396/29952 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 235, average loss: 1.8697, accuracy: 9441/30080 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 236, average loss: 1.8687, accuracy: 9493/30208 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 237, average loss: 1.8675, accuracy: 9540/30336 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 238, average loss: 1.8666, accuracy: 9592/30464 (31%)\n",
      "*Task <1> SubProcess*: Train - batches : 239, average loss: 1.8659, accuracy: 9643/30592 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 240, average loss: 1.8653, accuracy: 9687/30720 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 241, average loss: 1.8649, accuracy: 9737/30848 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 242, average loss: 1.8645, accuracy: 9785/30976 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 243, average loss: 1.8635, accuracy: 9830/31104 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 244, average loss: 1.8624, accuracy: 9893/31232 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 245, average loss: 1.8616, accuracy: 9941/31360 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 246, average loss: 1.8611, accuracy: 9994/31488 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 247, average loss: 1.8605, accuracy: 10040/31616 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 248, average loss: 1.8600, accuracy: 10086/31744 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 249, average loss: 1.8592, accuracy: 10128/31872 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 250, average loss: 1.8586, accuracy: 10164/32000 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 251, average loss: 1.8578, accuracy: 10218/32128 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 252, average loss: 1.8569, accuracy: 10268/32256 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 253, average loss: 1.8557, accuracy: 10319/32384 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 254, average loss: 1.8549, accuracy: 10364/32512 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 255, average loss: 1.8539, accuracy: 10414/32640 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 256, average loss: 1.8533, accuracy: 10464/32768 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 257, average loss: 1.8520, accuracy: 10525/32896 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 258, average loss: 1.8519, accuracy: 10568/33024 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 259, average loss: 1.8512, accuracy: 10624/33152 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 260, average loss: 1.8503, accuracy: 10673/33280 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 261, average loss: 1.8494, accuracy: 10725/33408 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 262, average loss: 1.8487, accuracy: 10773/33536 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 263, average loss: 1.8483, accuracy: 10818/33664 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 264, average loss: 1.8475, accuracy: 10864/33792 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 265, average loss: 1.8468, accuracy: 10909/33920 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 266, average loss: 1.8462, accuracy: 10962/34048 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 267, average loss: 1.8456, accuracy: 11009/34176 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 268, average loss: 1.8448, accuracy: 11060/34304 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 269, average loss: 1.8439, accuracy: 11115/34432 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 270, average loss: 1.8429, accuracy: 11169/34560 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 271, average loss: 1.8428, accuracy: 11219/34688 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 272, average loss: 1.8417, accuracy: 11277/34816 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 273, average loss: 1.8409, accuracy: 11319/34944 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 274, average loss: 1.8401, accuracy: 11371/35072 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 275, average loss: 1.8394, accuracy: 11420/35200 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 276, average loss: 1.8390, accuracy: 11460/35328 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 277, average loss: 1.8383, accuracy: 11517/35456 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 278, average loss: 1.8378, accuracy: 11564/35584 (32%)\n",
      "*Task <1> SubProcess*: Train - batches : 279, average loss: 1.8369, accuracy: 11622/35712 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 280, average loss: 1.8364, accuracy: 11666/35840 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 281, average loss: 1.8357, accuracy: 11710/35968 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 282, average loss: 1.8348, accuracy: 11768/36096 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 283, average loss: 1.8342, accuracy: 11828/36224 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 284, average loss: 1.8336, accuracy: 11875/36352 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 285, average loss: 1.8326, accuracy: 11928/36480 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 286, average loss: 1.8320, accuracy: 11979/36608 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 287, average loss: 1.8312, accuracy: 12032/36736 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 288, average loss: 1.8304, accuracy: 12078/36864 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 289, average loss: 1.8301, accuracy: 12119/36992 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 290, average loss: 1.8292, accuracy: 12171/37120 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 291, average loss: 1.8287, accuracy: 12219/37248 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 292, average loss: 1.8275, accuracy: 12280/37376 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 293, average loss: 1.8267, accuracy: 12339/37504 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 294, average loss: 1.8263, accuracy: 12385/37632 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 295, average loss: 1.8254, accuracy: 12437/37760 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 296, average loss: 1.8247, accuracy: 12495/37888 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 297, average loss: 1.8246, accuracy: 12531/38016 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 298, average loss: 1.8243, accuracy: 12574/38144 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 299, average loss: 1.8237, accuracy: 12626/38272 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 300, average loss: 1.8228, accuracy: 12681/38400 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 301, average loss: 1.8224, accuracy: 12732/38528 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 302, average loss: 1.8219, accuracy: 12781/38656 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 303, average loss: 1.8211, accuracy: 12837/38784 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 304, average loss: 1.8205, accuracy: 12884/38912 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 305, average loss: 1.8200, accuracy: 12928/39040 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 306, average loss: 1.8196, accuracy: 12969/39168 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 307, average loss: 1.8189, accuracy: 13023/39296 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 308, average loss: 1.8179, accuracy: 13083/39424 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 309, average loss: 1.8170, accuracy: 13139/39552 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 310, average loss: 1.8163, accuracy: 13192/39680 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 311, average loss: 1.8158, accuracy: 13243/39808 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 312, average loss: 1.8153, accuracy: 13297/39936 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 313, average loss: 1.8143, accuracy: 13349/40064 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 314, average loss: 1.8132, accuracy: 13402/40192 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 315, average loss: 1.8123, accuracy: 13454/40320 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 316, average loss: 1.8116, accuracy: 13511/40448 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 317, average loss: 1.8112, accuracy: 13561/40576 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 318, average loss: 1.8108, accuracy: 13614/40704 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 319, average loss: 1.8102, accuracy: 13667/40832 (33%)\n",
      "*Task <1> SubProcess*: Train - batches : 320, average loss: 1.8097, accuracy: 13724/40960 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 321, average loss: 1.8095, accuracy: 13766/41088 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 322, average loss: 1.8089, accuracy: 13812/41216 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 323, average loss: 1.8087, accuracy: 13859/41344 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 324, average loss: 1.8080, accuracy: 13911/41472 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 325, average loss: 1.8075, accuracy: 13960/41600 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 326, average loss: 1.8068, accuracy: 14012/41728 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 327, average loss: 1.8065, accuracy: 14055/41856 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 328, average loss: 1.8062, accuracy: 14106/41984 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 329, average loss: 1.8056, accuracy: 14162/42112 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 330, average loss: 1.8048, accuracy: 14222/42240 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 331, average loss: 1.8041, accuracy: 14271/42368 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 332, average loss: 1.8031, accuracy: 14329/42496 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 333, average loss: 1.8025, accuracy: 14379/42624 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 334, average loss: 1.8019, accuracy: 14433/42752 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 335, average loss: 1.8008, accuracy: 14490/42880 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 336, average loss: 1.7999, accuracy: 14545/43008 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 337, average loss: 1.7991, accuracy: 14601/43136 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 338, average loss: 1.7989, accuracy: 14653/43264 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 339, average loss: 1.7984, accuracy: 14709/43392 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 340, average loss: 1.7979, accuracy: 14763/43520 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 341, average loss: 1.7972, accuracy: 14814/43648 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 342, average loss: 1.7962, accuracy: 14881/43776 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 343, average loss: 1.7963, accuracy: 14925/43904 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 344, average loss: 1.7960, accuracy: 14974/44032 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 345, average loss: 1.7957, accuracy: 15014/44160 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 346, average loss: 1.7953, accuracy: 15067/44288 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 347, average loss: 1.7950, accuracy: 15118/44416 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 348, average loss: 1.7945, accuracy: 15167/44544 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 349, average loss: 1.7940, accuracy: 15217/44672 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 350, average loss: 1.7936, accuracy: 15265/44800 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 351, average loss: 1.7930, accuracy: 15327/44928 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 352, average loss: 1.7921, accuracy: 15385/45056 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 353, average loss: 1.7916, accuracy: 15428/45184 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 354, average loss: 1.7909, accuracy: 15483/45312 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 355, average loss: 1.7906, accuracy: 15525/45440 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 356, average loss: 1.7898, accuracy: 15577/45568 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 357, average loss: 1.7890, accuracy: 15630/45696 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 358, average loss: 1.7882, accuracy: 15683/45824 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 359, average loss: 1.7875, accuracy: 15739/45952 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 360, average loss: 1.7869, accuracy: 15786/46080 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 361, average loss: 1.7864, accuracy: 15839/46208 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 362, average loss: 1.7859, accuracy: 15886/46336 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 363, average loss: 1.7851, accuracy: 15943/46464 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 364, average loss: 1.7844, accuracy: 15993/46592 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 365, average loss: 1.7839, accuracy: 16049/46720 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 366, average loss: 1.7835, accuracy: 16094/46848 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 367, average loss: 1.7831, accuracy: 16145/46976 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 368, average loss: 1.7824, accuracy: 16202/47104 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 369, average loss: 1.7818, accuracy: 16262/47232 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 370, average loss: 1.7812, accuracy: 16306/47360 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 371, average loss: 1.7804, accuracy: 16369/47488 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 372, average loss: 1.7797, accuracy: 16420/47616 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 373, average loss: 1.7793, accuracy: 16469/47744 (34%)\n",
      "*Task <1> SubProcess*: Train - batches : 374, average loss: 1.7787, accuracy: 16526/47872 (35%)\n",
      "*Task <1> SubProcess*: Train - batches : 375, average loss: 1.7783, accuracy: 16579/48000 (35%)\n",
      "*Task <1> SubProcess*: Train - batches : 376, average loss: 1.7776, accuracy: 16638/48128 (35%)\n",
      "*Task <1> SubProcess*: Train - batches : 377, average loss: 1.7772, accuracy: 16694/48256 (35%)\n",
      "*Task <1> SubProcess*: Train - batches : 378, average loss: 1.7769, accuracy: 16742/48384 (35%)\n",
      "*Task <1> SubProcess*: Train - batches : 379, average loss: 1.7763, accuracy: 16791/48512 (35%)\n",
      "*Task <1> SubProcess*: Train - batches : 380, average loss: 1.7759, accuracy: 16846/48640 (35%)\n",
      "*Task <1> SubProcess*: Train - batches : 381, average loss: 1.7755, accuracy: 16901/48768 (35%)\n",
      "*Task <1> SubProcess*: Train - batches : 382, average loss: 1.7753, accuracy: 16950/48896 (35%)\n",
      "*Task <1> SubProcess*: Train - batches : 383, average loss: 1.7747, accuracy: 17004/49024 (35%)\n",
      "*Task <1> SubProcess*: Train - batches : 384, average loss: 1.7742, accuracy: 17063/49152 (35%)\n",
      "*Task <1> SubProcess*: Train - batches : 385, average loss: 1.7739, accuracy: 17115/49280 (35%)\n",
      "*Task <1> SubProcess*: Train - batches : 386, average loss: 1.7734, accuracy: 17173/49408 (35%)\n",
      "*Task <1> SubProcess*: Train - batches : 387, average loss: 1.7728, accuracy: 17225/49536 (35%)\n",
      "*Task <1> SubProcess*: Train - batches : 388, average loss: 1.7723, accuracy: 17276/49664 (35%)\n",
      "*Task <1> SubProcess*: Train - batches : 389, average loss: 1.7716, accuracy: 17326/49792 (35%)\n",
      "*Task <1> SubProcess*: Train - batches : 390, average loss: 1.7713, accuracy: 17375/49920 (35%)\n",
      "*Task <1> SubProcess*: Train - batches : 391, average loss: 1.7706, accuracy: 17412/50000 (35%)\n",
      "*Task <1> SubProcess*: Test - batches: 391, average loss: 0.0129, accuracy: 3919/10000 (39%)\n",
      "*Task <1> SubProcess*: 2021-02-10 16:41:46.522313 434 INFO Finish running user model with exit code 0\n",
      "*Task <1> SubProcess*: 2021-02-10 16:41:46.535243 434 INFO Finalize\n",
      "*Task <1> SubProcess*: Enforce permission\n",
      "*Task <1> SubProcess*: finish with rc=0\n",
      "*Task <1> SubProcess*: 2021-02-10 16:41:47.036839 309 finishing command. Results are:  0\n",
      "*Task <1> SubProcess*: 2021-02-10 16:41:47.037345 309 callback on_task_invoke_end is started\n",
      "*Task <1> SubProcess*: 2021-02-10 16:41:47.175253 309 callback on_task_invoke_end is finished\n",
      "*Task <1> SubProcess*: 2021-02-10 16:41:47.175519 309 on_task_invoke finished. return_code=0, time_cost=2.21 minutes\n",
      "*Task <1> SubProcess*: 2021-02-10 16:41:47.198990 39 INFO List GPUs\n",
      "*Task <1> SubProcess*: Wed Feb 10 16:41:47 2021       \n",
      "*Task <1> SubProcess*: +-----------------------------------------------------------------------------+\n",
      "*Task <1> SubProcess*: | NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "*Task <1> SubProcess*: |-------------------------------+----------------------+----------------------+\n",
      "*Task <1> SubProcess*: | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "*Task <1> SubProcess*: | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "*Task <1> SubProcess*: |                               |                      |               MIG M. |\n",
      "*Task <1> SubProcess*: |===============================+======================+======================|\n",
      "*Task <1> SubProcess*: |   0  Tesla V100-PCIE...  On   | 00000000:0B:00.0 Off |                    0 |\n",
      "*Task <1> SubProcess*: | N/A   51C    P0    50W / 250W |      0MiB / 32510MiB |      0%      Default |\n",
      "*Task <1> SubProcess*: |                               |                      |                  N/A |\n",
      "*Task <1> SubProcess*: +-------------------------------+----------------------+----------------------+\n",
      "*Task <1> SubProcess*:                                                                                \n",
      "*Task <1> SubProcess*: +-----------------------------------------------------------------------------+\n",
      "*Task <1> SubProcess*: | Processes:                                                                  |\n",
      "*Task <1> SubProcess*: |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "*Task <1> SubProcess*: |        ID   ID                                                   Usage      |\n",
      "*Task <1> SubProcess*: |=============================================================================|\n",
      "*Task <1> SubProcess*: |  No running processes found                                                 |\n",
      "*Task <1> SubProcess*: +-----------------------------------------------------------------------------+\n",
      "*Task <1> SubProcess*: 2021-02-10 16:41:47.279380 39 INFO NVIDIA_VISIBLE_DEVICES=GPU-a2ca4f2a-c061-0bbe-bd05-dc2c837f7554\n",
      "*Task <1> SubProcess*: 2021-02-10 16:41:47.293652 39 INFO NVIDIA_DRIVER_CAPABILITIES=compute,utility\n",
      "*Task <1> SubProcess*: 2021-02-10 16:41:47.321406 39 INFO Command exit with 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_executor_stdout_log(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2e6b41ad92f648798297104d2c3ad558"
   },
   "source": [
    "## Download trained model from Watson Machine Learning Accelerator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "d6f15e89a33948ea872a5e8499312ddd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wmla-console-wmla.apps.cpd35-beta.cpolab.ibm.com/platform/rest/deeplearning/v1/execs/wmla-329/result\n",
      "Save model:  /project_data/data_asset/wmla-329.zip\n"
     ]
    }
   ],
   "source": [
    "download_trained_model(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d54fd7cf1f2549b38b72c11a1e5293ae"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 + GPU with applications",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
